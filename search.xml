<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Snowboy使用说明_Hotword Detection]]></title>
    <url>%2F2017%2F08%2F16%2FSnowboy%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_Hotword%20Detection%2F</url>
    <content type="text"><![CDATA[IntroductionSnowboy是一个高度自定义的基于实时甚至离线情况的hotword detection engine(敏感词检测机制？)，兼容Raspberry Pi,Linux and Mac OS X. hotword也被称为唤醒词(wake word)或触发词(trigger word)通常是一个关键词或短语，计算机会一直监听作为一个信号用于触发其他操作。 有一些例子，如Amazon Echo的“Alexa”、Google Assistant的”OK Google”和iPhone的“Hey Siri”. 这些关键词用于触发一个全面的语音交互。但是，hotwords同样可以用于其他方面，像是命令和控制等。 一种简单的方案是，运行全ASR(Automatic Speech Recognition)来检测hotword detection.在这种方案中，设备会一直观察特定触发词。但是ASR也会消耗设备和带宽资源。同样的，如果基于云的应用，将不能保护你的隐私。（因为会一直开着麦克风来检测关键词，那么周围环境包括你说的话也会实时被监听了).幸运的是，Snowboy会用来解决此类问题。 highly customizable(高度自定义） always listening but protects your privacy(总是检测但是不会泄露隐私） light-weight and embedded Apache licensed! Quick Start准备工作： 一台带有microphone的设备 相应的解压软件 训练好的模型 树莓派pre-packaged Access Microphone使用PortAudio作为一个跨平台的音频输入/输出。同样也使用sox快速检查microphone是否正确安装。 Install Sox 1sudo apt-get install python-pyaudio python3-pyaudio sox Install PortAudio’s Python bindings: 12pip install pyaudio#pip-3.2 install pyaudio To check whether you can record via your microphone, open a terminal and run: 12rec temp.wav#记录个几秒中，ctrl+c,再play temp.wav,听声 Decoder Structures上面预安装包下载解压后，如下： ├── README.md ├── _snowboydetect.so ├── demo.py ├── demo2.py ├── light.py ├── requirements.txt ├── resources │ ├── ding.wav │ ├── dong.wav │ ├── common.res │ └── snowboy.umdl ├── snowboydecoder.py ├── snowboydetect.py └── version _snowboydetect.so是用SWIG编译的一个动态链接库，依赖于系统的Python2库。snowboy所有相关库都被静态连接在这个文件里。 snowboydetect.py是一个SWIG生成的python wrapper文件。因为不易阅读，我们创建了高等级的wrapper: snowboydecoder.py 应该在https://snowboy.kitt.ai 上训练你的模型(snowboy.pmdl)，或者你也可以使用同一模型resources/snowboy.umdl Runing a Demo To access the simple demo in main code of snowboydecoder.py, run the following command in your Terminal: 12python demo.py snowboy.pmdl#snowboy.pmdl是你训练的hotword模型 When prompt,speak into your microphone to see whether snowboy detects your magic phrase. 12345678910111213141516171819202122232425262728293031323334#demo.pyimport snowboydecoderimport sysimport signalinterrupted = Falsedef signal_handler(signal, frame): global interrupted interrupted = Truedef interrupt_callback(): global interrupted return interruptedif len(sys.argv) == 1: print("Error: need to specify model name") print("Usage: python demo.py your.model") sys.exit(-1)model = sys.argv[1]signal.signal(signal.SIGINT, signal_handler)detector = snowboydecoder.HotwordDetector(model, sensitivity=0.5)print('Listening... Press Ctrl+C to exit')detector.start(detected_callback=snowboydecoder.ding_callback, interrupt_check=interrupt_callback, sleep_time=0.03)detector.terminate() 主程序在detector.start()中循环，每sleep_time=0.03: 检查ring buffer 是否有hotword, if YES,调用detected_callback函数 调用interrupt_check函数，if True，中断主程序，返回. 目前，在demo中令detected_callback=snowboydecoder.ding_callback,所以每当检测到关键词时，设备会“叮”一下。 原文出处：http://docs.kitt.ai/snowboy/#running-a-demo]]></content>
      <categories>
        <category>树莓派3</category>
      </categories>
      <tags>
        <tag>语音识别</tag>
        <tag>树莓派3</tag>
        <tag>UNIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReSpeaker智能语音双麦克风阵列]]></title>
    <url>%2F2017%2F08%2F11%2FReSpeaker%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%8F%8C%E9%BA%A6%E5%85%8B%E9%A3%8E%E9%98%B5%E5%88%97%2F</url>
    <content type="text"><![CDATA[去年圣诞节买了块树莓派，看了网上好多基于树莓派的DIY语音助手，一直想自己也山寨一个。主要有两个问题，一是语音输入如何解决？二是Alexa和google home都是国外，不仅仅是不支持中文，而且还有一堵“墙”在，比较麻烦。 第一个问题，网上主要有两种办法，要么直接搞免驱mini USB麦克风或者大的比较丑的的麦克风，要么就是科技范十足的麦克风阵列。我是非常倾向于用麦克风阵列的，也没想4麦克、7麦克、8麦克等，两麦克阵列就OK了，但是找了一圈没找着合适的。主要是价钱太贵，不忍心剁手。后来将就着买了个mini USB麦克风，但是除了敲桌子声音或者直接吼，根本没啥效果，检测不到声音(明明看着国外的小哥哥们用的挺好的。。。也可能是淘宝上买到假货了)。 第二个，明显今年百度开始在AI上下了不少功夫。语音这一块，主要有UNIT、DuerOS平台。虽然做的没有Google Home 和Alexa那么知名，但是支持中文，而且在国内。 所以借着百度之星UNIT对话系统，又开始了我的瞎折腾(之前搞了Alexa和Google Home,加上不支持中文、墙、硬件问题等，搞得头大就放弃了)。这一次，借助着百度AI平台，语音识别转文字，然后利用UNIT解析，再语音合成输出。那么就剩下硬件了，还是上面的选择，USB麦克风或者麦克风阵列。经人推荐，找到了一款双麦克阵列模块，兼容树莓派3，淘宝上有旗舰店，80元RMB.(哈哈，当时直接就下单了！) 淘宝上给的简介是： ReSpeaker智能语音方案 双麦克风扩展板 兼容树莓派Zero/3B/2B此产品集成了亚马逊语言和谷歌助手等，兼容树莓派Zero、树莓派3B/2B，可以构建一个更强大更灵活的语音产品。 淘宝链接 但实际上，这货的外国名是ReSpeaker 2-Mics Pi HAT 相关链接 ReSpeaker 2-Mics Pi HAT是一款为树莓派而设计针对AI或语音应用的双麦克风扩展板。这意味着你可以基于树莓派（集成Amazon Alexa,Google Assistant)建立一个更强大、更灵活的语音产品. 这块板子基于WM8960,一片低功耗立体声编解码器。有两个麦克风分别位于板子的两侧用于采集声音。板子上还有3个APA102 RGB LED,1个用户按键和两个Grove接口用于扩展应用。更惊喜的是，还有3.5mm Audio Jack和JST 2.0 Speaker接口用于输出声音。 特点 兼容树莓派(Raspberry Pi Zero and Zero W, Raspberry Pi B+,Raspberry Pi 2B and Raspberry Pi 3B) 2个麦克风 2个Grove接口 1个用户按键 3.5mm音频接口 JST2.0音频输出 应用领域 语音交互应用 AI助手 硬件介绍 BUTTON:用户按键，连接到GPIO17 MIC_L &amp; MIC_R:位于板子两侧的麦克风 RGB LED:3颗APA102 RGB LED,连接到了SPI接口 WM8960: 低功耗立体声编解码器 Raspberry Pi 40-Pin Headers: POWER:板子的USB供电口，当使用扬声器时，要保证足够的电流 I2C:Grove I2C接口，连接I2C-1 GPIO 12：Grove 数字端口，连接GPIO12 &amp; GPIO13 JST 2.0 SPEAKER OUT:连接扬声器 3.5mm AUDIO JACK:连接带有3.5mm插口的耳机或扬声器 具体配置及使用参考：http://wiki.seeed.cc/Respeaker_2_Mics_Pi_HAT/]]></content>
      <categories>
        <category>树莓派3</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>树莓派3</tag>
        <tag>UNIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017百度之星开发者大赛--资格赛2]]></title>
    <url>%2F2017%2F08%2F10%2F%E7%99%BE%E5%BA%A6%E4%B9%8B%E6%98%9F%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E8%B5%84%E6%A0%BC%E8%B5%9B2%2F</url>
    <content type="text"><![CDATA[7月28日，百度之星小姐姐给打过电话后，连着搞了两天，其实也没两天，还帮老板整理了好些文档资料。前一篇博客上也说了，搞了69.06分，第9名。然后想着反正肯定可以进前100啦，就没怎么管理了。优化方案也是按着百度AI开发论坛上提供的UNIT机器人优化指南结合UNIT使用手册做的。 最后结果： 事先，基本的意图、词槽、Bot回应等等按照规定做好。实际中，我开始就没认真搞好，漏了一个错误的词槽。但是在最后资格赛结束之前发现了，改正之后提高了1点几个分数。 基本的对话系统搞定之后，后期的优化主要分为两块： 增加对话样本集 增加对话模板集 对话样本集对应着样本学习，假设对话样本足够多、样本质量非常高，那通过神经网络学习得到的模型自然也不会差。 对话模板集则是对应着规则学习，正常人对话时，会有一定的语言结构，如，主语+谓语+宾语结构。特别是在一个特定的场景下，可以充分考虑到用户的对话的模板，进行一些提炼，得到一些语言“公式”。显然这种对话模板集方式的泛化能力是有限的，但是它的优点在于可以保证在符合“公式”的对话时不会出现错误。 我所侧重的是第一个方法，即增加对话样本集。使用了目前UNIT的隐藏功能，推荐对话样本。自己可以先整一个小的对话样本，然后让UNIT平台根据自己DIY的对话样本生成一些样本，然后自己去标注。大约标注了3500个样本吧，自己整的样本比例是按照百度提供的对话样本中各个意图的比例整的，保证同分布吧。]]></content>
      <categories>
        <category>UNIT</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>UNIT</tag>
        <tag>百度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017百度之星开发者大赛--资格赛]]></title>
    <url>%2F2017%2F07%2F30%2F%E7%99%BE%E5%BA%A6%E4%B9%8B%E6%98%9F%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E8%B5%84%E6%A0%BC%E8%B5%9B%2F</url>
    <content type="text"><![CDATA[之前有看到网上说每年百度之星是一个不错的编程比赛，可以检验一下自己的编程能力。所以，当宿舍楼下见到贴的海报的时候也就报名了，报名的时候发现今年百度之星有些变革。与以往不同的是，今年多了个百度之星开发者大赛，以前都是程序设计大赛啊（全TM是那帮搞ACM的天下啊。。。）。仔细想想前段时间搞美团的Code-M比赛，那叫一个惨啊，不是不会啊，全是运行超时，需要优化！咱们又不是计算机学院的，又不没搞过ACM,没练过啊。于是这次直接就报了开发者大赛，都没看看程序设计是搞啥的。 开发者大赛是利用百度自己一个新的平台UNIT(Understanding and Interaction Technology),之前博客也介绍过。估计百度是想借着比赛推广一下平台，顺便再收集一些数据。总之，利用UNIT平台做出一个东西，如智能硬件、APP啥的，没有具体的限制。 比赛分为资格赛和正赛两部分，资格赛是用来熟悉UNIT平台的，只有在两个场景下进入前200名的才有资格进入正赛。 资格赛任务介绍 任务：基于UNIT平台优化给定场景的对话能力 使用UNIT平台优化题设场景的对话能力，提供两个固定场景供参赛者选择： 场景一：订餐馆，包括查询餐馆、订位或发起导航 场景二：看电影，包括查询电影、影院、购票电影票 每个场景由主办方预先提供意图与词槽的定义，并提供一定量对话样本作为训练数据。参赛者需要使用UNIT完成场景的意图与词槽的配置，并利用对话样本优化场景的对话理解能力（参赛者自行增加对话样本与词槽的词表等），并最终在UNIT平台上产出模型与服务。 正赛任务介绍 任务：设计和开发一个以对话式人机交互为核心的智能产品 产品形式包括但不限于手机APP、智能硬件等。产品必须使用百度提供的理解与交互技术平台（以下简称&gt; &gt; UNIT平台）实现核心对话能力，可以使用百度AI平台开放的其他技术能力作为辅助，其余百度未提供的功&gt; 能与能力，实现形式不限。 刚开始接触感觉挺难的，想找些队员，在群里吼了两嗓子，但只有两个妹子回应，最后也就没有了下文。正好自己又忙着开题，也就打算放弃了。就这样，7月28号，这次百度之星的组织者吧，听声音应该是个小姐姐直接打电话过来了。说是资格赛不是很难，尽量还是参与一下，比较容易可以进正赛。还举例子说东北大学、大连理工的几位同学两天就搞到了60、70分。后来，想了下，觉得也是，小姐姐都打电话过来了。抱着试一试的心态，搞了将近两天吧，69.06分，第9名。 头半天在及格线附近，又连忙按着使用手册搞了一天，标注了快3000条样本才把分数提上去。下回写一下具体优化过程，感觉UNIT平台以后要是集成语音交互功能，再加上百度的中文处理资源，还是有一定钱途的。]]></content>
      <categories>
        <category>UNIT</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>UNIT</tag>
        <tag>百度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Complete Tutorial to Learn Data Science with Python from Scratch(3)]]></title>
    <url>%2F2017%2F07%2F27%2FA%20Complete%20Tutorial(3)%2F</url>
    <content type="text"><![CDATA[4. Data Munging in Python : Using PandasFor those, who have been following, here are your must wear shoes to start running. Data munging – recap of the need While our exploration of the data, we found a few problems in the data set, which needs to be solved before the data is ready for a good model. This exercise is typically referred as “Data Munging”. Here are the problems, we are already aware of: There are missing values in some variables. We should estimate those values wisely depending on the amount of missing values and the expected importance of variables.While looking at the distributions, we saw that ApplicantIncome and LoanAmount seemed to contain extreme values at either end. Though they might make intuitive sense, but should be treated appropriately.In addition to these problems with numerical fields, we should also look at the non-numerical fields i.e. Gender, Property_Area, Married, Education and Dependents to see, if they contain any useful information. If you are new to Pandas, I would recommend reading this article(Pandas的12种奇淫异巧) before moving on. It details some useful techniques of data manipulation. 数据修复 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1df=pd.read_csv("G:/Datahack/Loanprediction/TrainFile/train.csv") #读取数据 Check missing values in the datasetLet us look at missing values in all the variables because most of the models don’t work with missing data and even if they do, imputing them helps more often than not. So, let us check the number of nulls / NaNs in the dataset.(检查一下数据集的缺失值) 12df.apply(lambda x:sum(x.isnull()),axis=0) # 匿名函数，axis=0表示针对每列应该用，可以看看#与 df.apply(lambda x:sum(x.isnull()),axis=1)的区别 Loan_ID 0 Gender 13 Married 3 Dependents 15 Education 0 Self_Employed 32 ApplicantIncome 0 CoapplicantIncome 0 LoanAmount 22 Loan_Amount_Term 14 Credit_History 50 Property_Area 0 Loan_Status 0 dtype: int64 可以看到Gender丢失13个，Married丢失3个，Dependents丢失15个，Self_Employed丢失32个，LoanAmount丢失22个，Loan_Amount_Term丢失14个，Credit_History丢失50个。 Though the missing values are not very high in number, but many variables have them and each one of these should be estimated and added in the data. Get a detailed view on different imputation techniques through this article(A Comprehensive Guide to Data Exploration). Note: Remember that missing values may not always be NaNs. For instance, if the Loan_Amount_Term is 0, does it makes sense or would you consider that missing? I suppose your answer is missing and you’re right. So we should check for values which are unpractical.(值得注意的是，并非所有丢失值都是NaN,也可能是0或者其他不合理的值) How to fill missing values in LoanAmount?如何填充丢失值是一个问题，下面以LoanAmount为例。 There are numerous ways to fill the missing values of loan amount – the simplest being replacement by mean, which can be done by following code:（最简单的方法，以平均值进行填充） 1df['LoanAmount'].fillna(df['LoanAmount'].mean(),inplace=True)# 平均值填充 The other extreme could be to build a supervised learning model to predict loan amount on the basis of other variables and then use age along with other variables to predict survival.(另一个方法是建立一个有监督学习模型利用其他变量来预测未知值） Since, the purpose now is to bring out the steps in data munging, I’ll rather take an approach, which lies some where in between these 2 extremes. A key hypothesis is that the whether a person is educated or self-employed can combine to give a good estimate of loan amount.（但这里只是一个入门教程，采用一种介于上述两种方法之间的手段来填充丢失值） First, let’s look at the boxplot to see if a trend exists: 1df.boxplot(column='LoanAmount',by=['Education','Self_Employed']) &lt;matplotlib.axes._subplots.AxesSubplot at 0x9d956d8&gt; Thus we see some variations in the median of loan amount for each group and this can be used to impute the values. But first, we have to ensure that each of Self_Employed and Education variables should not have a missing values.(由上面的Boxplot可以看出，不同Education和Self_Employed的组合对LoanAmount的中位数还是有影响的。因此，可以用Education和Self_Employed来进行推断。但在此之前，必须保证Education和Self_Employed是完整的) As we say earlier, Self_Employed has some missing values. Let’s look at the frequency table: 1df['Self_Employed'].value_counts() No 500 Yes 82 Name: Self_Employed, dtype: int64 Since ~86% values are “No”, it is safe to impute the missing values as “No” as there is a high probability of success. This can be done using the following code: 1df['Self_Employed'].fillna('No',inplace=True) Now, we will create a Pivot table, which provides us median values for all the groups of unique values of Self_Employed and Education features. Next, we define a function, which returns the values of these cells and apply it to fill the missing values of loan amount:(由Education和Self_Employed两列所确定的LoanAmount的中位数来填充LoanAmount的丢失值) 1234567table = df.pivot_table(values='LoanAmount', index='Self_Employed' , columns='Education', aggfunc=np.median)# Define function to return value of this pivot_tabledef fage(x): return table.loc[x['Self_Employed'],x['Education']]# Replace missing valuesdf['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True) How to treat for extreme values in distribution of LoanAmount and ApplicantIncome ?Let’s analyze LoanAmount first. Since the extreme values are practically possible, i.e. some people might apply for high value loans due to specific needs. So instead of treating them as outliers, let’s try a log transformation to nullify their effect:(使用Log函数消除极端值的影响） 12df['LoanAmount_log']=np.log(df['LoanAmount'])df['LoanAmount_log'].hist(bins=20) &lt;matplotlib.axes._subplots.AxesSubplot at 0x9f000f0&gt; Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided. Coming to ApplicantIncome. One intuition can be that some applicants have lower income but strong support Co-applicants. So it might be a good idea to combine both incomes as total income and take a log transformation of the same. 123df['TotalIncome']=df['ApplicantIncome']+df['CoapplicantIncome']df['TotalIncome_log']=np.log(df['TotalIncome'])df['LoanAmount_log'].hist(bins=20) &lt;matplotlib.axes._subplots.AxesSubplot at 0xa007da0&gt; Now we see that the distribution is much better than before. I will leave it upto you to impute the missing values for Gender, Married, Dependents, Loan_Amount_Term, Credit_History. Also, I encourage you to think about possible additional information which can be derived from the data. For example, creating a column for LoanAmount/TotalIncome might make sense as it gives an idea of how well the applicant is suited to pay back his loan. 1df.apply(lambda x:sum(x.isnull()),axis=0) Loan_ID 0 Gender 13 Married 3 Dependents 15 Education 0 Self_Employed 0 ApplicantIncome 0 CoapplicantIncome 0 LoanAmount 0 Loan_Amount_Term 14 Credit_History 50 Property_Area 0 Loan_Status 0 LoanAmount_log 0 TotalIncome 0 TotalIncome_log 0 dtype: int64 丢失的还有Gender,Married,Dependents,Loan_Amount_Term,Credit_History5个。其中Married比较好处理，直接以大多数类别填充，先把它处理啦。 1df['Married'].value_counts() Yes 398 No 213 Name: Married, dtype: int64 1df['Married'].fillna('Yes',inplace=True) 接着处理Gender(性别),缺了13个。直观上，先看看Gender和Married、Education、Self_Employed、Property_Area的关系。 1234def percConvert(ser): return ser/float(ser[-1])pd.crosstab(df['Married'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Married No 0.380952 0.619048 1.0 Yes 0.081841 0.918159 1.0 All 0.186356 0.813644 1.0 上表可以看出已婚人士中，Male占0.918，那假如某人性别未知，只知其已婚，是不是有0.918的概率判断其为男性呢？ 1pd.crosstab(df['Education'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Education Graduate 0.196581 0.803419 1.0 Not Graduate 0.150376 0.849624 1.0 All 0.186356 0.813644 1.0 上表可以看出性别与其Education关系好像并不大。 1pd.crosstab(df['Self_Employed'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Self_Employed No 0.185468 0.814532 1.0 Yes 0.192308 0.807692 1.0 All 0.186356 0.813644 1.0 上表可以看出性别与其Self_Employed关系好像也并不大。 1pd.crosstab(df['Dependents'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Dependents 0 0.236686 0.763314 1.0 1 0.188119 0.811881 1.0 2 0.070707 0.929293 1.0 3+ 0.062500 0.937500 1.0 All 0.186007 0.813993 1.0 1234567def coding(col,codeDict): colCoded=pd.Series(col,copy=True) for key,value in codeDict.items(): colCoded.replace(key,value,inplace=True) return colCodeddf['Gender']=coding(df['Gender'],&#123;'Male':1,'Female':0&#125;) #将‘Gender'转换为0,1表示（男士：1，女生：0）table2=df.pivot_table(values=['Gender'],index=['Married','Dependents'],aggfunc=np.median) 1234567#pd.crosstab(df['Married'],df['Dependents'],margins=True).apply(percConvert,axis=1)def dependent(x): if x['Married']=='No': return '0' else: return '1'df['Dependents'].fillna(df[df['Dependents'].isnull()].apply(dependent,axis=1),inplace=True) 12345678# Define function to return value of this pivot_tabledef fage(x): if table2.loc[x['Married'],x['Dependents']].values[0]==1.0: return 1.0 else: return 0.0# Replace missing valuesdf['Gender'].fillna(df[df['Gender'].isnull()].apply(fage, axis=1), inplace=True) 1df.apply(lambda x:sum(x.isnull()),axis=0) Loan_ID 0 Gender 0 Married 0 Dependents 0 Education 0 Self_Employed 0 ApplicantIncome 0 CoapplicantIncome 0 LoanAmount 0 Loan_Amount_Term 14 Credit_History 50 Property_Area 0 Loan_Status 0 LoanAmount_log 0 TotalIncome 0 TotalIncome_log 0 dtype: int64 可以看到只剩下Loan_Amount_Term和Credit_History没有处理啦，因为Credit_History由前面分析，对于结果来说是非常重要的，那么就放在最后处理好了。现在来对Loan_Amount_Term进行分析，Loan_Amount_Term是数值型。 1df.boxplot(column='Loan_Amount_Term',by=['Education','Self_Employed']) &lt;matplotlib.axes._subplots.AxesSubplot at 0xbc43d30&gt; 12termMedian=df['Loan_Amount_Term'].median()df['Loan_Amount_Term'].fillna(termMedian,inplace=True)#直接由中位数填充好了…… 按理说，Credit_History应该是根据一个人的性别，婚姻，家属，教育等申请之前的状态有很大关系，而与当前该次申请贷款的数额、期限等业务关系不大。 1df.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 614 entries, 0 to 613 Data columns (total 17 columns): Loan_ID 614 non-null object Gender 614 non-null float64 Married 614 non-null object Dependents 614 non-null object Education 614 non-null object Self_Employed 614 non-null object ApplicantIncome 614 non-null int64 CoapplicantIncome 614 non-null float64 LoanAmount 614 non-null float64 Loan_Amount_Term 614 non-null object Credit_History 564 non-null float64 Property_Area 614 non-null object Loan_Status 614 non-null object LoanAmount_log 614 non-null float64 TotalIncome 614 non-null float64 TotalIncome_log 614 non-null float64 LoanAmount/TotalIncome 614 non-null float64 dtypes: float64(8), int64(1), object(8) memory usage: 81.6+ KB 123456from sklearn.preprocessing import LabelEncodervar_mod=['Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']le=LabelEncoder()for i in var_mod: df[i]=le.fit_transform(df[i])df.dtypes#将以上非数值型，转化为数值型量 Loan_ID object Gender float64 Married int64 Dependents int64 Education int64 Self_Employed int64 ApplicantIncome int64 CoapplicantIncome float64 LoanAmount float64 Loan_Amount_Term object Credit_History float64 Property_Area int64 Loan_Status int64 LoanAmount_log float64 TotalIncome float64 TotalIncome_log float64 LoanAmount/TotalIncome float64 dtype: object 5. Building a Predictive Model in PythonAfter, we have made the data useful for modeling, let’s now look at the python code to create a predictive model on our data set. Skicit-Learn (sklearn) is the most commonly used library in Python for this purpose and we will follow the trail. I encourage you to get a refresher on sklearn through this article. Next, we will import the required modules. Then we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores. Since this is an introductory article, I will not go into the details of coding. Please refer to this article for getting details of the algorithms with R and Python codes. Also, it’ll be good to get a refresher on cross-validation through this article, as it is a very important measure of power performance. 123456#Import models from scikit learn module:from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import KFoldfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifier, export_graphvizfrom sklearn import metrics 1234567891011121314151617181920212223242526272829303132#Generic function for making a classification model and accessing performance:def classification_model(model,data,predictors,outcome): #Fit the model: model.fit(data[predictors],data[outcome]) #Make predictions on training set: predictions=model.predict(data[predictors]) #Print accuracy accuracy=metrics.accuracy_score(predictions,data[outcome]) print("Accuracy : %s" % "&#123;0:.3%&#125;".format(accuracy)) #Perform k-fold cross-validation with 5 folds kf=KFold(n_splits=5).split(data) error=[] for train,test in kf: #Filter training data train_predictors=(data[predictors].iloc[train,:]) #The target we're using to train the algorithm. train_target=data[outcome].iloc[train] #Training the algorithm using the predictors and target. model.fit(train_predictors,train_target) #Record error from each cross-validation run error.append(model.score(data[predictors].iloc[test,:],data[outcome].iloc[test])) print("Cross-Validation Score : %s" % "&#123;0:.3%&#125;".format(np.mean(error))) #Fit the model again so that it can be refered outside the function: model.fit(data[predictors],data[outcome]) 1creditTrain=df[df['Credit_History'].notnull()] #以Credit_History为输出进行预测 1234outcome_var='Credit_History'model=LogisticRegression()#模型选用普通的线性回归predictor_var=['Gender','Married','Education','TotalIncome']classification_model(model,creditTrain,predictor_var,outcome_var) Accuracy : 84.220% Cross-Validation Score : 84.218% 1234outcome_var='Credit_History'model=DecisionTreeClassifier()#模型改为决策树predictor_var=['Gender','Married','Education','TotalIncome']classification_model(model,creditTrain,predictor_var,outcome_var) Accuracy : 99.113% Cross-Validation Score : 74.651% 1234outcome_var='Credit_History'model=RandomForestClassifier(n_estimators=25,min_samples_split=25,max_depth=7,max_features=1)#模型改为Random Forestpredictor_var=['Gender','Married','Education','TotalIncome']classification_model(model,creditTrain,predictor_var,outcome_var) Accuracy : 84.929% Cross-Validation Score : 83.864% 12creditTest=df[df['Credit_History'].isnull()]#以Credit_History丢失值作为测试集，进行预测填充丢失值creditPredictions=model.predict(creditTest[predictor_var]) 1df.loc[df['Credit_History'].isnull(),'Credit_History']=creditPredictions #以预测值进行填充 Logistic RegressionLet’s make our first Logistic Regression model. One way would be to take all the variables into the model but this might result in overfitting (don’t worry if you’re unaware of this terminology yet). In simple words, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well. Read more about Logistic Regression. We can easily make some intuitive hypothesis to set the ball rolling. The chances of getting a loan will be higher for: Applicants having a credit history (remember we observed this in exploration?) Applicants with higher applicant and co-applicant incomes Applicants with higher education level Properties in urban areas with high growth perspectives So let’s make our first model with ‘Credit_History’. 1234outcome_var='Loan_Status'model=LogisticRegression()predictor_var=['Credit_History']#仅仅使用一个特征来预测classification_model(model,df,predictor_var,outcome_var) Accuracy : 80.945% Cross-Validation Score : 80.946% 123#再加上一些特征predictor_var=['Credit_History','Education','Married','Self_Employed','Property_Area','LoanAmount/TotalIncome']classification_model(model,df,predictor_var,outcome_var) Accuracy : 80.945% Cross-Validation Score : 80.946% Generally we expect the accuracy to increase on adding variables. But this is a more challenging case. The accuracy and cross-validation score are not getting impacted by less important variables. Credit_History is dominating the mode. We have two options now:(可以看到使用线性回归方法，在增加更多特征的时候，预测效果并没有很好改变，几乎和原来一样。这时候就需要考虑以下两种方法。） Feature Engineering: dereive new information and try to predict those. I will leave this to your creativity. Better modeling techniques. Let’s explore this next. Decision TreeDecision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model. Read more about Decision Trees. 123model=DecisionTreeClassifier()#换模型predictor_var=['Credit_History','Gender','Married','Education']classification_model(model,df,predictor_var,outcome_var) Accuracy : 80.945% Cross-Validation Score : 80.946% Here the model based on categorical variables is unable to have an impact because Credit History is dominating over them. Let’s try a few numerical variables: 12predictor_var=['Credit_History','Loan_Amount_Term','LoanAmount_log']classification_model(model,df,predictor_var,outcome_var) Accuracy : 88.925% Cross-Validation Score : 69.371% Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Let’s try an even more sophisticated algorithm and see if it helps: Random ForestRandom forest is another algorithm for solving the classification problem. Read more about Random Forest. An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features. 1234model=RandomForestClassifier(n_estimators=100)predictor_var=['Gender','Married','Dependents','Education','Self_Employed','Loan_Amount_Term','Credit_History', 'Property_Area','LoanAmount_log','TotalIncome_log','LoanAmount/TotalIncome']classification_model(model,df,predictor_var,outcome_var) Accuracy : 100.000% Cross-Validation Score : 78.665% Here we see that the accuracy is 100% for the training set(显然过拟合). This is the ultimate case of overfitting and can be resolved in two ways: Reducing the number of predictors Tuning the model parameters Let’s try both of these. First we see the feature importance matrix from which we’ll take the most important features. 123#Create a series with feature importances:featimp=pd.Series(model.feature_importances_,index=predictor_var).sort_values(ascending=False)print(featimp) Credit_History 0.263870 LoanAmount/TotalIncome 0.192546 TotalIncome_log 0.182355 LoanAmount_log 0.164331 Property_Area 0.041543 Dependents 0.041328 Loan_Amount_Term 0.035326 Married 0.023571 Gender 0.019269 Education 0.018238 Self_Employed 0.017623 dtype: float64 Let’s use the top 6 variables for creating a model. Also, we will modify the parameters of random forest model a little bit: 1234model=RandomForestClassifier(n_estimators=25,min_samples_split=25,max_depth=7,max_features=1)predictor_var=['TotalIncome_log','LoanAmount_log','Credit_History','LoanAmount/TotalIncome','Property_Area', 'Dependents']classification_model(model,df,predictor_var,outcome_var) Accuracy : 83.550% Cross-Validation Score : 80.782% Notice that although accuracy reduced, but the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. But the output should stay in the ballpark. You would have noticed that even after some basic parameter tuning on random forest, we have reached a cross-validation accuracy only slightly better than the original logistic regression model. This exercise gives us some very interesting and unique learning: Using a more sophisticated model does not guarantee better results. Avoid using complex modeling techniques as a black box without understanding the underlying concepts. Doing so would increase the tendency of overfitting thus making your models less interpretable Feature Engineering is the key to success. Everyone can use an Xgboost models but the real art and creativity lies in enhancing your features to better suit the model. So are you ready to take on the challenge? Start your data science journey with Loan Prediction Problem. 参考文章：https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Complete Tutorial to Learn Data Science with Python from Scratch(1)]]></title>
    <url>%2F2017%2F07%2F25%2FA%20Complete%20Tutorail(1)%2F</url>
    <content type="text"><![CDATA[最近一段时间进行大论文研究方向的调研，主要是关于电子系统或设备故障预测和健康管理（PHM）这一块。也跟老板沟通了，大致就做这一块了。研一在实验室除了上理论课之外，就负责实验室项目中上位机软件编程，也没掌握啥儿高大上的东西。现在论文大方向基本定了，趁着暑假想掌握一些数据分析(Data Analysis)方面的基本技术，包括（Python及其相关库、机器学习、数据挖掘算法等）。 在https://www.analyticsvidhya.com网站里看到一篇很不错的博客，跟着学习了一下。算是对前一段时间Numpy、Pandas、Scipy库的复习运用吧，整体上对用Python进行机器学习过程有一个把握。 目录(Table of Contents) Python基础知识(Basics of Python for Data Analysis) Why learn Python for data analysis? Python 2.7 vs 3.4 How to install Python? Running a few simple program in Python Python库和数据结构(Python libraries and data structures) Python Data Structures Python Iteration and Conditional Constructs Python Libraries 使用Pandas库探索分析(Exploratory analysis in Python using Pandas) Introduction to series and dataframes Analytics Vidhy a dataset- Loan Prediction Problem 使用Pandas数据修复(Data Munging in Python using Pandas) 建立预测模型(Building a Predictive Model in Python) Logistic Regression Decision Tree Random Forest 1、Python基础知识Why learn Python for data analysis? - 开源、免费安装 - 丰富的在线社区论坛 - 简单易学 - 可以成为Data Science的一种标准语言 但是缺点也显而易见， - 解释型语言，速度慢 Python 2.7 vs 3.4 这是一个非常具有争议性的话题，但是，我个人感觉，如果现在入门学习一定要用Python3了。 How to install Python? 有两种方法： -直接从Python官网上下载安装包，一步一步配置环境，需要什么包装什么包 -直接安装Anaconda,基本库都预安装了，啥事儿没有，不过要自己百度Anaconda教程，看看如何使用，挺简单的！ 推荐使用第二种方法，特别是国内&amp;Windows系统，自己配置Python环境，安装相关库真的是非常麻烦的！ 2、Python库和数据结构Python Data Structures Lists-Lists 是Python中使用最为频繁，功能也最为丰富的数据结构。可以简单的由逗号和中括号定义。Lists可以包含不同类型的元素，但是使用时基本都是同类型的。Python中lists是可变的，list中的每个元素也都可以改写。 ListsA list can be simply defined by writing comma separated values in square brackets. 1squares_list=[0,1,4,9,16,25] 1squares_list [0, 1, 4, 9, 16, 25] Individual elements of a list can be accessed by writing the index number in square bracket. Please note that the first index of a list is 0 and not 1 1squares_list[0]#Indexing returns the item 0 A range of script can be accessed by having first index and last index 1squares_list[2:4] #Slicing returns a new list [4, 9] A Negative index access the list from end 1squares_list[-2] #It should return the second last element in the list 16 A few common methods applicable to lists include: append(),extend(),insert(),remove(),pop(),count(),sort(),reverse() Strings- Strings 可以简单的用单引号(‘),双引号(“),三引号( ”’)表示。 Strings用三引号时可以包含多行，因此在Python docstrings中使用较多。‘\’是一个换行符，另起一行。但是需要注意Python strings是不可变的，不能改变strings! StringsA string can be simply defined by using single(‘),double(‘’) or triple(‘’’) quotation 1234greeting='Hello'print(greeting[1])print(len(greeting))print(greeting+'World') e 5 HelloWorld Raw strings can be used to pass on string as is. Python interpretter does not alter the string, if you specify a string to be raw. Raw string can be defined by adding r to the string 12stmt=r'\n is a newline character by default.'print(stmt) \n is a newline character by default. Python strings are immutable and hence can be changed. Doing so will result in an error. Common string methods include lower(),strip(),isdigit(),isspace(),find(),replace(),split() and join(). These are usually very helpful when you need to perform data mainpulation or cleaning on text fields. Tuples - 元组可以用逗号和圆括号表示。元组是不可变的，输出由圆括号包围。尽管元组是不可变的，但是可以包含可变元素。 因为元组是不可变的，在处理上相对于List更快。可以使用tuple代替使用不会更改的list. TuplesA tuple is represented by a number of values separated by commas. 1tuple_example=0,1,4,9,16,25 1tuple_example (0, 1, 4, 9, 16, 25) 1tuple_example[2] 4 1tuple_example[2]=6 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-12-59fc5567f2da&gt; in &lt;module&gt;() ----&gt; 1 tuple_example[2]=6 TypeError: &apos;tuple&apos; object does not support item assignment Dictionary -Dictionary是一种无序的key-value对集合，同一Dictionary中Key必须是唯一的。用大括号变可以建立一个空的Dictionary. DictionaryA dictionary is an unordered set of key: value pairs, with the requirement that the keys are unique (within one dictionary). A pair of braces creates an empty dictionary:{} 12extensions=&#123;'Kunal':9073,'Tavish':9128,'Sunil':9223,'Nitin':9330&#125;extensions {&apos;Kunal&apos;: 9073, &apos;Nitin&apos;: 9330, &apos;Sunil&apos;: 9223, &apos;Tavish&apos;: 9128} 12extensions['Mukesh']=9150extensions {&apos;Kunal&apos;: 9073, &apos;Mukesh&apos;: 9150, &apos;Nitin&apos;: 9330, &apos;Sunil&apos;: 9223, &apos;Tavish&apos;: 9128} 1extensions.keys() dict_keys([&apos;Kunal&apos;, &apos;Tavish&apos;, &apos;Sunil&apos;, &apos;Nitin&apos;, &apos;Mukesh&apos;]) Python Iteration and Conditional ConstructsLike most languages, Python also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax: 12for i in [Python Iterable]: expression(i) Here “Python Iterable” can be a list, tuple or other advanced data structures which we will explore in later sections. Let’s take a look at a simple example, determining the factorial of a number. 123fact=1for i in range(1,N+1): fact *=i Coming to conditional statements, these are used to execute code fragments based on a condition. The most commonly used construct is if-else, with following syntax: 1234if [condition]: __execution if true__else: __execution if false__ For instance, if we want to print whether the number N is even or odd: 1234if N%2 == 0: print('Even')else: print('Odd') Now that you are familiar with Python fundamentals, let’s take a step further. Python LibrariesLets take one step ahead in our journey to learn Python by getting acquainted with some useful libraries. The first step is obviously to learn to import them into our environment. There are several ways of doing so in Python: 1import math as m 1from math import * In the first manner, we have defined an alias m to library math. We can now use various functions from math library (e.g. factorial) by referencing it using the alias m.factorial(). In the second manner, you have imported the entire name space in math i.e. you can directly use factorial() without referring to math. 推荐使用前一种导入方法。 科学计算和数据分析常用的库： NumPy stands for Numerical Python. The most powerful feature of NumPy is n-dimensional array. This library also contains basic linear algebra functions, Fourier transforms, advanced random number capabilities and tools for integration with other low level languages like Fortran, C and C++ SciPy stands for Scientific Python. SciPy is built on NumPy. It is one of the most useful library for variety of high level science and engineering modules like discrete Fourier transform, Linear Algebra, Optimization and Sparse matrices. Matplotlib for plotting vast variety of graphs, starting from histograms to line plots to heat plots.. You can use Pylab feature in ipython notebook (ipython notebook –pylab = inline) to use these plotting features inline. If you ignore the inline option, then pylab converts ipython environment to an environment, very similar to Matlab. You can also use Latex commands to add math to your plot. Pandas for structured data operations and manipulations. It is extensively used for data munging and preparation. Pandas were added relatively recently to Python and have been instrumental in boosting Python’s usage in data scientist community. Scikit Learn for machine learning. Built on NumPy, SciPy and matplotlib, this library contains a lot of effiecient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction. Statsmodels for statistical modeling. Statsmodels is a Python module that allows users to explore data, estimate statistical models, and perform statistical tests. An extensive list of descriptive statistics, statistical tests, plotting functions, and result statistics are available for different types of data and each estimator. Seaborn for statistical data visualization. Seaborn is a library for making attractive and informative statistical graphics in Python. It is based on matplotlib. Seaborn aims to make visualization a central part of exploring and understanding data. Bokeh for creating interactive plots, dashboards and data applications on modern web-browsers. It empowers the user to generate elegant and concise graphics in the style of D3.js. Moreover, it has the capability of high-performance interactivity over very large or streaming datasets. Blaze for extending the capability of Numpy and Pandas to distributed and streaming datasets. It can be used to access data from a multitude of sources including Bcolz, MongoDB, SQLAlchemy, Apache Spark, PyTables, etc. Together with Bokeh, Blaze can act as a very powerful tool for creating effective visualizations and dashboards on huge chunks of data. Scrapy for web crawling. It is a very useful framework for getting specific patterns of data. It has the capability to start at a website home url and then dig through web-pages within the website to gather information. SymPy for symbolic computation. It has wide-ranging capabilities from basic symbolic arithmetic to calculus, algebra, discrete mathematics and quantum physics. Another useful feature is the capability of formatting the result of the computations as LaTeX code. Requests for accessing the web. It works similar to the the standard python library urllib2 but is much easier to code. You will find subtle differences with urllib2 but for beginners, Requests might be more convenient. 还有一些其他的库可能会用到： os for Operating system and file operations networkx and igraph for graph based data manipulations regular expressions for finding patterns in text data BeautifulSoup for scrapping web. It is inferior to Scrapy as it will extract information from just a single webpage in a run. Now that we are familiar with Python fundamentals and additional libraries, lets take a deep dive into problem solving through Python. Yes I mean making a predictive model! In the process, we use some powerful libraries and also come across the next level of data structures. We will take you through the 3 key phases: Data Exploration (分析数据） – finding out more about the data we have Data Munging （数据修复） – cleaning the data and playing with it to make it better suit statistical modeling Predictive Modeling （预测模型） – running the actual algorithms and having fun]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Complete Tutorial to Learn Data Science with Python from Scratch(2)]]></title>
    <url>%2F2017%2F07%2F25%2FA%20Complete%20Tutorail(2)%2F</url>
    <content type="text"><![CDATA[3. Exploratory analysis in Python using PandasPandas是Python中最为实用的一种数据分析库，接下来会简单介绍Pandas，然后利用Pandas读取Analytics Vidhya竞赛中的一个数据集进行探索分析，建立一个简单的分类算法来解决这个问题。 Introduction to Series and Dataframe开始读取数据集之前，先来理解Pandas的两个关键数据结构-Series &amp; DataFrame Series 可以理解为一个1维带标签（labelled）或索引(indexed)的数组,可以通过labels来读取series中的元素（elements). DataFrame 则与Excel表格很相像，可以通过Column names来引用某一列，可以使用Row numbers 来引用某一行。区别在于，在DataFrame中，Column names被称为Column,Row numbers被称为row index. Series and dataframes form the core data model for Pandas in Python. The data sets are first read into these dataframes and then various operations (e.g. group by, aggregation etc.) can be applied very easily to its columns. More: + 10 Minutes to Pandas(亲身经历，感觉10分钟根本没看完，可能英语渣吧……) Practice data set — Loan Prediction Problem 数据集下载地址 貌似是Analytics Vidhya举办的一个竞赛吧，贷款预测问题（Loan Prediction Problem). 1234567891011121314151617181920212223242526272829VARIABLE DESCRIPTIONS:(数据集变量描述)Variable DescriptionLoan_ID(贷款人标识) Unique Loan IDGender(性别) Male/ FemaleMarried(婚姻) Applicant married (Y/N)Dependents(家属) Number of dependentsEducation(教育) Applicant Education (Graduate/ Under Graduate)Self_Employed(自营) Self employed (Y/N)ApplicantIncome(申请人收入) Applicant incomeCoapplicantIncome Coapplicant incomeLoanAmount(贷款金额) Loan amount in thousandsLoan_Amount_Term(贷款期限) Term of loan in monthsCredit_History(信用记录) credit history meets guidelinesProperty_Area(居住地) Urban/ Semi Urban/ RuralLoan_Status(贷款状况) Loan approved (Y/N) Importing libraries and the data set numpy matploylib pandas 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf=pd.read_csv("G:/Datahack/Loanprediction/TrainFile/train.csv") Quick Data ExplorationOnce you have read the dataset, you can have a look at few top rows by using the function head() 1df.head() #print first 5 rows of dataset .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Loan_ID Gender Married Dependents Education Self_Employed ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History Property_Area Loan_Status 0 LP001002 Male No 0 Graduate No 5849 0.0 NaN 360.0 1.0 Urban Y 1 LP001003 Male Yes 1 Graduate No 4583 1508.0 128.0 360.0 1.0 Rural N 2 LP001005 Male Yes 0 Graduate Yes 3000 0.0 66.0 360.0 1.0 Urban Y 3 LP001006 Male Yes 0 Not Graduate No 2583 2358.0 120.0 360.0 1.0 Urban Y 4 LP001008 Male No 0 Graduate No 6000 0.0 141.0 360.0 1.0 Urban Y Next, you can look at summary of numerical fields by using describe() function. describe() function would provide count,mean,standard deviation(std),min,quartiles and max in its output. More: + basic statistics to understand population distribution 同时，可以看到部分列还有缺失值。 1df.describe() #Get summary of numerical variables .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History count 614.000000 614.000000 592.000000 600.00000 564.000000 mean 5403.459283 1621.245798 146.412162 342.00000 0.842199 std 6109.041673 2926.248369 85.587325 65.12041 0.364878 min 150.000000 0.000000 9.000000 12.00000 0.000000 25% 2877.500000 0.000000 100.000000 360.00000 1.000000 50% 3812.500000 1188.500000 128.000000 360.00000 1.000000 75% 5795.000000 2297.250000 168.000000 360.00000 1.000000 max 81000.000000 41667.000000 700.000000 480.00000 1.000000 For the non-numerical values, we can look at frequency distribution to understand whether they make sense or not. The frequency table can be printed by following command: 1df['Property_Area'].value_counts() Semiurban 233 Urban 202 Rural 179 Name: Property_Area, dtype: int64 Distribution analysis通过前面的操作已经熟悉了基本数据特征，下面开始研究各个变量的分布情况。 Lets start by plotting the histogram of ApplicantIncome using the follow commands: 12df['ApplicantIncome'].hist(bins=50,figsize=(15,5))plt.show() 可以看到上图中有少数的极端值。 Next, we look at box plots to understand the distributions. 12df.boxplot(column='ApplicantIncome')plt.show() This confirms the presence of a lot of outliers/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education: 12df.boxplot(column='ApplicantIncome',by='Education')plt.show() We can see that there is no substantial different between the mean income of graduate and non-graduates. But there are a higher number of graduates with very high incomes, which are appearing to be the outliers.可以看出，大学毕业和大学未毕业人数平均收入是差不多的，但是大学毕业会有部分高收入值，形成离群值。 Now, Let’s look at the histogram and boxplot of LoanAmount using the following command: 12df['LoanAmount'].hist(bins=50,figsize=(15,5))plt.show() 12df.boxplot(column='LoanAmount')plt.show() Again, there are some extreme values. Clearly, both ApplicantIncome and LoanAmount require some amount of data munging.（小白表示懵逼啊，有很多离群值就不能用了？需要数据修复？好像吴恩达的视频是提到过） LoanAmount has missing and well as extreme values values, while ApplicantIncome has a few extreme values, which demand deeper understanding. We will take this up in coming sections. Categorical variable analysisNow that we understand distributions for ApplicantIncome and LoanIncome, let us understand categorical variables in more details. We will use Excel style pivot table and cross-tabulation. For instance, let us look at the chances of getting a loan based on credit history. This can be achieved in MS Excel using a pivot table as: Now we will look at the steps required to generate a similar insight using Python. Please refer to this article(12种奇淫异巧) for getting a hang of the different data manipulation techniques in Pandas. 1234567temp1=df['Credit_History'].value_counts(ascending=True)temp2=df.pivot_table(values='Loan_Status',index=['Credit_History'], aggfunc=lambda x:x.map(&#123;'Y':1,'N':0&#125;).mean())print("Frequency Table for Credit History:")print(temp1)print("\nProbility of getting loan for each Credit History class:")print(temp2) Frequency Table for Credit History: 0.0 89 1.0 475 Name: Credit_History, dtype: int64 Probility of getting loan for each Credit History class: Loan_Status Credit_History 0.0 0.078652 1.0 0.795789 1234567891011temp1.plot(kind='bar')plt.xlabel('Credit_History')plt.ylabel('Count of Applicants')plt.title('Applicants by Credit_History')temp2.plot(kind='bar')plt.xlabel('Credit_History')plt.ylabel('Probability of getting loan')plt.title('Probability of getting loan by credit history')plt.show() This shows that the chances of getting a loan are eight-fold if the applicant has a valid credit history.(在有Credit_History的人获得贷款的概率是无Credit_History的8倍)You can plot similar graphs by Married, Self-Employed, Property_Area, etc. Alternately, these two plots can also be visualized by combining them in a stacked chart:: 123temp3=pd.crosstab(df['Credit_History'],df['Loan_Status'])temp3.plot(kind='bar',stacked=True,color=['red','blue'],grid=False)plt.show() You can also add gender into the mix. If you have not realized already, we have just created two basic classification algorithms here, one based on credit history, while other on 2 categorical variables (including gender). You can quickly code this to create your first submission on AV Datahacks. We just saw how we can do exploratory analysis in Python using Pandas. I hope your love for pandas (the animal) would have increased by now – given the amount of help, the library can provide you in analyzing datasets. Next let’s explore ApplicantIncome and LoanStatus variables further, perform data munging and create a dataset for applying various modeling techniques. I would strongly urge that you take another dataset and problem and go through an independent example before reading further.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习书单【转】]]></title>
    <url>%2F2017%2F07%2F13%2FPython%E5%AD%A6%E4%B9%A0%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[In my fifteen years as a Python developer and author, one question seems to come up over and over again: “Can anyone recommend a good book on Python?” To book authors, this is pretty much a conflict-of-interest question. “Why, mine!” is what we would all say. The problem is that there is no correct answer. The right answer depends on the reader’s level and skillset, as well as the style of learning that they find most compatible. This means the best book is different for different people. As lofty as it may sound, my main goal is the good of the community. If my book is the right one for certain classes of users, that’s great. If not, I’m happy to recommend others which may be a better fit. Before we get started, I’ve have one comment on another popular question: Python 2 or Python 3? While Python 3 has been around for more than 3 years, many libraries and packages have been ported, and a new 3.3 release forthcoming late summer, most of the world still runs on Python 2. If you have no old software to port and are getting into Python now, you can start with 3.x. If you have existing code that still runs under 2.x, start there, because the changes are mostly cosmetic (albeit backwards-incompatible). Once you learn one, you can get up-to-speed with the other quickly. Now let’s get reading! In this article, I’ll go over three different reading lists for three different audiences. The first audience is existing programmers who need to learn Python. For Programmers New to Python Dive into Python 3 by Mark Pilgrim, Apress Core Python Programming by Wesley Chun, Prentice Hall The Quick Python Book by Naomi Ceder, Manning Beginning Python: From Novice to Professional by Magnus Lie Hetland, Apress Learn Python the Hard Way by Zed Shaw by Mark Pilgrim, Apress, 2009 One of the most popular Python books has been Dive into Python. Originally published in 2004, a new version for Python 3 was published in 2009. For developers who prefer to learn by just diving into code, this is one of your top choices, even more so since the author is one of my co-workers! However, if you prefer to learn a lot more before venturing into programming, there are other options for you.. by Wesley Chun, Prentice Hall, 2006 Core Python Programming is pretty much the opposite of Dive into Python. Instead of a “quick dive,” I would call it a deep dive into the Python language. The goal of this book is to teach you Python as quickly but as comprehensively as possible. There are plenty of code samples to look at and try during your reading, so you don’t have to read that much before getting started. Even better are the exercises at the end of every chapter to help you put what you learned into practice. Furthermore, a healthy dose of charts and tables provides reference material for readers. In 2009, I added two new appendices on Python 2.6 (also applicable for 2.7) and 3.x to keep the book contemporary. These appendices are available in the 5th and newer printings. All other readers can download both appendices as well as a cleaned up index on the book’s Web site at corepython.com. second edition by Naomi Ceder, Manning, 2010 The Quick Python Book is similar to Dive into Python, but originally published well before the latter except that it goes into a bit more detail than Dive. Its reviews are just as good as its newer brother. A few years ago, it was updated to Python 3. second edition by Magnus Lie Hetland, Apress, 2008 The Beginning Python book also goes more into detail than Apress’ fellow book, Dive into Python. It is very readable and user-friendly; however, like the Quick Python Book, it doesn’t dive in as deeply as Core Python Programming. It’s just right in the middle and thus could be your cup of tea. There’s even a companion Web site for the book. third edition by Zed Shaw, 2013 This series takes a completely different approach: the author forces you to code and code correctly, then explains what you did and why. But since you already had to experience it, you pick up programming skills more quickly. This book is also suitable for those who have never programmed before, and is “brutally-friendly” for these readers. ReferencesThe final list of books to look at are the references–those companion tomes that you should also have on your shelf. The best ones are those that you can just pull off the shelf, look up something, then put away. Python Essential Reference fourth edition by David Beazley, Addison-Wesley Python in a Nutshell second edition by Alex Martelli, O’Reilly Python Cookbook third edition by David Beazley and Brian K. Jones, O’Reilly Python Standard Library by Example by Doug Hellmann, Addison-Wesley Python Essential Reference fourth edition by David Beazley, Addison-Wesley, 2009 The first book in this list is the classic “PER” (Python Essential Reference). It was the very first one (at least its original edition was). Back in the Python 1.5 days, the only real reference Python programmers had was the Standard Library Reference online documentation. Printing it out was enormous: about an inch thick double-sided! Developers craved a “library reference to take home.” Python Essential Reference alleviated that need and represented exactly that: a small, portable version of the library reference. It has since been updated regularly by jazz musician and mad (computer) scientist, David Beazley. Python in a Nutshell second edition by Alex Martelli, O’Reilly, 2006 Several years later, a second reference book came out, this one from O’Reilly as part of their classic Nutshell reference series written by the incomparable Alex Martelli, another co-worker of mine. Both the Nutshell and PER references are written by luminaries in the Python world, and both books are similar. The best suggestion I can offer you is to flip through several pages of both and find which writing style suits you best. Python Cookbook third edition by David Beazley and Brian K. Jones, O’Reilly, 2013 The final two books are not references as much as the first two, but they are still references to consider if you want to go beyond the pure lookup reference guides. This book is based on the online Python Cookbook, a series of “recipes” that are Python snippets of code that “do something.” You can find all the recipes here at http://code.activestate.com/recipes/langs/python, but the book contains “the best” ones, plus additional commentary by the editors. Python Standard Library by Example by Doug Hellmann, Addison-Wesley, 2011 If you can imagine one of the earlier references along with many more code samples, you’ll arrive here. Rather than covering every single module and package in the standard library, this book takes the most popular ones that are used by developers today. It is based on the popular blog series, PyMotW (Python Module of the Week), maintained by the author himself. The Next StepWhat do you do after you’ve learned Python? You may have read my book or others like Dive into Python, Beginning Python, or Learning Python and have written some basic tools/apps. However, to go to the next level, you have nowhere to turn other than dive deeply into specific topics with books about game programming, databases, graphics/multimedia, GUIs, scientific programming, networking, etc. There are definitely books on advanced topics such as these and more, don’t get me wrong. But if you want to develop more than one of these skills, you’d have to buy a book on every topic of interest. This is overkill, especially if you’re only looking to expand your skillset. In that case, you’ll want to reach for this book: Core Python Applications Programming by Wesley Chun, Prentice Hall, 2012 Those of you who have read Core Python Programming will recognize much of the material in this book, because it comes from Part II of Core Python Programming. In the original book, I felt I had done a good enough job of teaching Python to readers, but didn’t have the room to get into any details about what you could build with it. Now that that material has expanded beyond the borders of an introductory book, it was time to split out this intermediate/advanced material into its own volume. Thus I am pleased to announce that those chapters have been extracted to form their own book, Core Python Applications Programming! The contents have been cleaned up and retrofitted with Python 3 examples paired w/their 2.x friends as a hybrid to help you learn both 2.x &amp; 3.x. There is plenty of new material added to existing chapters, as well as completely brand new ones on Web framework development using Django, an introduction to cloud computing with Google App Engine, and text processing with CSV, JSON, and XML. The purpose is to provide comprehensive introductions to each of these areas of application development, hence the title. I hope you are as excited about the new book as I am! Missing Lists?As some astute readers have noticed, I’m missing one or two lists I would have liked to include. One is a list of advanced Python books. This category is, unfortunately, pretty small as there are just a handful of them, including Expert Python Programming and Pro Python. Which of these two or other advanced books have you read, which have you liked and why? The other missing list I’d like to consider is that for scientific computing in Python. Send me your favorites and why. I can be reached via Google+ (+wescpy) or Twitter (@wescpy). ConclusionNow that you’ve seen a variety of ways to address that question of which book with the possible correct answer, we hope that you’re able to use this article to get the right Python book(s) for you. While I hope you find that my books best fit your needs, I’m more happy that you get what you need to build great applications in Python. If you’re new to Python, welcome to our family!**]]></content>
      <categories>
        <category>book</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学的完整学习路径【转】]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%9A%84%E5%AE%8C%E6%95%B4%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[从Python菜鸟到Python Kaggler的旅程。 假如你想成为一个数据科学家，或者已经是数据科学家的你想扩展你的技能，那么你已经来对地方了。本文的目的就是给数据分析方面的Python新手提供一个完整的学习路径。该路径提供了你需要学习的利用Python进行数据分析的所有步骤的完整概述。如果你已经有一些相关的背景知识，或者你不需要路径中的所有内容，你可以随意调整你自己的学习路径，并且让大家知道你是如何调整的。 步骤0：热身 开始学习旅程之前，先回答第一个问题：为什么使用Python？或者，Python如何发挥作用？观看DataRobot创始人Jeremy在PyCon Ukraine 2014上的30分钟演讲，来了解Python是多么的有用。 步骤1：设置你的机器环境 现在你已经决心要好好学习了，也是时候设置你的机器环境了。最简单的方法就是从Continuum.io上下载分发包Anaconda。Anaconda将你以后可能会用到的大部分的东西进行了打包。采用这个方法的主要缺点是，即使可能已经有了可用的底层库的更新，你仍然需要等待Continuum去更新Anaconda包。当然如果你是一个初学者，这应该没什么问题。 如果你在安装过程中遇到任何问题，你可以在这里找到不同操作系统下更详细的安装说明。 步骤2：学习Python语言的基础知识 你应该先去了解Python语言的基础知识、库和数据结构。Codecademy上的Python课程是你最好的选择之一。完成这个课程后，你就能轻松的利用Python写一些小脚本，同时也能理解Python中的类和对象。 具体学习内容：列表Lists，元组Tuples，字典Dictionaries，列表推导式，字典推导式。任务：解决HackerRank上的一些Python教程题，这些题能让你更好的用Python脚本的方式去思考问题。 替代资源：如果你不喜欢交互编码这种学习方式，你也可以学习谷歌的Python课程。这个2天的课程系列不但包含前边提到的Python知识，还包含了一些后边将要讨论的东西。 步骤3：学习Python语言中的正则表达式 你会经常用到正则表达式来进行数据清理，尤其是当你处理文本数据的时候。学习正则表达式的最好方法是参加谷歌的Python课程，它会让你能更容易的使用正则表达式。 任务：做关于小孩名字的正则表达式练习。 如果你还需要更多的练习，你可以参与这个文本清理的教程。数据预处理中涉及到的各个处理步骤对你来说都会是不小的挑战。 步骤4：学习Python中的科学库—NumPy, SciPy, Matplotlib以及Pandas 从这步开始，学习旅程将要变得有趣了。下边是对各个库的简介，你可以进行一些常用的操作： •根据NumPy教程进行完整的练习，特别要练习数组arrays。这将会为下边的学习旅程打好基础。 •接下来学习Scipy教程。看完Scipy介绍和基础知识后，你可以根据自己的需要学习剩余的内容。 •这里并不需要学习Matplotlib教程。对于我们这里的需求来说，Matplotlib的内容过于广泛。取而代之的是你可以学习这个笔记中前68行的内容。 •最后学习Pandas。Pandas为Python提供DataFrame功能（类似于R）。这也是你应该花更多的时间练习的地方。Pandas会成为所有中等规模数据分析的最有效的工具。作为开始，你可以先看一个关于Pandas的10分钟简短介绍，然后学习一个更详细的Pandas教程。 您还可以学习两篇博客Exploratory Data Analysis with Pandas和Data munging with Pandas中的内容。 额外资源： •如果你需要一本关于Pandas和Numpy的书，建议Wes McKinney写的“Python for Data Analysis”。 •在Pandas的文档中，也有很多Pandas教程，你可以在这里查看。 任务：尝试解决哈佛CS109课程的这个任务。 步骤5：有用的数据可视化 参加CS109的这个课程。你可以跳过前边的2分钟，但之后的内容都是干货。你可以根据这个任务来完成课程的学习。 步骤6：学习Scikit-learn库和机器学习的内容 现在，我们要开始学习整个过程的实质部分了。Scikit-learn是机器学习领域最有用的Python库。这里是该库的简要概述。完成哈佛CS109课程的课程10到课程18，这些课程包含了机器学习的概述，同时介绍了像回归、决策树、整体模型等监督算法以及聚类等非监督算法。你可以根据各个课程的任务来完成相应的课程。 额外资源： •如果说有那么一本书是你必读的，推荐Programming Collective Intelligence。这本书虽然有点老，但依然是该领域最好的书之一。 •此外，你还可以参加来自Yaser Abu-Mostafa的机器学习课程，这是最好的机器学习课程之一。如果你需要更易懂的机器学习技术的解释，你可以选择来自Andrew Ng的机器学习课程，并且利用Python做相关的课程练习。 •Scikit-learn的教程 任务：尝试Kaggle上的这个挑战 步骤7：练习，练习，再练习 恭喜你，你已经完成了整个学习旅程。 你现在已经学会了你需要的所有技能。现在就是如何练习的问题了，还有比通过在Kaggle上和数据科学家们进行竞赛来练习更好的方式吗？深入一个当前Kaggle上正在进行的比赛，尝试使用你已经学过的所有知识来完成这个比赛。 步骤8：深度学习 现在你已经学习了大部分的机器学习技术，是时候关注一下深度学习了。很可能你已经知道什么是深度学习，但是如果你仍然需要一个简短的介绍，可以看这里。 我自己也是深度学习的新手，所以请有选择性的采纳下边的一些建议。deeplearning.net上有深度学习方面最全面的资源，在这里你会发现所有你想要的东西—讲座、数据集、挑战、教程等。你也可以尝试参加Geoff Hinton的课程，来了解神经网络的基本知识。 附言：如果你需要大数据方面的库，可以试试Pydoop和PyMongo。大数据学习路线不是本文的范畴，是因为它自身就是一个完整的主题。 英文出处：http://www.analyticsvidhya.com 本文转自：http://dataunion.org/9805.html?utm_source=tuicool]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度UNIT]]></title>
    <url>%2F2017%2F07%2F10%2FBaiduUNIT%2F</url>
    <content type="text"><![CDATA[前几天看到百度在宿舍楼下贴的海报，“百度之星”，2017年比赛分为开发者大赛和程序设计大赛。开发者大赛是要求基于百度UNIT平台设计开发一个对话系统的智能硬件或APP等都可以。主要目的应该是推广UNIT平台。 UNIT（Understanding and Interaction Technology），即理解与交互技术，它是建立在百度多年积累的自然语言处理与对话技术以及大数据的基础上，面向第三方开发者提供的对话系统开发平台. 从介绍上看，百度的出发点是挺好的，之前做过智能镜子，想加入语音交互功能，但是没有相应中文API提供，很蛋疼！ 随着AI技术和理念的兴起，很多产品都希望采用对话式的人机交互方式。然而对话系统的研发对于大多数开发者而言却是一个很困难的工作，对技术和数据的要求都很高。 为此，百度将积累多年的自然语言理解与交互技术对外开放，将积累多年的自然语言理解与交互技术对外开放，推出了可定制的对话系统开发UNIT（Understanding and Interaction Technology），将业界领先的技术能力输出给广大的开发者，以便降低对话系统的研发门槛。 1、UNIT能做什么 帮开发者打造“面向任务的理解与交互能力”，可以解答用户的某些问题（天气、新闻、快递等生活服务）、执行用户指令（开空调等）、通过一系列交互引导用户达到某项需求（通过注册-选座-下单完成订票). 2、UNIT基本概念 （1）场景 一个场景对应一个独立完整的对话系统，用来满足您某个具体业务场景的需求。通常按垂类划分（例如，银行信用卡办理场景、电视遥控器场景等）。 (2)对话单元 在UNIT里，对话单元用来定义系统在一个具体的对话任务下对用户对话的理解、以及机器人的回应方式，是系统中的最小对话单位。对话任务例如查天气、查询信用卡年费、控制空调温度等。 (3)意图 意图表示用户的目的（例如，”北京天气”，意图是查天气）。 (4)词槽 是满足用户意图时的关键信息或限定条件，可以理解为用户需要提供的筛选条件。例如在查询天气时，词槽是地点和时间。 (5)对话样本 对话样本就是您给对话系统做示范，教它在用户说的具体句子里，该如何理解意图，哪个词是重要信息，对应的词槽是什么。 (6)训练模型 即把场景下所有的配置、标注的对话样本、对话模板等打包提交给系统来训练模型。 模型生效一般需要几分钟时间。 (7)沙盒 每个场景都配有一个沙盒环境，将训练好的模型生效到沙盒环境后，就可以进行效果验证了，同时可接入到您自己的业务系统中使用。您可以生成多个模型版本，但只能选择一个放到沙盒环境中。 3、UNIT使用步骤 （a)梳理业务逻辑 （b)配置对话系统 （c)标注对话数据 （d)对话训练与验证 （e)应用调用 相关连接 1、http://astar2017.baidu.com/ 2、http://ai.baidu.com/docs#/UNIT-guide/top]]></content>
      <categories>
        <category>百度语音</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 20 Valid Parentheses]]></title>
    <url>%2F2017%2F07%2F03%2FValidParentheses%2F</url>
    <content type="text"><![CDATA[LeetCode 20.Valid Parentheses问题如下： Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. The brackets must close in the correct order, “()” and “()[]{}” are all valid but “(]” and “([)]” are not. 仔细思考 ，发现用栈来处理是很好做的，但C语言没有栈这种数据类型，只能自己构建了。 typedef struct{ int *base;//栈底指针 int *top;//栈顶指针，非空栈中的栈顶指针始终在栈顶元素的下一个位置 int stackSize;//栈长度 }SqStack;//定义结构体 int Push(SqStack *S,int e) { *(*S).top++ = e;//入栈操作 return 1; } int Pop(SqStack *S, int *e) { *e = *--(*S).top;//出栈 return 1; } int StackEmpty(SqStack S) { return S.top == S.base;//栈空 } bool isValid(char* s) { if(s==NULL) return true; SqStack S; int N = strlen(s); if(N==1) return false; if(N%2) return false; S.base = (int *)malloc(N*sizeof(int));//分配空间 S.top = S.base;//空栈，栈顶指针等于栈底指针 S.stackSize = N;//栈长度，本例中，假设不会超过初始长度 int table[128] = { 10 }; int e=0; table[&apos;(&apos;] = &apos;)&apos;; table[&apos;{&apos;] = &apos;}&apos;; table[&apos;[&apos;] = &apos;]&apos;; table[&apos;]&apos;] = &apos;-&apos;; table[&apos;}&apos;] = &apos;-&apos;; table[&apos;)&apos;] = &apos;-&apos;; for (int i = 0; i &lt; N; i++) { if (!StackEmpty(S)) { Pop(&amp;S, &amp;e); if (table[e] == s[i]) { continue; } else Push(&amp;S, e); } if (table[s[i]] == s[i + 1]) { i++; continue; } else { Push(&amp;S, s[i]); } } int result = StackEmpty(S); free(S.base); return result; }]]></content>
      <categories>
        <category>编程</category>
        <category>C</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>算法</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[世纪三部曲]]></title>
    <url>%2F2017%2F06%2F30%2Ffallofgiants%2F</url>
    <content type="text"><![CDATA[第一部 巨人的陨落 在第一次世界大战的硝烟中，每一个迈向死亡的生命都在热烈地生长——威尔士的矿工少年、刚失恋的美国法律系大学生、穷困潦倒的俄国兄弟、富有英俊的英格兰伯爵，以及痴情的德国特工……从充满灰尘和危险的煤矿到闪闪发光的皇室宫殿，从代表着权力的走廊到爱恨纠缠的卧室，五个家族迥然不同又纠葛不断的命运逐渐揭晓，波澜壮阔地展现了一个我们自认为了解，但从未如此真切感受过的20世纪。 第二部 世界的凛冬 我亲眼目睹，每一个迈向死亡的生命都在热烈地生长。《世界的凛冬》是火遍全球的20世纪人类史诗“世纪三部曲”的第二部，是《巨人的陨落》的续篇。整个20世纪的吉光片羽，都被肯·福莱特写进了这部伟大的小说里。一切都始于那个裂变中的大时代——希特勒上台，爱德华八世退位，原子弹在广岛和长崎爆炸……世界剧烈改变，我该怎么办？这正是他们的困惑——一群处于人生黄金时代的少男少女，来自德国、美国、英国、苏俄和威尔士的五大家族，他们父辈的命运因一战而彻底改变。如今，世界再次破碎，甚至更加暴烈和残酷。然而，这就是他们的时代！在时间的永恒流动中，每个人都在创造历史。所以，为什么不一起来，会一会命运？ 第三部 永恒的边缘 《巨人的陨落》的大结局！我亲眼目睹，每一个迈向死亡的生命都在热烈地生长。“世纪三部曲”终于迎来了一个完美结局。如果说《巨人的陨落》是祖辈的传奇，《世界的凛冬》是父辈的人生，那么，《永恒的边缘》就是新一代的奋斗。真正残酷和激烈的世界大战，是思想的大战。来自美国、德国、苏联、英国和威尔士的五大家族，又一次迎来了新的考验。东西德分裂、柏林墙、苏联秘密警察、刺杀肯尼迪、民权运动、古巴导弹危机、入侵黎巴嫩、弹劾尼克松……此外，第三代生活中还有摇滚、嬉皮士、跨种族婚恋、性解放，以及对过去的误会与和解。说到底，世上只有一种英雄主义，就是在认清生活真相之后，依然热爱生活。 很棒的小说，作者是肯•福莱特（Ken Follett，1949－）现象级畅销小说大师，爱伦坡终身大师奖得主，完全可以当做历史书来读。当年的当年明月写的《明朝那些事儿》,以一种诙谐幽默的手法展现了明朝几百年历史，让人们发现原来历史还可以这么写。当时也是连着一段时间将《明朝那些事儿》一系列书刷完，好像是7本吧，后来过了一段时间又刷了一遍。而肯.福莱特的《世纪三部曲》在我看来和《明朝那些事儿》同样成功，通过几个家族的百年兴衰，将一战、二战及冷战历史描述的非常细致。相比历史教科书，这3部书，在历史中加入了更多的人性，与其说《世纪三部曲》是一本历史长篇小说，更不如说其是一本爱情长篇小说。]]></content>
      <categories>
        <category>book</category>
      </categories>
      <tags>
        <tag>reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chrome浏览器总是崩溃的一种解决办法]]></title>
    <url>%2F2017%2F06%2F28%2Fchromefiex%2F</url>
    <content type="text"><![CDATA[更新过Chrome浏览器后，任何网页都无法打开，包括设置页面！重新安装过好多次，各种版本也都尝试过了，都是崩溃页面。后来在知乎上找到了一种解决办法。 原因在于C:\Windows\System32\drivers\bd0001.sys这个文件，不管用什么方法，只要把该文件移除当前目录或者直接删除，重启电脑，Chrome浏览器就恢复正常咯！ bd0001.sys不知道是不是百度干的，好无耻~]]></content>
      <categories>
        <category>电脑</category>
      </categories>
      <tags>
        <tag>computer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速整数平方根算法]]></title>
    <url>%2F2017%2F06%2F28%2FFastSqrt%2F</url>
    <content type="text"><![CDATA[LeetCode 69.Sqrt(x)问题如下： Implement int sqrt(int x).Compute and return the square root of x. 其实意思就是对于一个32位无符号整数，找到一个16位无符号整数，使满足n^2≤v的n最大。相应的可以写作下式： 关键在于a_i 的确定，使用迭代求导算法，不需要乘法，只简单的使用加法和移位操作。方便起见，定义 则(1)可以表示为 根据参考文章3得到一个非常简便的算法，3ms. 代码如下： int Sqrt(int v) { unsigned long temp, nHat = 0, b = 0x8000, bshft = 15; do{ if (v &gt;= (temp = (((nHat &lt;&lt; 1) + b) &lt;&lt; bshft--))) { nHat += b; v -= temp; } } while (b &gt;&gt;= 1); return nHat; } 参考文章 https://wapwenku.baidu.com/view/53e2d77b168884868762d6fb.html?pn=3&amp;pu=usm@1,sz@320_1002,ta@iphone_2_7.0_2_6.6 http://www.cnblogs.com/nsnow/archive/2010/08/09/1796111.html http://www.azillionmonkeys.com/qed/ulerysqroot.pdf]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[要怎样努力，才能成为很厉害的人]]></title>
    <url>%2F2017%2F06%2F27%2FMy-First-Blog%2F</url>
    <content type="text"><![CDATA[#要怎样努力，才能成为很厉害的人 首先，少年，答应别人的承诺，就一定要兑现。 我以前啊，和你一样，很想成为一个很厉害很厉害很厉害的人。 喜欢看热血的东西，幻想自己是屠龙的勇士，登塔的先锋，我左手有剑，右手有光，没头没脑的燃烧自己，敌人的骑军来了，我说你们何人堪与之战，我的女人在等我。 后来我现实了一点，我觉得我要成为那种说走就走，说日就日的男人，我梳大背头，流浪在欧洲或者新几内亚的，我拍孩子，拍野兽，拍流浪的雏妓，与罗伯特德尼罗握手，说嘿，我给你写了愤怒的公牛2。 再后来，我觉得我人生的梦想，是在城市中心买上一间顶层公寓，把一整面墙都改造成钢化玻璃，在灯火通明的夜晚，我就要端着酒站在巨大的窗前，看整个城市在呼吸，然后我的朋友叩门，他带来了一打嫩模，我们就玩一些成年人的游戏 现在，我发现龙并不存在，我不会骑马，不会用单反，家住2楼，我能做的，就是把眼前的事儿做好，赚到足够的钱，这样我可以给我的姑娘一个地球仪，然后用飞镖扎它，扎到哪儿，就去哪儿玩。 这样看来，虽然我的想法随着生殖器的发育，始终在变，但那个很厉害很厉害的人，一直离我很远，甚至越来越远。 我心中曾经执剑的少年，此刻也混迹在市井之间。 血似乎都凉了。 我也不是没有惶恐过，是不是我这一生，都不能左手持剑，右手握着罗伯特德尼罗，说这里的嫩模随便你玩但是你他妈别从窗户上掉下去。 这样一看，我逊得不行，我的朋友都是一些凡人，比我还逊，业余生活就是推塔、中单、跪。 我心想，我是不是这辈子都要做一个逊逼，直到我的坟墓上写好墓志铭，我甚至都想好了： ###我来，我见，我挂了。最后我给了自己一个否定的答复，我不要。 我喜欢我的朋友们，喜欢我现在的生活，首先我希望你明白，没有厉害与逊逼得区分，只有血的凉与热，有的人觉得生活就这样吧，我算了，现在没什么不好。 有的人觉得生活这样挺好，但是我还要更好。 这种只要剧情稍微热血一点就会热泪盈眶的傻逼，已经不多了，一刻也不要停留。 所以现在，我和你不一样了，我仍然想成为一个很厉害很厉害很厉害的人，像我们这种剧情稍微热血一点就会热泪盈眶的傻逼，要好好珍惜自己。 很多人坐下来了，跟你说你不行，说你省点儿心吧，说你请静一静。 汹涌的人群就把你这样的少年淹没了，人群散去的时候，你也不见了，你那些承诺，谁也听不见，这个世界对于你，就再不可能有什么更有趣更漂亮的女朋友。 你就失约了，小逼崽子。 这么跟你说。 虽然随着年龄的增长，我趋于现实，不能像你那样分分钟冲动的燃烧，然而我每时每刻都有想做的事，有想达成的目标。 不排除以后的某一年，我会握着罗伯特德尼罗的手，他说这是你写得吗，愤怒的公牛2，只要他还没死。 故事里拳王拉莫塔忍着伤，他举着铁拳，挥汗如雨，要和命运斗争，他说我怎么能失约呢，我是那个要成为很厉害很厉害的拳王拉莫塔！ 小伙儿，成为很厉害很厉害的人，最重要的，就是要热血，永远也不要让你的血凉下去，你凉下去了，就再也不能找到一个更有趣更漂亮的女友，你就失约了，于是那天她踏梦而来，就成了一个彻头彻尾的笑话。 当有一天你成为你讨厌的那种人，浑浑噩噩，你走在街上，看见那些更有趣更漂亮的女孩，你会不会想起多年以前，你说我答应你，在一个承诺就是永远的年纪。 读书，交友，美容，都不如你这一腔狗血，滚烫，灼人，你要燃上大半辈子，才对得起你现在说的这些话。 我听闻最美的故事，是公主死去了，屠龙的少年还在燃烧。 火苗再小，你都要反复的点燃。 所谓热血的少年，青涩的爱恋，死亡与梦之约。 这么好的故事。 你可别演砸了。 最后我给你点个人建议：1.读书，读到倦，网上有很多方法，但你从来沉不下心看。2.学习，学到疼，网上有很多方法，但你从来沉不下心看。3.开口说话，冷场也要说话，脸皮薄也要说话，挨打也要说话。4.如果你现在不知道做什么，至少你还可以先从做一个牛逼的学生开始。5.更漂亮更有趣的女孩，五年以后再找。6.承诺是鞭子，不是兴奋剂。7.年纪大了，也不要说什么心如死灰。改变自己是非常，非常，非常痛苦的，我能看出来你一腔热血的优点，自然知道你孤僻懒散自以为是的缺点，方法很多，不过我不确定你吃不吃得了苦，我和你共勉吧。在成为最厉害最厉害最厉害的道路上。 完]]></content>
      <categories>
        <category>鸡汤</category>
      </categories>
      <tags>
        <tag>day day up</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迷宫问题]]></title>
    <url>%2F2016%2F07%2F03%2F%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[求迷宫中从入口到出口的所有路径是一个经典的程序设计问题。最简单方法是穷举法，即从某一入口出发，顺着某一方向向前探索，若能走通，则继续往前走；否则沿原路返回，换个方向继续尝试。因此，使用栈结构来处理较为合适。栈结构存储从入口到出口的路径。 比较简单的迷宫，‘+’表示墙壁，不能通过；‘o’表示通道，可以通过；但是从入口到出口路径上不能两次通过同一块通道。 求迷宫中一条路径的算法的基本思想是： 若当前位置“可通”，则入栈“当前路径”，并继续朝下一位置探索，如此重复；若当前位置“不可通”，则应顺着“来向”退回到“前一通道块”，然后朝着除“来向”之外的其他方向继续探索；若该通道块的四周4个方块均“不可通”，则应从“当前路径”上删除该通道块。 C语言代码如下： typedef struct{ int x; int y; }PosType;//坐标 typedef struct{ int ord; PosType seat; int di; }SElemType;//栈基本元素 //---------栈相关操作------------------// #define STACK_INIT_SIZE 100 typedef struct{ SElemType *base; SElemType *top; int stacksize; }SqStack; int InitStack(SqStack &amp;S) { S.base = (SElemType *)malloc(STACK_INIT_SIZE*sizeof(SElemType)); S.top = S.base; S.stacksize = STACK_INIT_SIZE; return 1; } int Push(SqStack &amp;S, SElemType e){ *S.top++ = e; return 1; } int Pop(SqStack &amp;S, SElemType &amp;e) { e = *--S.top; return 1; } int StackEmpty(SqStack S) { return S.top == S.base; } //---------栈相关操作------------------// int maze[10][10] = { {-1,-1,-1,-1,-1,-1,-1,-1,-1,-1}, {-1, 0, 0, 0, 0, 0, 0, 0,-1,-1}, {-1, 0, 0, 0,-1, 0,-1,-1, 0,-1}, {-1,-1,-1, 0,-1, 0, 0,-1, 0,-1}, {-1, 0, 0, 0,-1,-1, 0,-1, 0,-1}, {-1, 0, 0,-1, 0, 0, 0, 0, 0,-1}, {-1, 0, 0,-1, 0, 0,-1,-1, 0,-1}, {-1,-1,-1, 0, 0, 0, 0,-1, 0,-1}, {-1, 0, 0, 0, 0, 0, 0, 0, 0,-1}, {-1,-1,-1,-1,-1,-1,-1,-1,-1,-1} };//迷宫 PosType start = { 1, 1 };//迷宫起点 PosType end = { 8, 8 };//迷宫终点 int Pass(PosType pos)//当前位置是否可通过 { return (maze[pos.x][pos.y] != -1) &amp;&amp; (maze[pos.x][pos.y] != 1); } void FootPrint(PosType pos)//走过的通道块 { maze[pos.x][pos.y] = 1;//走过 } PosType NextPos(PosType pos, int di)//顺时钟方向尝试 { PosType resultPos; switch (di) { case 1://东 resultPos.x = pos.x + 1; resultPos.y = pos.y; break; case 2://南 resultPos.x = pos.x; resultPos.y = pos.y + 1; break; case 3://西 resultPos.x = pos.x - 1; resultPos.y = pos.y; break; case 4://北 resultPos.x = pos.x; resultPos.y = pos.y - 1; break; default: printf(&quot;Error!\n&quot;); } return resultPos; } void MarkPrint(PosType pos)//标记不可通过的位置 { maze[pos.x][pos.y] = -1; } int MazePath(){ SqStack S; InitStack(S);//初始化栈 PosType curpos = start; int curstep = 1; SElemType e; do{ if (Pass(curpos)){ FootPrint(curpos); e.ord = curstep; e.seat.x = curpos.x; e.seat.y = curpos.y; e.di = 1; Push(S, e); if ((curpos.x == end.x) &amp;&amp; (curpos.y == end.y)) {//到达终点，逐一打印栈中保持的路径； while (!StackEmpty(S)) { Pop(S, e); printf(&quot;(%d, %d)\n&quot;, e.seat.x, e.seat.y); } free(S.base); return true; } curpos = NextPos(curpos, 1); curstep++; } else { if (!StackEmpty(S)) { Pop(S, e); while (e.di == 4 &amp;&amp; !StackEmpty(S)){ MarkPrint(e.seat); Pop(S, e); } if (e.di &lt; 4){ e.di++; Push(S, e); curpos = NextPos(e.seat, e.di); } } } } while (!StackEmpty(S)); free(S.base); return false; } 运行结果： 红色是正确路径，蓝色是尝试路径。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>算法</tag>
      </tags>
  </entry>
</search>