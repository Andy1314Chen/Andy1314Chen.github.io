<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习实战第五章学习笔记(Logistic回归)]]></title>
    <url>%2F2017%2F09%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%AC%E4%BA%94%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[回归的概念：假设现在有一些数据点，用一条直线对这些点进行拟合，这个拟合的过程就被称为回归。 利用Logistic回归进行分类的主要思想：根据现有数据对分类边界线建立回归公式，以此进行分类。而拟合过程中，需要寻找最佳拟合参数，得到最佳拟合直线，这就需要一些最优化算法。 Logistic的一般过程： 收集数据：采用任意方法收集数据。 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。 分析数据：采用任意方法对数据进行分析。 训练数据：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。 测试数据：一旦训练步骤完成，分类将会很快。 使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。 基于Logistic回归和Sigmoid函数的分类 优点：计算代价不高，易于理解和实现。 缺点：容易欠拟合，分类精度可能不高。 适用数据类型：数值型和标称型数据。 我们想通过已知数据拟合出一个函数，函数接收所有新的数据输入，然后预测出类别。在二分类情况下，上述函数输出0或1. 最简单的类似函数应该是单位阶跃函数，也被称为海维赛德阶跃函数(Heaviside step function). 而单位阶跃函数存在一个问题是，在跳跃点上，由0直接到1，这种瞬间跳跃过程是比较麻烦的。幸运的是，另外一种函数，Sigmoid函数具有和阶跃函数类似的性质，而且数学上更加易于处理。 说白了，Logistic回归分类器就是，在每个特征上都乘以相应的一个回归系数，然后把所有的结果值相加，并将总和代入Sigmoid函数中，进而可以得到一个范围在0~1之间的数值。结果大于0.5的数据被判定为类别1，小于0.5的数据被判定为类别0. 从这个角度来说，Logistic回归也可以被看作是一种概率估计。 由回归系数可以构成一个函数或者一条曲线(z=w0*x0+w1*x1+w2*x2+…+wn*xn=0），这条曲线位于两个类别的交界处。 那么，现在的关键在于，如何确定上述的最佳回归系数？ 基于最优化方法的最佳回归系数确定Sigmoid函数： 这里以二分类为例，假设输入数据的特征向量是[x0,x1,x2,…,xn],乘以回归系统[w0,w1,w2,…,wn]，累加后代入Sigmoid函数中，输出就是一个0~1的值，用于分类。 梯度上升法梯度上升法的主要思想是：从当前函数的最大梯度方向寻找，以求得到函数的最大值。 梯度上升算法沿着梯度方向移动，总是指向函数值增长最快的方向。 梯度下降算法和梯度上升算法的区别 梯度下降算法中的系数更新公式中，是减号。可以这样理解，梯度下降算法是下山的过程，寻找CostFunction最小的点。而梯度上升算法则是上山的过程，但是无论“上山”还是“下山”过程，走的方向始终是沿着当前函数值变化最快的方向(增长最快就是对应上山，下降最快就是对应下山). 训练算法：使用梯度上升找到最佳参数#梯度上升法的伪代码： 每个回归系数初始化为1 重复R次： 计算整个数据集的梯度 使用α*gradient更新回归系数的向量 返回回归系数 import numpy as np #Logistic回归梯度上升法优化算法 def loadDataSet():#读取样本数据 dataMat=[];labelMat=[] fr=open(&apos;testSet.txt&apos;) for line in fr.readlines(): lineArr=line.strip().split()#除去空格后划分 dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])])#因为只有两个特征 labelMat.append(int(lineArr[2]))#标签 return dataMat,labelMat def sigmoid(inX): return 1.0/(1+np.exp(-inX)) def gradAscent(dataMatIn,classLabels): dataMatrix=np.mat(dataMatIn)#100*3 labelMat=np.mat(classLabels).transpose() m,n=np.shape(dataMatrix)#m是样本个数，n是特征个数 alpha=0.001#步长 maxCycles=500#最大迭代次数 weights=np.ones((n,1))#回归系数初始化为1 for k in range(maxCycles): h=sigmoid(dataMatrix*weights)#m*n * n*1 -&gt; m*1 error=(labelMat-h)# weights=weights+alpha*dataMatrix.transpose()*error return weights #画出数据集和Logistic回归最佳拟合直线的函数 def plotBestFit(wei): import matplotlib.pyplot as plt weights=wei#.getA() dataMat,labelMat=loadDataSet() dataArr=np.array(dataMat) n=np.shape(dataArr)[0] xcord1=[];ycord1=[] xcord2=[];ycord2=[] for i in range(n): if int(labelMat[i])==1: xcord1.append(dataArr[i,1]) ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]) ycord2.append(dataArr[i,2]) fig=plt.figure() ax=fig.add_subplot(111) print(len(xcord1),len(ycord1)) ax.scatter(xcord1,ycord1,s=30,c=&apos;red&apos;,marker=&apos;s&apos;) ax.scatter(xcord2,ycord2,s=30,c=&apos;green&apos;) x=np.arange(-3.0,3.0,0.1) y=(-weights[0]-weights[1]*x)/weights[2] ax.plot(x,y) plt.xlabel(&apos;X1&apos;);plt.ylabel(&apos;X2&apos;); plt.show() 公式推导回归问题中，需要一个代价函数用来评论拟合过程的“好坏”。对于线性回归问题，它的代价函数是估计值与实际值差的平方和来评断拟合的“好坏”，J(θ)越小，估计值和实际值差距越小，拟合效果越好。 那对于Logistic回归也沿用这个代价函数行不行？ 答案是否定的！因为Sigmoid函数带入J(θ)后，J(θ)不是一个凸函数，会存在很多个局部极值点，无法进行评价拟合的效果。有人定义另外一个代价函数： 观察这个函数的特性，当预测准确时(实际值y=1且预测值h=1或者实际值y=0且预测值h=0),cost是接近于0的，而当预测很离谱时(实际值y=1且预测值h=0或者实际值y=0且预测值h=1)，cost趋于∞。当然，中间的过程也是符合要求的。 这个分段函数还可以简写为： 那么对于m条训练样本而言，代价函数就是m个预测代价的累加和： 为了使J(θ)取最小值(这里是取最大值，因为对J(θ)加了一个负号)，对其求偏导： 所以结果就是: weights=weights+alpha*dataMatrix.transpose()*error 是一致的！！ 训练算法：随机梯度上升梯度上升算法在每次更新回归系数时都需要遍历整个数据集(因为代价函数是m个训练样本数据代价的累加和),该方法在处理100个左右的数据集时尚可，但是如果样本数据过多，特征数过多，那么该方法的计算复杂度就更高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。该方法的另外一个优点是，可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，一次处理所有数据被称作是“批处理”。 #随机梯度上升算法伪代码： 所有回归系数初始化为1 对数据集中每个样本 计算该样本的梯度 使用alpha*gradient更新回归系数值 返回回归系数值 #随机梯度上升算法 def stocGradAscent0(dataMatrix,classLabels): m,n=np.shape(dataMatrix) alpha=0.01#步长 weights=np.ones(n)#1*n X=[]#保持weights的历史数据 for j in range(300):#迭代300次 for i in range(m): h=sigmoid(sum(dataMatrix[i]*weights))#1*n * 1*n = error=classLabels[i]-h weights=weights+alpha*error*dataMatrix[i] X.append(weights) return weights 这段代码与梯度上升算法很相似，但是也有些区别： 这里的变量h和error都是数值，而梯度上升算法都是向量(因为这里的是一个样本) 没有矩阵的转换过程，所有变量的数据类型都是NumPy数组 但是有个问题，为什么随机梯度上升算法也可以达到最优呢？书中并没有给出相关解释。 在迭代200次过程中，weights=[X0,X1,X2]变化曲线；而我自己的300次跌打过程中，变化曲线是： X0: X1: X2: 从图中看，在大的波动停止之后，还会有一些小的周期波动。产生这种现象的原因是存在一些不能正确分类的样本点，在每次迭代时会引发系数的剧烈变化。 因为这个算法是将m个训练样本一次代入进行weights的更新的，那么如果从m个样本中随机一个进行训练会不会就可以消除这个周期波动呢? 答案是肯定的，作者称之为改进的随机梯度上升算法。 代码如下： #改进的随机梯度上升算法 def stocGradAscent1(dataMatrix,classLabels,numIter=150): m,n=np.shape(dataMatrix) weights=np.ones(n) X=[] for j in range(numIter): dataIndex=list(range(m)) for i in range(m): alpha=4/(1.0+j+i)+0.01 randIndex=int(np.random.uniform(0,len(dataIndex))) h=sigmoid(sum(dataMatrix[randIndex]*weights)) error=classLabels[randIndex]-h weights=weights+alpha*error*dataMatrix[randIndex] X.append(weights) del dataIndex[randIndex] X=np.array(X) return weights 改进之处有两个方面，每次迭代时调整alpha和随机选取更新。 步长alpha在每次迭代的时候都会调整，这会缓解数据波动和高频波动。另外alpha会随着迭代次数不断减小，但永远不会减小到0，这是因为alpha=4/(1.0+j+i)+0.01,存在一个常数项。必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。如果要处理的问题是动态变化的，那么可以适当加大上述常数项，来确保新的值获得更大的回归系数。另一点值得注意的是，在降低alpha的函数中，alpha每次减少1/(j+i)，其中j是迭代次数，i是样本点的下标。这样当j&lt;&lt;max(i)时，alpha就不是严格下降的。 通过随机选取样本来更新回归系数，可以减少周期性的波动。 示例：从疝气病症预测病马的死亡率使用Logistic回归来预测患有疝气病的马的存活问题。数据集包括368个样本和21个特征。 使用Logistic回归估计马疝气病的死亡率 收集数据：UCI机器学习数据库 准备数据：用Python解析文本文件并填充缺失值 分析数据：可视化并观察数据 训练算法：使用优化算法，找到最佳的系数 测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。 使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果。 (数据集中有30%的值是缺失的) 准备数据：处理数据中的缺失值由于一些不可避免的原因，数据集经常会是缺失不全的，所以必须采用一些方法来解决这个问题： 使用可用特征的均值来填充缺失值； 使用特殊值来填补缺失值，如-1； 忽略有缺失值的样本； 使用相似样本的均值添补缺失值； 使用另外的机器学习算法预测缺失值； 在数据预处理阶段，所有缺失值用实数0代替。原因有二： 根据回归系数的更新公式，若特征值为0，不会对特征系数进行更新； sigmoid(0)=0.5,对结果不具有任何倾向性； 对样本数据中类别标签已经缺失的，应该丢弃。因为类别标签与特征不同，很难确定采用某个合适的值来替代，而且对训练模型影响较大。 测试算法：用Logistic回归进行分类#Logistic回归分类函数 def classifyVector(inX,weights): prob=sigmoid(sum(inX*weights)) if prob&gt;0.5: return 1.0 else: return 0.0 def colicTest(): frTrain=open(&apos;horseColicTraining.txt&apos;) frTest=open(&apos;horseColicTest.txt&apos;) trainingSet=[];trainingLabels=[] for line in frTrain.readlines(): currLine=line.strip().split(&apos;\t&apos;) lineArr=[] for i in range(21):#21个特征 lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) #标签 trainWeights=stocGradAscent1(np.array(trainingSet),trainingLabels,500) errorCount=0;numTestVec=0.0 for line in frTest.readlines(): numTestVec+=1.0 currLine=line.strip().split(&apos;\t&apos;) lineArr=[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(np.array(lineArr),trainWeights))!=int(currLine[21]): errorCount+=1 errorRate=(float(errorCount)/numTestVec) print(&quot;the error rate of this test is: %f&quot; % errorRate) return errorRate def multiTest(): numTests=10;errorSum=0.0 for k in range(numTests): errorSum+=colicTest() print(&quot;after %d iterations the average error rate is: %f&quot; % (numTests,errorSum/float(numTests))) 参考博客 http://blog.csdn.net/lu597203933/article/details/38468303#comments http://blog.csdn.net/CharlieLincy/article/details/70767791#comments]]></content>
      <categories>
        <category>Python</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战第四章学习笔记(朴素贝叶斯分类器)]]></title>
    <url>%2F2017%2F09%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[之前两章，k-NN近邻算法和决策树算法构建的分类器都是对测试数据给出一种具体的类别，即‘非黑即白’。而朴素贝叶斯分类器会给出每个类别的概率，将概率最大的类别作为最优类别猜测结果。 基于贝叶斯决策理论的分类方法 优点：在数据较少的情况下仍然有效，可以处理多类别问题。 缺点：对于输入数据的准备方式较为敏感 适用数据类型：离散型 贝叶斯决策理论举个简单的小例子： 假设有一个数据集，每条数据样本都有两个特征(x,y)和一个类别标签(0,1).目标是把一条新的数据贴上准确的标签。 用p1(x,y)表示数据(x,y)属于类别1的概率，用p2(x,y)表示数据(x,y)属于类别2的概率。那么，用条件概率公式来表示这两个概率，就分别是P(c1|x,y)和P(c2|x,y),具体意义就是：由x和y这两个属性值确定的数据，其属于类别1、类别2的概率。 由贝叶斯定理，可得到： p(x,y|ci):基于一个样本数据集，在ci这个类中，数据(x,y)出现的概率。 p(ci):基于这个样本数据集，属于ci这个类别的数据，占所有数据的数量。 p(x,y):在所有样本数据中，数据(x,y)出现的概率。 用具体的数字说明一下： 该样本数据集中共有20个样本，标签为c1的为15个，标签c2的为5个，(X0,Y0)是该样本集中独特的一个，标签为c1。 则p(X0,Y0|c1)=1/15, p(c1)=2/3, p(X0,Y0)=1/20 对于数据集中任意一个样本数据(x,y)来说，用p1(x,y)表示数据点(x,y)属于类别1的概率，用p2(x,y)表示数据点(x,y)属于类别2的概率，那么对于一个新数据点(x,y)，可以用下面的规则判断它的类别： 如果p1(x,y)=p(c1|x,y) &gt; p2(x,y)=p(c2|x,y),那么类别为1. 如果p1(x,y)=p(c1|x,y) &lt; p2(x,y)=p(c2|x,y),那么类别为2. 简单的讲，我们会选择高概率对应的类别，这也是贝叶斯决策理论的核心思想。 使用朴素贝叶斯进行文档分类朴素贝叶斯的一般过程： 收集数据：可以使用任何方法，这里使用RSS源 准备数据：需要数值型或者布尔型数据。 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。 训练算法：计算不同的独立特征的条件概率。 测试算法：计算错误率。 使用算法：一个常见的朴素贝叶斯应用是文档分类。 由统计学知识： 如果每个特征需要N个样本，那么对于10个特征将需要N^10个样本，对于包含1000个特征的词汇表将需要N^1000个样本。 如果特征之间相互独立，那么样本数就可以从N^1000减少到1000N. 那么什么是独立呢？？* 所谓独立指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。 而这正是朴素贝叶斯分类器的一个假设，即特征之间相互统计独立。朴素贝叶斯分类器的另一个假设是，每个特征同等重要。 而这两个天真的假设正是朴素贝叶斯分类器中朴素(naive)一词的来源。这个假设虽然看起来很不靠谱，但在实际运用的时候，往往能达到比较好的效果。 朴素贝叶斯分类器通常有两种实现方式：一种基于贝努力模型实现，一种基于多项式模型实现。区别在于，是否考虑词在文档中出现的次数。 使用Python进行文本分类首先给出将文本转换为数字向量的过程，然后介绍如何基于这些向量来计算条件概率，并在此基础上构建分类器，最后还要介绍一些利用Python实现朴素贝叶斯过程中需要考虑的问题。 从文本中构建词向量#词表到向量的转换函数 def loadDataSet(): postingList=[[&apos;my&apos;,&apos;dog&apos;,&apos;has&apos;,&apos;flea&apos;,&apos;problems&apos;,&apos;help&apos;,&apos;please&apos;], [&apos;maybe&apos;,&apos;not&apos;,&apos;take&apos;,&apos;him&apos;,&apos;to&apos;,&apos;dog&apos;,&apos;park&apos;,&apos;stupid&apos;], [&apos;my&apos;,&apos;dalmation&apos;,&apos;is&apos;,&apos;so&apos;,&apos;cute&apos;,&apos;I&apos;,&apos;love&apos;,&apos;him&apos;], [&apos;stop&apos;,&apos;posting&apos;,&apos;stupid&apos;,&apos;worthless&apos;,&apos;garbage&apos;], [&apos;mr&apos;,&apos;licks&apos;,&apos;ate&apos;,&apos;my&apos;,&apos;steak&apos;,&apos;how&apos;,&apos;to&apos;,&apos;stop&apos;,&apos;him&apos;], [&apos;quit&apos;,&apos;buying&apos;,&apos;worthless&apos;,&apos;dog&apos;,&apos;food&apos;,&apos;stupid&apos;]] classVec=[0,1,0,1,0,1] #1，代表侮辱性文字，0，代表正常言论 return postingList,classVec #创建词汇表 def createVocabList(dataSet): vocabSet=set([]) for document in dataSet: vocabSet=vocabSet|set(document) return list(vocabSet) #将文本转换为向量 #依据词汇表中单词的有无，输出向量相应位置置1或置0 def setOfWords2Vec(vocabList,inputSet): returnVec=[0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)]=1 else: print(&quot;the word: %s is not in my Vocabulary!&quot; % word) return returnVec 第一个函数创建的是数据样本，第二个函数合并所有样本文档，生成一个词汇表，这个不重复词表可看做是一个一维向量。第三个函数，根据样本文档，参考词汇表向量，若样本文档中含有某些词汇，则输出向量相应位置会置1. 训练算法：从词向量计算概率 将之前的x,y替换为w,w在这里是一个词向量。 这个公式，等号右边的p(ci)是最好计算的，用类别i中文档树除以总的文档数，就可以得到类别ci的概率。接着计算p(w|ci),这里使用朴素贝叶斯假设，如果将w展开为一个个独立特征，则p(w|ci)等于p(w0,w1,w2,…,wN|ci),即等于p(w0|ci)p(w1|ci)p(w2|ci)…p(wN|ci). #基于条件独立性假设 #朴素贝叶斯分类器训练函数 def trainNB0(trainMatrix,trainCategory): numTrainDocs=len(trainMatrix)#训练样本数量 numWords=len(trainMatrix[0])#每个训练样本的特征数 pAbusive=sum(trainCategory)/float(numTrainDocs)#侮辱性概率 p0Num=np.ones(numWords);p1Num=np.ones(numWords) p0Denom=2.0;p1Denom=2.0 for i in range(numTrainDocs): if trainCategory[i]==1: p1Num+=trainMatrix[i]#标签为abusive的文档中，词汇表的单词出现过多少次 p1Denom+=sum(trainMatrix[i])#到目前为止，标签为abusive的文档中出现过单词的总量 else: p0Num+=trainMatrix[i]#到目前为止，标签为normal的文档中，词汇表的单词出现过多少次 p0Denom+=sum(trainMatrix[i])#到目前为止，标签为normal的文档中出现过单词的总量 p1Vect=np.log(p1Num/p1Denom)#标签为abusive的文档中，词汇的各个单词的出现概率 #计算的是p(w*j*|c*i*) p0Vect=np.log(p0Num/p0Denom)#取自然对数，防止下溢 return p0Vect,p1Vect,pAbusive 函数的伪代码如下： 计算每个类别中的文档数目 对每篇训练文档： 对每个类别： 如果词条出现文档中--&gt;增加该词条的计数值 增加所有词条的计数值 对每个类别： 对每个词条： 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率 函数返回结果是： #类别0的各个词条条件概率 p0Vect=[p(w0|c*0*),p(w1|c*0*),p(w2|c*0*),...,p(wN|c*0*)] #类别1的各个词条条件概率 p1Vect=[p(w0|c*1*),p(w1|c*1*),p(w2|c*1*),...,p(wN|c*1*)] #类别为Abusive的概率 pAbusive 测试算法：根据现实情况修改分类器利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别，即计算p(w0|ci)p(w1|ci)p(w2|ci)。若其中一个概率值为0，那么最后的乘积也为0.为降低这种影响，可以将所有词的出现次数初始化为1，并将分母初始化为2.（因为要计算的是概率，若样本词条数较大的话，这样处理对结果的影响并不是很大）。 另一个问题，下溢出。当计算p(w0|ci)p(w1|ci)p(w2|ci)…p(wN|ci)时，可能大部分因子都是非常小的，所以程序会产生下溢出或者得不到正确答案（结果非常接近0，计算机精度有限，而直接将其近似为0）。一种解决办法是对乘积取自然对数，ln(a\b)=ln(a)+ln(b)*. 因为最终判决时，是比较两个类别概率的大小，对他们取自然对数并不影响他们的相对大小关系。 #朴素贝叶斯分类器 def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): p1=sum(vec2Classify*p1Vec)+np.log(pClass1) #因为取自然对数，所以原来的乘积运算转换为加法运算 p0=sum(vec2Classify*p0Vec)+np.log(1.0-pClass1) if p1&gt;p0: return 1 else: return 0 def testingNB(): listOPosts,listClasses=loadDataSet() myVocabList=createVocabList(listOPosts)#词汇表 trainMat=[] for postinDoc in listOPosts:#获取训练样本集 trainMat.append(setOfWords2Vec(myVocabList,postinDoc)) p0V,p1V,pAb=trainNB0(np.array(trainMat),np.array(listClasses))#训练 testEntry=[&apos;love&apos;,&apos;my&apos;,&apos;dalmation&apos;] thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry)) print(testEntry,&apos;classified as: &apos;,classifyNB(thisDoc,p0V,p1V,pAb)) testEntry=[&apos;stupid&apos;,&apos;garbage&apos;] thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry)) print(testEntry,&apos;classified as: &apos;,classifyNB(thisDoc,p0V,p1V,pAb)) 做个简单的小结： 首先将文档合并提取出一个不重复的词汇表，对每个文档依据这个不重复词汇表转换为一维向量，向量的长度就是词汇表的个数。接下来，根据文档的类别标签，计算p(wj|ci),计算过程是：在ci类别中，词条wj出现的次数除以ci中总词条数目。得到p(wj|ci)后，基于朴素贝叶斯分类器的假设，可以计算p(w|ci).而p(ci)可以很容易求出来。然后比较p(w|c0)p(c0)和p(w|c1)p(c1)的大小，就可以判定属于哪个类别。 文档词袋模型目前为止，我们是将每个词的出现与否作为一个特征的，这可以被描述为词集模型（set-of-words model).如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为词袋模型(bag-of-words model).显然，词条出现的次数应该也要被考虑在内，因此，将函数setOfWords2Vec()修改为bagOfWords2Vec(). #词集模型--》词袋模型 #朴素贝叶斯词袋模型 def bagOfWords2VecMN(vocabList,inputSet): returnVec=[0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)]+=1 return returnVec 示例：使用朴素贝叶斯过滤垃圾邮件 收集数据：提供文本文件 准备数据：将文本文件解析成词条向量 分析数据：检查词条确保解析的正确性 训练算法：使用我们之前建立的trainNB0()函数 测试算法：使用classifyNB(),并且构建一个新的测试函数来计算文档集的错误率 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。 测试算法：使用朴素贝叶斯进行交叉验证#文件解析及完整的垃圾邮件测试函数 def textParse(bigString): import re listOfTokens=re.split(r&apos;\W*&apos;,bigString) return [tok.lower() for tok in listOfTokens if len(tok)&gt;2] #垃圾邮件分类测试 def spamTest(): docList=[];classList=[];fullText=[] for i in range(1,26): wordList=textParse(open(&apos;email/spam/%d.txt&apos; % i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(open(&apos;email/ham/%d.txt&apos; % i).read()) #第23个文件有些问题，需要修改 docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=createVocabList(docList)#构建词汇表 trainingSet=list(range(50));testSet=[] for i in range(10):#随机挑选10个邮件作为测试集 randIndex=int(np.random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat=[];trainClasses=[] for docIndex in trainingSet: trainMat.append(setOfWords2Vec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses))#进行训练 errorCount=0 for docIndex in testSet: wordVector=setOfWords2Vec(vocabList,docList[docIndex]) if classifyNB(np.array(wordVector),p0V,p1V,pSpam)!= classList[docIndex]: errorCount+=1 print(&apos;the error rate is: &apos;,float(errorCount)/len(testSet)) 示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向 收集数据：从RSS源收集内容，这里需要对RSS源构建一个借口 准备数据：将文本文件解析成词条向量 分析数据：检查词条确保解析的正确性 训练算法：使用我们之前建立的trainNB0()函数 测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。 使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。 收集数据：导入RSS源安装feedparser包 pip install feedparser #RSS源分类器及高频词去除函数 def calcMostFreq(vocabList,fullText): import operator freqDict={} for token in vocabList: freqDict[token]=fullText.count(token) sortedFreq=sorted(freqDict.items(),key=operator.itemgetter(1),\ reverse=True) return sortedFreq[:30] #返回前30个高频词条 def localWords(feed1,feed0): import feedparser docList=[];classList=[];fullText=[] minLen=min(len(feed1[&apos;entries&apos;]),len(feed0[&apos;entries&apos;]))#帖子数量 for i in range(minLen): wordList=textParse(feed1[&apos;entries&apos;][i][&apos;summary&apos;])#帖子内容解析 docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(feed0[&apos;entries&apos;][i][&apos;summary&apos;]) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=createVocabList(docList)#创建词汇表 top30words=calcMostFreq(vocabList,fullText) for pairW in top30words: if pairW[0] in vocabList: vocabList.remove(pairW[0])#去除出现次数最高的那些词 trainingSet=list(range(2*minLen));testSet=[] for i in range(int(minLen/10)):#选取10%的数据，建立测试集 randIndex=int(np.random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat=[];trainClasses=[] for docIndex in trainingSet: trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses)) errorCount=0 for docIndex in testSet: wordVector=bagOfWords2VecMN(vocabList,docList[docIndex]) if classifyNB(np.array(wordVector),p0V,p1V,pSpam) !=\ classList[docIndex]: errorCount+=1 print(&apos;the error rate is: &apos;,float(errorCount)/len(testSet)) return vocabList,p0V,p1V 这个分类器的作用是给出一个帖子，判断它是来自哪个地区的（东部还是西部，因为两地的语言用词可能不太一样）。前30个高频词去掉的原因是，一般很多词都是虚词，如“maybe”、”yes”、“will”、”can”等等。当然这样处理是比较粗糙的，一般会根据停用词表来进行处理。 #最具表征性的词汇显示函数 def getTopWords(ny,sf): import operator vocabList,p0V,p1V=localWords(ny,sf) topNY=[];topSF=[] for i in range(len(p0V)): if p0V[i]&gt;-6.0:topSF.append((vocabList[i],p0V[i])) if p1V[i]&gt;-6.0:topNY.append((vocabList[i],p1V[i])) sortedSF=sorted(topSF,key=lambda pair:pair[1],reverse=True) print(&quot;SF**&quot;*10) for item in sortedSF: print(item[0]) sortedNY=sorted(topNY,key=lambda pair:pair[1],reverse=True) print(&quot;NY**&quot;*10) for item in sortedNY: print(item[0]) Craigslist这个网站是分地区的，比如纽约（New York，美国东部）和旧金山（San Francisco，美国西部）。我们从这两个地区的Ctaigslist里面选取一些帖子，通过分析这些帖子里的征婚广告信息，来比较这两个城市的人们在广告用词上是否存在差异。如果确实存在差异，那么两个地区的人各自常用的词是哪些？ 参考博客 《机器学习实战》第四章：朴素贝叶斯（2）两个实例]]></content>
      <categories>
        <category>Python</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战第三章学习笔记(决策树-Decision Tree)]]></title>
    <url>%2F2017%2F09%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%89%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[决策树(Decision Tree)的概念基本思想：决策树是一种树结构，其中的每个内部节点代表对某一特征的一次测试，每条边代表一个测试结果，叶节点代表某个类或类的分布。决策树的决策过程需要从决策树的根节点开始，待测试数据与决策树中的特征节点进行比较，并按照比较结果选择下一个比较分支，直到叶子节点作为最终的决策结果。 决策树算法是一个分类算法，较k-近邻算法(无法给出数据的内在含义)的主要优势在于数据形式非常容易理解。 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过度匹配问题。 适用数据类型：数值型和标称型 决策树的构造构造决策树时，首先面临的问题是，当前数据集上哪个特征在划分数据分类时起决定性作用。根据决定性的特征，进行划分，这样原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。直到所有具有相同类型的数据均在一个数据子集内。 决策树的一般流程 收集数据：可以使用任何方法。 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。 分析数据：可以使用任何方法，构造树完成之后，应该检查图形是否符合预期。 训练算法：构造树的数据结构。 测试算法：使用经验树计算错误率 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 这里使用ID3算法划分数据集，相关算法还有C4.5算法等。 信息增益划分数据集的大原则是：将无序的数据变得更加有序 ？(数据的无序和有序如何度量呢，用熵！) 在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择，就是起决定性作用的特征！ 集合信息的度量方式被称为香农熵或简称熵，来源于信息论之父克劳德.香农。 克劳德.香农是被公认是20世纪最聪明的人之一，威廉.庞德斯通在2005年出版的《财富公式》一书中是这样描述克劳德.香农的: “贝尔实验室和MIT有很多人将香农和爱因斯坦相提并论，而其他人则认为这种对比是不公平的—对香农是不公平的。” 信息增益(information gain)和熵(entropy) 熵是指信息的期望值，信息增益是数据集划分之前的熵减去划分之后的熵。 如果待分类事务可能划分在多个分类之中，假设Xi是其中的一个类，则符号Xi的信息定义为： p(Xi)是选择该分类的概率。 为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到： 其中，n是分类的数目。 from math import log import operator #计算给定数据集的香农熵 def calcShannonEnt(dataSet): numEntries=len(dataSet) labelCounts={} for featVec in dataSet: currentLabel=featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel]=0 labelCounts[currentLabel]+=1 shannonEnt=0.0 for key in labelCounts: prob=float(labelCounts[key])/numEntries shannonEnt-=prob*log(prob,2.0) return shannonEnt #创建简单数据集 def createDataSet(): dataSet=[[1,1,&apos;yes&apos;], [1,1,&apos;yes&apos;], [1,0,&apos;no&apos;], [0,1,&apos;no&apos;], [0,1,&apos;no&apos;]] labels=[&apos;no surfacing&apos;,&apos;flippers&apos;] return dataSet, labels 划分数据集对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。 #按照给定特征划分数据集 def splitDataSet(dataSet,axis,value): #&quot;&quot;&quot; # 待划分的数据集、划分数据集的特征，特征的返回值 #&quot;&quot;&quot; #对数据集进行划分，数据子集的个数由该特征所有可能值个数 retDataSet=[] for featVec in dataSet: if featVec[axis]== value: reducedFeatVec=featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet #选择最好的数据集划分方式 def chooseBestFeatureToSplit(dataSet): numFeatures=len(dataSet[0])-1 baseEntropy=calcShannonEnt(dataSet) bestInfoGain=0.0;bestFeature=-1 for i in range(numFeatures): featList=[example[i] for example in dataSet] uniqueVals=set(featList) newEntropy=0.0 for value in uniqueVals: subDatSet=splitDataSet(dataSet,i,value) prob=len(subDatSet)/float(len(dataSet)) newEntropy+=prob*calcShannonEnt(subDatSet) infoGain=baseEntropy-newEntropy if(infoGain&gt;bestInfoGain): bestInfoGain=infoGain bestFeature=i return bestFeature 递归构建决策树决策树构建过程大致如下： 得到原始数据集，然后基于最好的属性值（特征）划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据集。因此我们可以采用递归的原则处理数据集。 递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。 #返回出现次数最多的分类名称 def majorityCnt(classList): classCount={} for vote in classList: if vote not in classCount.keys(): classCount[vote]=0 classCount[vote]+=1 sortedClassCount=sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] #创建树的函数代码 def createTree(dataSet,labels): #dataSet:数据集 #labels:标签列表 classList=[example[-1] for example in dataSet] if classList.count(classList[0])==len(classList): return classList[0]#递归结束条件2，类别完全相同 if len(dataSet[0])==1: return majorityCnt(classList)#递归结束条件3，遍历完所有特征后，类别还未完全相同，返回出现次数最多的 bestFeat=chooseBestFeatureToSplit(dataSet) bestFeatLabel=labels[bestFeat] myTree={bestFeatLabel:{}} del(labels[bestFeat]) featValues=[example[bestFeat] for example in dataSet] uniqueVals=set(featValues) for value in uniqueVals: subLabels=labels[:] myTree[bestFeatLabel][value]=createTree(splitDataSet\ (dataSet,bestFeat,value),subLabels) #构建最佳属性的值为Value的子树 return myTree 该函数有3个return，前两个返回类型是‘标签’对应叶子节点，而第3个返回类型是一棵树（dict). 利用Matplotlib注解绘制树形图Matplotlib注解Matplotlib提供了一个注解工具annotations,可以在数据图形上添加文本注释，注解通常用于解释数据的内容。 import matplotlib.pyplot as plt #文本框 decisionNode = dict(boxstyle=&apos;sawtooth&apos;,fc=&apos;0.8&apos;) leafNode=dict(boxstyle=&apos;round4&apos;,fc=&apos;0.8&apos;) #箭头格式 arrow_args=dict(arrowstyle=&apos;&lt;-&apos;) 绘制带箭头的注解 def plotNode(nodeTxt,centerPt,parentPt,nodeType): createPlot.ax1.annotate(nodeTxt,xy=parentPt, xycoords=&apos;axes fraction&apos;, xytext=centerPt, textcoords=&apos;axes fraction&apos;, va=&apos;center&apos;,ha=&apos;center&apos;,bbox=nodeType,arrowprops= arrow_args) def createPlot(inTree): fig=plt.figure(1,facecolor=&apos;white&apos;) fig.clf() axprops=dict(xticks=[],yticks=[]) createPlot.ax1=plt.subplot(111,frameon=False,**axprops) plotTree.totalW=float(getNumLeafs(inTree)) plotTree.totalD=float(getTreeDepth(inTree)) plotTree.xOff=-0.5/plotTree.totalW;plotTree.yOff=1.0; plotTree(inTree,(0.5,1.0),&apos;&apos;) # plotNode(&apos;a decision node&apos;,(0.5,0.1),(0.1,0.5),decisionNode) # plotNode(&apos;a leaf node&apos;,(0.8,0.1),(0.3,0.8),leafNode) plt.show() 构造注解树定义两个新函数getNumLeafs()和getTreeDepth()，来获取叶节点的数目和树的层数，以确定图x轴和y轴。 #获取叶节点的数目 def getNumLeafs(myTree): numLeafs=0 firstStr=list(myTree.keys())[0] secondDict=myTree[firstStr] for key in secondDict.keys(): #测试节点的类型是否为字典，不是则为叶子节点 if type(secondDict[key]).__name__==&apos;dict&apos;: numLeafs+=getNumLeafs(secondDict[key]) else: numLeafs+=1 return numLeafs #获取树的层数 def getTreeDepth(myTree): maxDepth=0 firstStr=list(myTree.keys())[0] secondDict=myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__==&apos;dict&apos;: thisDepth=1+getTreeDepth(secondDict[key]) else: thisDepth=1 if thisDepth&gt;maxDepth: maxDepth=thisDepth return maxDepth def retrieveTree(i): listOfTrees=[{&apos;no surfacing&apos;:{0:&apos;no&apos;,1:{&apos;flippers&apos;:\ {0:&apos;no&apos;,1:&apos;yes&apos;}}}}, {&apos;no surfacing&apos;:{0:&apos;no&apos;,1:{&apos;flippers&apos;:\ {0:{&apos;head&apos;:{0:&apos;no&apos;,1:&apos;yes&apos;}},1: &apos;no&apos;}}}}] return listOfTrees[i] 函数retrieveTree输出预先存储的树信息，避免了每次测试代码时都要从数据中创建树的麻烦。 def plotMidText(cntrPt,parentPt,txtString): xMid=(parentPt[0]-cntrPt[0])/2.0+cntrPt[0] yMid=(parentPt[1]-cntrPt[1])/2.0+cntrPt[1] createPlot.ax1.text(xMid,yMid,txtString) def plotTree(myTree,parentPt,nodeTxt): numLeafs=getNumLeafs(myTree) depth=getTreeDepth(myTree) firstStr=list(myTree.keys())[0] cntrPt=(plotTree.xOff+(1.0+float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt,parentPt,nodeTxt) plotNode(firstStr,cntrPt,parentPt,decisionNode) secondDict=myTree[firstStr] plotTree.yOff=plotTree.yOff-1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__==&apos;dict&apos;: plotTree(secondDict[key],cntrPt,str(key)) else: plotTree.xOff=plotTree.xOff+1.0/plotTree.totalW plotNode(secondDict[key],(plotTree.xOff,plotTree.yOff), cntrPt,leafNode) plotMidText((plotTree.xOff,plotTree.yOff),cntrPt,str(key)) plotTree.yOff=plotTree.yOff+1.0/plotTree.totalD 绘制的树形图 测试和存储分类器依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类，在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。 测试算法：#使用决策树的分类函数 def classify(inputTree,featLabels,testVec): firstStr=list(inputTree.keys())[0] secondDict=inputTree[firstStr] featIndex=featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex]==key: if type(secondDict[key]).__name__==&apos;dict&apos;: classLabel=classify(secondDict[key],featLabels,testVec) else: classLabel=secondDict[key] return classLabel 使用算法：决策树的存储构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。为了解决这个问题，需要使用Python模块pickle序列化对象，可以在磁盘上保存对象，并在需要的时候读取出来。 #使用pickle模块存储决策树 def storeTree(inputTree,filename): import pickle fw=open(filename,’wb’) pickle.dump(inputTree,fw) fw.close() #读取决策树 def grabTree(filename): import pickle fr=open(filename) return pickle.load(fr) 示例：使用决策树预测隐形眼镜类型由ID3算法产生的决策树 上图所示的决策树可以非常好地匹配实验数据，然而这些匹配选项可能太多了，造成过度匹配问题。 小结决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。ID3算法可以用于划分标称型数据集。构建决策树时，通常采用递归的方法将数据集转化为决策树。 参考博客 机器学习：决策树ID3\C4.5\CART\随机森林总结及python上的实现 (2) 《机器学习实战》第三章：决策树（1）基本概念 《机器学习实战》第三章：决策树（2）树的构造 《机器学习实战》第三章：决策树（3）测试、存储、实例]]></content>
      <categories>
        <category>Python</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战第二章学习笔记(k-NN)]]></title>
    <url>%2F2017%2F09%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[k-近邻算法(kNN)采用测量不同特征值之间的距离方法进行分类。 优点： 精度高、对异常值不敏感、无数据输入假定 缺点： 计算复杂度高、空间复杂度高 基本原理存在训练样本集，每个样本都有标签，输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据(也称为最近邻)的分类标签。一般来说，只选择样本数据集中前k个最相似的数据，这也就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k-近邻算法的一般流程 收集数据：可以使用任何方法 准备数据：距离计算所需要的数值，最好是结构化的数据格式 分析数据：可以使用任何方法 训练算法：此步骤不适用于k-近邻算法 测试算法：计算错误率 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。 kNN代码实现伪代码： 计算已知类别数据集中的点和当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出行频率； 返回前k个点出行频率最高的类别作为当前点的预测分类。 Python代码： #python3.6 import numpy as np import operator from os import listdir def createDataSet(): group=np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels=[&apos;A&apos;,&apos;A&apos;,&apos;B&apos;,&apos;B&apos;] return group,labels # inX: input # dataSet: train data set # labels: labels of dataSet # k: k of kNN def classify0(inX,dataSet,labels,k): dataSetSize=dataSet.shape[0]#样本个数 diffMat=np.tile(inX,(dataSetSize,1))-dataSet #np.tile()函数用法 sqDiffMat=diffMat**2 sqDistances=sqDiffMat.sum(axis=1)#欧式距离 distances=sqDistances**0.5 sortedDistIndicies=distances.argsort()#按距离排序，返回从小到大的索引值 classCount={} for i in range(k): voteIlabel=labels[sortedDistIndicies[i]] classCount[voteIlabel]=classCount.get(voteIlabel,0)+1#计算前k个，各个类别的个数 sortedClassCount=sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) #Python3中没有.iteritems(),更换为.items() #print(sortedClassCount) return sortedClassCount[0][0] #测试 import kNN group,labels=kNN.createDataSet() kNN.classify0([0,0],group,labels,3) #结果应该是B,可以改变输入[0,0] 示例1、改进约会网站的配对效果约会数据存放在datingTestSet2.txt中，每个样本占一行，共有1000行，即1000个样本。主要特征： 每年获得的飞行常客里程数 玩视频游戏所耗时间百分比 每周消费的冰淇淋公升数 数据下载 将文本记录到转换NumPy的解析程序#输入文件名，输出训练样本矩阵和类标签向量 def file2matrix(filename): #打开文件 fr=open(filename) #按行全部读取数据 arrayOLines=fr.readlines() #行数 numberOfLines=len(arrayOLines) #用于存放样本，主要有3个特征 returnMat=np.zeros((numberOfLines,3)) #分类标签 classLabelVector=[] index=0 for line in arrayOLines: line=line.strip() listFromLine=line.split(&apos;\t&apos;) returnMat[index,:]=listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index+=1 return returnMat,classLabelVector #测试 reload(kNN)#重新加载kNN #python3中没有内置该函数，需要from imp import reload datingDataMat,datingLabels=kNN.file2matrix(&apos;datingTestSet2.txt&apos;) 归一化数值因为kNN是依赖样本间距离进行分类的，欧式距离的计算中，数字差值最大的特征对计算结果的影响要大一些。但是，一般来说，每个特征视为同等重要的。这样就需要对特征值进行处理，通常采用的方法是将数值归一化，如将取值范围处理为0-1或-1-1之间。 newValue=(oldValue-min)/(max-min) 公式可以将任意取值范围的特征值转化为0到1区间内的值。 归一化特征值Python3程序#输入样本集，输出归一化后的样本集、ranges，minVals def autoNorm(dataSet): #newValue=(oldValue-min)/(max-min) minVals=dataSet.min(0) maxVals=dataSet.max(0) ranges=maxVals-minVals normDataSet=np.zeros(np.shape(dataSet)) m=dataSet.shape[0] normDataSet=dataSet-np.tile(minVals,(m,1)) normDataSet=normDataSet/np.tile(ranges,(m,1)) return normDataSet,ranges,minVals #测试 reload(kNN) normMat,ranges,minVals=kNN.autoNorm(datingDataMat) kNN约会网站测试代码def datingClassTest(): hoRatio=0.10 datingDataMat,datingLabels=file2matrix(&apos;datingTestSet2.txt&apos;) normMat,ranges,minVals=autoNorm(datingDataMat) m=normMat.shape[0] numTestVecs=int(m*hoRatio) errorCount=0.0 for i in range(numTestVecs): classifierResult=classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],5) print(&apos;the classifier came back with: %d, the real answer is: %d&apos; % (classifierResult,datingLabels[i])) if (classifierResult != datingLabels[i]): errorCount+=1.0 print(&apos;the total error rate is : %f&apos; % (errorCount/float(numTestVecs))) #测试 kNN.datingClassTest() 约会网站预测函数def classifyPerson(): resultList=[&apos;not at all&apos;,&apos;in small doses&apos;,&apos;in large doses&apos;] percentTats=float(input(&apos;percentage of time spent playing video games?&apos;)) ffMiles=float(input(&apos;frequent fliter miles earned per year?&apos;)) iceCream=float(input(&apos;liters of ice cream consumed per year?&apos;)) datingDataMat,datingLabels=file2matrix(&apos;datingTestSet2.txt&apos;) normMat,ranges,minVals=autoNorm(datingDataMat) inArr=np.array([ffMiles,percentTats,iceCream]) classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print(&apos;You will probably like this person: &apos;,\ resultList[classifierResult-1]) #python2中的raw_input()，python3中为input() 示例2、手写识别系统将32*32的二进制图像矩阵转化为1*1024向量def img2vector(filename): returnVect=np.zeros((1,1024)) fr=open(filename) for i in range(32): lineStr=fr.readline() for j in range(32): returnVect[0,32*i+j]=int(lineStr[j]) return returnVect #测试 testVector=kNN.img2vector(&apos;testDigits/0_13.txt&apos;) 手写数字识别系统的测试代码from os import listdir def handwritingClassTest(): hwLabels=[] trainingFileList=listdir(&apos;trainingDigits&apos;) m=len(trainingFileList) trainingMat=np.zeros((m,1024)) for i in range(m): fileNameStr=trainingFileList[i] fileStr=fileNameStr.split(&apos;.&apos;)[0] classNumStr=int(fileStr.split(&apos;_&apos;)[0])#文件名是标签 hwLabels.append(classNumStr) trainingMat[i,:]=img2vector(&apos;trainingDigits/%s&apos; % fileNameStr) testFileList=listdir(&apos;testDigits&apos;) errorCount=0.0 mTest=len(testFileList) for i in range(mTest): fileNameStr=testFileList[i] fileStr=fileNameStr.split(&apos;.&apos;)[0] classNumStr=int(fileStr.split(&apos;_&apos;)[0]) vectorUnderTest=img2vector(&apos;testDigits/%s&apos; % fileNameStr) classifierResult=classify0(vectorUnderTest,\ trainingMat,hwLabels,3) print(&apos;the classifier came back with: %d, the real anser is: %d&apos; \ % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount+=1.0 print(&apos;the total number of errors is: %d&apos; % errorCount) #测试 kNN.handwritingClassTest() 小结k-近邻算法是分类数据最简单最有效的算法。k-近邻算法是基于实例的学习，使用算法时必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。 k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此无法知晓平均实例样本和典型实例样本具有什么特征。]]></content>
      <categories>
        <category>Python</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows系统下Xgboost安装【转】]]></title>
    <url>%2F2017%2F08%2F30%2FWindows%E4%B8%8B%E5%AE%89%E8%A3%85Xgboost%2F</url>
    <content type="text"><![CDATA[XGBoost是Gradient Boosting算法的一种高级实现，在Kaggle competitions上崭露头角。下面就对XGBoost在Windows上的安装作一个介绍，因为XGBoost在Windows平台上的安装不是那么简单直接。我在实验室的电脑上（Windows 7，64 bits）通过这些步骤安装成功，希望能对后来人有所帮助。 安装必要的软件为了能在Windows上通过Python使用XGBoost，需要先安装以下三个软件： Python Git MINGW Python和Git的安装对于Python，你可以到Python官网上下载你想安装的版本，安装很简单，这里就跳过。对于Git的安装有很多种选择，一种选择就是使用Git for Windows，Git for Windows的安装也比较简单，遵从指示就行，这里也跳过。 XGBoost的下载Git安装完成后，开始菜单中会出现一个叫Git Bash的程序，点开后就会出现一个类似Windows命令行的窗口，首先在这个Bash窗口，使用cd命令进入你想保存XGBoost代码的文件夹，比如下面的示例： $ cd /e/algorithm 然后输入下面的代码下载XGBoost文件包： $ git clone --recursive https://github.com/dmlc/xgboost $ cd xgboost $ git submodule init $ git submodule update 编译XGBoost代码MinGW-W64的安装接下来就是编译我们刚刚下载的XGBoost的代码。这就需要用到MinGW-W64。它的安装包我是从这里下载的，下载完成后双击安装，出现下面的安装界面，点击Next： 然后在Architecture选项处选择x86_64(!!!不要忘记了)即可，其他选项保持默认，如下图： 然后点击下一步，就能安装完成。我使用的是默认安装路径C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1。那么make命令和运行库就在下面的文件夹中（也就是包含mingw32-make的文件夹）：C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1\mingw64\bin，接下来就是把上面的路径添加到系统的Path中，关于如何添加环境变量到系统的Path中，可以参考这篇文章。 上面的步骤完成后，关闭Git Bash窗口后重新打开，为了确认添加环境变量已经添加成功，可以在Bash中键入下面的命令： $ which mingw32-make 如果添加成功的话，应该返回类似下面这样的信息： C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1\mingw64\bin\mingw32-make 为了输入的方便，可以简化mingw32-make命令为make $ alias make=&apos;mingw32-make&apos; XGBoost的编译现在就可以开始编译XGBoost了，首先进入xgboost文件夹 $ cd /e/algorithm/xgboost 通过这篇官方文档给出的统一编译的方法在写这篇文章时还不能正常编译成功，所以我们采用下面的命令来分开编译，每次编译一个子模块。注意，我们要等每个命令编译完成后才能键入下一个命令。 $ cd dmlc-core $ make -j4 $ cd ../rabit $ make lib/librabit_empty.a -j4 $ cd .. $ cp make/mingw64.mk config.mk $ make -j4 一旦最后一个命令完成后，整个编译过程就完成了。下面就开始安装Python模块。进入XGBoost文件夹下面的python-package子文件夹，然后键入： $ cd /e/algorithm/xgboost/python-package&gt;python setup.py install 进行到这儿，基本上就完成了，这时打开一个Jupyter notebook，直接导入xgboost包会出现错误，我们需要先运行下面的代码： import os mingw_path = &apos;C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1\mingw64\bin&apos; os.environ[&apos;PATH&apos;] = mingw_path + &apos;;&apos; + os.environ[&apos;PATH&apos;] 成功示例然后我们就可以开始导入xgboost包去运行下面的示例： import xgboost as xgb import numpy as np data = np.random.rand(5,10) # 5 entities, each contains 10 features label = np.random.randint(2, size=5) # binary target dtrain = xgb.DMatrix( data, label=label) dtest = dtrain param = {&apos;bst:max_depth&apos;:2, &apos;bst:eta&apos;:1, &apos;silent&apos;:1, &apos;objective&apos;:&apos;binary:logistic&apos; } param[&apos;nthread&apos;] = 4 param[&apos;eval_metric&apos;] = &apos;auc&apos; evallist = [(dtest,&apos;eval&apos;), (dtrain,&apos;train&apos;)] num_round = 10 bst = xgb.train( param, dtrain, num_round, evallist ) bst.dump_model(&apos;dump.raw.txt&apos;) 至此，如果没有出现错误，就表示安装成功。 附加 可能对git bash不熟悉的可以先看看git操作命令 上面添加PATH步骤很重要，有些网上的博客并未提及，我第一次就是这里没有安装成功 xgboost这个文件夹最好不要放在python的工作路径内，也不要随意删除 参考 https://wang-shuo.github.io/2017/02/21/%E5%9C%A8Windows%E4%B8%8B%E5%AE%89%E8%A3%85XGBoost/ https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows系统与树莓派间文件传输]]></title>
    <url>%2F2017%2F08%2F25%2Fwindows%E7%B3%BB%E7%BB%9F%E4%B8%8E%E6%A0%91%E8%8E%93%E6%B4%BE%E9%97%B4%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[windows系统和树莓派之间文件传输方式有很多种，利用FileZilla来传输，是我用的最简便快捷的一种。 FileZilla特点 易于使用 FileZilla比其他任何一款FTP软件都要简单 多协议支持 FileZilla支持FTP、FTPS、SFTP等文件传输协议 多种语言 FileZilla支持多国语言，完美支持简体中文 多标签界面 多标签界面 远程查找文件 FileZilla支持远程查找文件功能 站点管理器 FileZilla自带功能强大的站点管理和传输队列管理 使用虽然介绍的功能比较多，但我关心的是与树莓派的文件传输功能。 直接上图： 主机：sftp://YOUR DEVICE IP ADDRESS(192.168.1.102) 填写用户名和密码，端口可不填 快速连接 左边本底站点为你的windows系统资源管理器，右边远程站点是树莓派文件系统，直接拖拽就可以实现文件传输。需要注意的是有些树莓派中的文件是有访问等级的，可能无法操作 下载地址：FileZilla - The free FTP solution]]></content>
      <categories>
        <category>树莓派3</category>
        <category>好记性不如烂笔头</category>
      </categories>
      <tags>
        <tag>树莓派3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Relief特征选择算法Python实现]]></title>
    <url>%2F2017%2F08%2F22%2FReliefF%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9(python)%2F</url>
    <content type="text"><![CDATA[Relief算法Relief算法最早由Kira在1992年提出，最初局限于两类数据的分类问题。Relief算法是一种特征权重算法(Feature weighting algorithms)，根据各个特征和类别的相关性赋予特征不同的权重，权重小于某个阈值的特征将被移除。Relief算法中特征和类别的相关性是基于特征对近距离样本的区分能力。 基本思想：算法从训练集D中随机选择一个样本R，然后从和R同类的样本中寻找最近邻样本H，称为NearHit，从和R不同类的样本中寻找最近邻样本M，称为NearMiss，然后根据以下规则更新每个特征的权重：如果R和NearHit在某个特征上的距离小于R和NearMiss上的距离，则说明该特征对区分同类和不同类的最近邻是有益的，则增加该特征的权重；反之，如果R和NearHit在某个特征的距离大于R和NearMiss上的距离，说明该特征对区分同类和不同类的最近邻起负面作用，则降低该特征的权重。以上过程重复m次，最后得到各特征的平均权重。特征的权重越大，表示该特征的分类能力越强，反之，表示该特征分类能力越弱。Relief算法的运行时间随着样本的抽样次数m和原始特征个数N的增加线性增加，因而运行效率非常高。 具体算法： ReliefF算法虽然Relief算法比较简单，但运行效率高，并且结果也比较令人满意，因此得到广泛应用，但是其局限性在于只能处理两类别数据，因此1994年Kononeill对其进行了扩展，得到了ReliefF算法，可以处理多类别问题。ReliefF算法在处理多类问题时，每次从训练样本集中随机取出一个样本R，然后从和R同类的样本集中找出R的k个近邻样本(near Hits)，从每个R的不同类的样本集中均找出k个近邻样本(near Misses)，然后更新每个特征的权重，如下式所示： ReliefF算法python程序import numpy as np from sklearn.metrics.pairwise import pairwise_distances def reliefF(X,y,**kwargs): if &quot;k&quot; not in kwargs.keys(): k=5 else: k=kwargs[&quot;k&quot;] n_samples,n_features=X.shape #计算两两距离，曼哈顿距离 distance=pairwise_distances(X,metric=&apos;manhattan&apos;) score=np.zeros(n_features) for idx in range(n_samples): #同类最近邻 near_hit=[] #异类最近邻 near_miss=dict() self_fea=X[idx,:] #类别数 c=np.unique(y).tolist() stop_dict=dict() for label in c: stop_dict[label]=0 del c[c.index(y[idx])] P_dict=dict() p_label_idx=float(len(y[y==y[idx]]))/float(n_samples) for label in c: p_label_c=float(len(y[y==label]))/float(n_samples) p_dict[label]=p_label_c/(1-p_label_idx) near_miss[label]=[] distance_sort=[] distance[idx,idx]=np.max(distance[idx,:]) for i in range(n_samples): distance_sort.append([distance[idx,i],int(i),y[i]]) distance_sort.sort(key=lambda x: x[0]) for i in range(n_samples): #找到同类最近邻 if distance_sort[i][2]==y[idx]: if len(near_hit) &lt;k: near_hit.append(distance_sort[i][1]) elif len(near_hit)==k: stop_dict[y[idx]]=1 else: #异类最近邻 if len(near_miss[distance_sort[i][2])&lt;k: near_miss[distance_sort[i][2].append(distance_sort[i][1]) else: if len(near_miss[distance_sort[i][2]]) == k: stop_dict[distance_sort[i][2]] = 1 stop = True for (key, value) in stop_dict.items(): if value != 1: stop = False if stop: break #更新reliefF分数 for ele in near_hit: near_hit_term = np.array(abs(self_fea-X[ele, :]))+np.array(near_hit_term) near_miss_term = dict() for (label, miss_list) in near_miss.items(): near_miss_term[label] = np.zeros(n_features) for ele in miss_list: near_miss_term[label] = np.array(abs(self_fea-X[ele, :]))+np.array(near_miss_term[label]) score += near_miss_term[label]/(k*p_dict[label]) score -= near_hit_term/k return score def feature_ranking(score): idx = np.argsort(score, 0) return idx[::-1] 参考博文： http://www.cnblogs.com/asxinyu/archive/2013/08/29/3289682.html https://github.com/jundongl/scikit-feature 特征提取Python库：http://featureselection.asu.edu/]]></content>
      <categories>
        <category>Python</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
        <tag>Relief</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习路线【转】]]></title>
    <url>%2F2017%2F08%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[主要内容 前言 课程列表 推荐学习路线 数学基础初级 程序语言能力 机器学习课程初级 数学基础中级 机器学习课程中级 推荐书籍列表 机器学习专项领域学习 致谢 前言 我们要求把这些课程的所有Notes,Slides以及作者强烈推荐的论文看懂看明白，并完成所有的老师布置的习题，而推荐的书籍是不做要求的，如果有些书籍是需要看完的，我们会进行额外的说明。 课程列表 课程 机构 参考书 Notes等其他资料 单变量微积分 MIT Calculus with Analytic Geometry 链接 多变量微积分 MIT Multivariable Calculus 链接 线性代数 MIT Introduction to Linear Algebra 链接 统计入门 可汗学院 暂无 暂无 概率论入门: 链接1,链接2 NTU 暂无 暂无 概率与统计 MIT Introduction to Probability 链接 矩阵论 暂无 矩阵论 暂无 凸优化1 Stanford Convex Optimization 链接 凸优化2 Stanford 暂无 链接 统计学习入门 Stanford An Introduction to Statistical Learning 链接 机器学习基石 NTU Learning from Data 链接 机器学习技法 NTU 暂无 链接 机器学习 Caltech Learning from Data 链接 机器学习(matlab) Stanford 暂无 链接 Python程序语言设计 暂无 暂无 暂无 Matlab程序语言设计 暂无 暂无 暂无 推荐学习路线数学基础初级 课程 机构 参考书 Notes等其他资料 单变量微积分 MIT Calculus with Analytic Geometry 链接 多变量微积分 MIT Multivariable Calculus 链接 线性代数 MIT Introduction to Linear Algebra 链接 统计入门 可汗学院 暂无 暂无 概率论入门: 链接1,链接2 NTU 暂无 暂无 概率与统计 MIT Introduction to Probability 链接 程序语言能力考虑到机器学习的核心是里面的数学原理和算法思想，程序语言目前主要是帮助大家较好的完成课后作业以及实现自己的一些idea，此处我们仅仅给出推荐的参考学习链接，大家掌握一些常用的模块即可，即完成参考学习链接部分的内容即可，推荐书籍比较经典，但不做要求。 课程 参考学习链接 推荐书籍 Python程序语言设计 链接 暂无 Matlab程序语言设计 暂无 暂无 R程序语言设计 暂无 暂无 机器学习课程初级 课程 机构 参考书 Notes等其他资料 统计学习入门 Stanford An Introduction to Statistical Learning 链接 机器学习入门 Coursera 暂无 链接 数学基础中级 课程 机构 参考书 Notes等其他资料 矩阵论 暂无 矩阵论 暂无 凸优化1 Stanford Convex Optimization 链接 凸优化2 Stanford 暂无 链接 下面这个概述必须看完。 Convex Optimization: Algorithms and Complexity 机器学习课程中级 此处NTU和Caltech两个大学的课程是由《Learning from Data》一书的两个不同的作者讲的，所以仅仅只需选择一个完成即可，注意：如果选择完成NTU的机器学习课程，则NTU的“机器学习基石”和“机器学习技法”需同时完成。。 课程 机构 参考书 Notes等其他资料 机器学习基石 NTU Learning from Data 链接 机器学习技法 NTU 暂无 链接 机器学习 Stanford 暂无 链接 机器学习 Caltech Learning from Data 链接 推荐书籍列表 以下推荐的书籍都是公认的机器学习领域界的好书，建议一般难度的书籍至少详细阅读一本，建议看两本，而较难的书籍不做任何要求，大家可以在学有余力时细细品味经典。 书名 难度 统计学习方法 一般 An Introduction to Statistical Learning 一般 Machine Learning 一般 Learning from Data 一般，配套讲义 Pattern Recognition and Machine Learning 较难(偏贝叶斯),配套讲义 The Elements of Statistical Learning 较难 Understanding Machine Learning:From Theory to Algorithms 较难 Machine Learning: A probabilistic approach 较难 机器学习专项领域学习如果您已经完成了上述的所有科目，恭喜您已经拥有十分扎实的机器学习基础了，已经是一名合格的机器学习成员了，可以较为顺利的进入下面某一专项领域进行较为深入研究,因为并不是所有的专项领域都有对应的课程或者书籍等学习资料，所以此处我们仅列举一些我们知道的专项领域的学习资料，当然这些领域不能涵盖所有，还有很多领域没有整理（希望大家一起完善），如果这些领域适合你，那就继续加油！如果不清楚，那么大家可以去下面列举的高级会议期刊上去寻找自己感兴趣的话题进行学习研究。 一些专项领域资料 深度学习 图模型 强化学习 Hash 理论机器学习 其他(尚未完善) 领域会议期刊 NIPS ICML AAAI IJCAI KDD ICDM COLT 其他(尚未完善) 致谢 感谢南京大学LAMDA实验组杨杨博士的建议与资料的分享。 原文链接：https://github.com/JustFollowUs/Machine-Learning]]></content>
      <categories>
        <category>book</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Snowboy使用说明_Hotword Detection]]></title>
    <url>%2F2017%2F08%2F16%2FSnowboy%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_Hotword%20Detection%2F</url>
    <content type="text"><![CDATA[IntroductionSnowboy是一个高度自定义的基于实时甚至离线情况的hotword detection engine(敏感词检测机制？)，兼容Raspberry Pi,Linux and Mac OS X. hotword也被称为唤醒词(wake word)或触发词(trigger word)通常是一个关键词或短语，计算机会一直监听作为一个信号用于触发其他操作。 有一些例子，如Amazon Echo的“Alexa”、Google Assistant的”OK Google”和iPhone的“Hey Siri”. 这些关键词用于触发一个全面的语音交互。但是，hotwords同样可以用于其他方面，像是命令和控制等。 一种简单的方案是，运行全ASR(Automatic Speech Recognition)来检测hotword detection.在这种方案中，设备会一直观察特定触发词。但是ASR也会消耗设备和带宽资源。同样的，如果基于云的应用，将不能保护你的隐私。（因为会一直开着麦克风来检测关键词，那么周围环境包括你说的话也会实时被监听了).幸运的是，Snowboy会用来解决此类问题。 highly customizable(高度自定义） always listening but protects your privacy(总是检测但是不会泄露隐私） light-weight and embedded Apache licensed! Quick Start准备工作： 一台带有microphone的设备 相应的解压软件 训练好的模型 树莓派pre-packaged Access Microphone使用PortAudio作为一个跨平台的音频输入/输出。同样也使用sox快速检查microphone是否正确安装。 Install Sox 1sudo apt-get install python-pyaudio python3-pyaudio sox Install PortAudio’s Python bindings: 12pip install pyaudio#pip-3.2 install pyaudio To check whether you can record via your microphone, open a terminal and run: 12rec temp.wav#记录个几秒中，ctrl+c,再play temp.wav,听声 Decoder Structures上面预安装包下载解压后，如下： ├── README.md ├── _snowboydetect.so ├── demo.py ├── demo2.py ├── light.py ├── requirements.txt ├── resources │ ├── ding.wav │ ├── dong.wav │ ├── common.res │ └── snowboy.umdl ├── snowboydecoder.py ├── snowboydetect.py └── version _snowboydetect.so是用SWIG编译的一个动态链接库，依赖于系统的Python2库。snowboy所有相关库都被静态连接在这个文件里。 snowboydetect.py是一个SWIG生成的python wrapper文件。因为不易阅读，我们创建了高等级的wrapper: snowboydecoder.py 应该在https://snowboy.kitt.ai 上训练你的模型(snowboy.pmdl)，或者你也可以使用同一模型resources/snowboy.umdl Runing a Demo To access the simple demo in main code of snowboydecoder.py, run the following command in your Terminal: 12python demo.py snowboy.pmdl#snowboy.pmdl是你训练的hotword模型 When prompt,speak into your microphone to see whether snowboy detects your magic phrase. 12345678910111213141516171819202122232425262728293031323334#demo.pyimport snowboydecoderimport sysimport signalinterrupted = Falsedef signal_handler(signal, frame): global interrupted interrupted = Truedef interrupt_callback(): global interrupted return interruptedif len(sys.argv) == 1: print("Error: need to specify model name") print("Usage: python demo.py your.model") sys.exit(-1)model = sys.argv[1]signal.signal(signal.SIGINT, signal_handler)detector = snowboydecoder.HotwordDetector(model, sensitivity=0.5)print('Listening... Press Ctrl+C to exit')detector.start(detected_callback=snowboydecoder.ding_callback, interrupt_check=interrupt_callback, sleep_time=0.03)detector.terminate() 主程序在detector.start()中循环，每sleep_time=0.03: 检查ring buffer 是否有hotword, if YES,调用detected_callback函数 调用interrupt_check函数，if True，中断主程序，返回. 目前，在demo中令detected_callback=snowboydecoder.ding_callback,所以每当检测到关键词时，设备会“叮”一下。 原文出处：http://docs.kitt.ai/snowboy/#running-a-demo]]></content>
      <categories>
        <category>树莓派3</category>
      </categories>
      <tags>
        <tag>UNIT</tag>
        <tag>树莓派3</tag>
        <tag>语音识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReSpeaker智能语音双麦克风阵列]]></title>
    <url>%2F2017%2F08%2F11%2FReSpeaker%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%8F%8C%E9%BA%A6%E5%85%8B%E9%A3%8E%E9%98%B5%E5%88%97%2F</url>
    <content type="text"><![CDATA[去年圣诞节买了块树莓派，看了网上好多基于树莓派的DIY语音助手，一直想自己也山寨一个。主要有两个问题，一是语音输入如何解决？二是Alexa和google home都是国外，不仅仅是不支持中文，而且还有一堵“墙”在，比较麻烦。 第一个问题，网上主要有两种办法，要么直接搞免驱mini USB麦克风或者大的比较丑的的麦克风，要么就是科技范十足的麦克风阵列。我是非常倾向于用麦克风阵列的，也没想4麦克、7麦克、8麦克等，两麦克阵列就OK了，但是找了一圈没找着合适的。主要是价钱太贵，不忍心剁手。后来将就着买了个mini USB麦克风，但是除了敲桌子声音或者直接吼，根本没啥效果，检测不到声音(明明看着国外的小哥哥们用的挺好的。。。也可能是淘宝上买到假货了)。 第二个，明显今年百度开始在AI上下了不少功夫。语音这一块，主要有UNIT、DuerOS平台。虽然做的没有Google Home 和Alexa那么知名，但是支持中文，而且在国内。 所以借着百度之星UNIT对话系统，又开始了我的瞎折腾(之前搞了Alexa和Google Home,加上不支持中文、墙、硬件问题等，搞得头大就放弃了)。这一次，借助着百度AI平台，语音识别转文字，然后利用UNIT解析，再语音合成输出。那么就剩下硬件了，还是上面的选择，USB麦克风或者麦克风阵列。经人推荐，找到了一款双麦克阵列模块，兼容树莓派3，淘宝上有旗舰店，80元RMB.(哈哈，当时直接就下单了！) 淘宝上给的简介是： ReSpeaker智能语音方案 双麦克风扩展板 兼容树莓派Zero/3B/2B此产品集成了亚马逊语言和谷歌助手等，兼容树莓派Zero、树莓派3B/2B，可以构建一个更强大更灵活的语音产品。 淘宝链接 但实际上，这货的外国名是ReSpeaker 2-Mics Pi HAT 相关链接 ReSpeaker 2-Mics Pi HAT是一款为树莓派而设计针对AI或语音应用的双麦克风扩展板。这意味着你可以基于树莓派（集成Amazon Alexa,Google Assistant)建立一个更强大、更灵活的语音产品. 这块板子基于WM8960,一片低功耗立体声编解码器。有两个麦克风分别位于板子的两侧用于采集声音。板子上还有3个APA102 RGB LED,1个用户按键和两个Grove接口用于扩展应用。更惊喜的是，还有3.5mm Audio Jack和JST 2.0 Speaker接口用于输出声音。 特点 兼容树莓派(Raspberry Pi Zero and Zero W, Raspberry Pi B+,Raspberry Pi 2B and Raspberry Pi 3B) 2个麦克风 2个Grove接口 1个用户按键 3.5mm音频接口 JST2.0音频输出 应用领域 语音交互应用 AI助手 硬件介绍 BUTTON:用户按键，连接到GPIO17 MIC_L &amp; MIC_R:位于板子两侧的麦克风 RGB LED:3颗APA102 RGB LED,连接到了SPI接口 WM8960: 低功耗立体声编解码器 Raspberry Pi 40-Pin Headers: POWER:板子的USB供电口，当使用扬声器时，要保证足够的电流 I2C:Grove I2C接口，连接I2C-1 GPIO 12：Grove 数字端口，连接GPIO12 &amp; GPIO13 JST 2.0 SPEAKER OUT:连接扬声器 3.5mm AUDIO JACK:连接带有3.5mm插口的耳机或扬声器 具体配置及使用参考：http://wiki.seeed.cc/Respeaker_2_Mics_Pi_HAT/]]></content>
      <categories>
        <category>树莓派3</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>UNIT</tag>
        <tag>树莓派3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017百度之星开发者大赛--资格赛2]]></title>
    <url>%2F2017%2F08%2F10%2F%E7%99%BE%E5%BA%A6%E4%B9%8B%E6%98%9F%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E8%B5%84%E6%A0%BC%E8%B5%9B2%2F</url>
    <content type="text"><![CDATA[7月28日，百度之星小姐姐给打过电话后，连着搞了两天，其实也没两天，还帮老板整理了好些文档资料。前一篇博客上也说了，搞了69.06分，第9名。然后想着反正肯定可以进前100啦，就没怎么管理了。优化方案也是按着百度AI开发论坛上提供的UNIT机器人优化指南结合UNIT使用手册做的。 最后结果： 事先，基本的意图、词槽、Bot回应等等按照规定做好。实际中，我开始就没认真搞好，漏了一个错误的词槽。但是在最后资格赛结束之前发现了，改正之后提高了1点几个分数。 基本的对话系统搞定之后，后期的优化主要分为两块： 增加对话样本集 增加对话模板集 对话样本集对应着样本学习，假设对话样本足够多、样本质量非常高，那通过神经网络学习得到的模型自然也不会差。 对话模板集则是对应着规则学习，正常人对话时，会有一定的语言结构，如，主语+谓语+宾语结构。特别是在一个特定的场景下，可以充分考虑到用户的对话的模板，进行一些提炼，得到一些语言“公式”。显然这种对话模板集方式的泛化能力是有限的，但是它的优点在于可以保证在符合“公式”的对话时不会出现错误。 我所侧重的是第一个方法，即增加对话样本集。使用了目前UNIT的隐藏功能，推荐对话样本。自己可以先整一个小的对话样本，然后让UNIT平台根据自己DIY的对话样本生成一些样本，然后自己去标注。大约标注了3500个样本吧，自己整的样本比例是按照百度提供的对话样本中各个意图的比例整的，保证同分布吧。]]></content>
      <categories>
        <category>UNIT</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>UNIT</tag>
        <tag>百度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017百度之星开发者大赛--资格赛]]></title>
    <url>%2F2017%2F07%2F30%2F%E7%99%BE%E5%BA%A6%E4%B9%8B%E6%98%9F%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B-%E8%B5%84%E6%A0%BC%E8%B5%9B%2F</url>
    <content type="text"><![CDATA[之前有看到网上说每年百度之星是一个不错的编程比赛，可以检验一下自己的编程能力。所以，当宿舍楼下见到贴的海报的时候也就报名了，报名的时候发现今年百度之星有些变革。与以往不同的是，今年多了个百度之星开发者大赛，以前都是程序设计大赛啊（全TM是那帮搞ACM的天下啊。。。）。仔细想想前段时间搞美团的Code-M比赛，那叫一个惨啊，不是不会啊，全是运行超时，需要优化！咱们又不是计算机学院的，又不没搞过ACM,没练过啊。于是这次直接就报了开发者大赛，都没看看程序设计是搞啥的。 开发者大赛是利用百度自己一个新的平台UNIT(Understanding and Interaction Technology),之前博客也介绍过。估计百度是想借着比赛推广一下平台，顺便再收集一些数据。总之，利用UNIT平台做出一个东西，如智能硬件、APP啥的，没有具体的限制。 比赛分为资格赛和正赛两部分，资格赛是用来熟悉UNIT平台的，只有在两个场景下进入前200名的才有资格进入正赛。 资格赛任务介绍 任务：基于UNIT平台优化给定场景的对话能力 使用UNIT平台优化题设场景的对话能力，提供两个固定场景供参赛者选择： 场景一：订餐馆，包括查询餐馆、订位或发起导航 场景二：看电影，包括查询电影、影院、购票电影票 每个场景由主办方预先提供意图与词槽的定义，并提供一定量对话样本作为训练数据。参赛者需要使用UNIT完成场景的意图与词槽的配置，并利用对话样本优化场景的对话理解能力（参赛者自行增加对话样本与词槽的词表等），并最终在UNIT平台上产出模型与服务。 正赛任务介绍 任务：设计和开发一个以对话式人机交互为核心的智能产品 产品形式包括但不限于手机APP、智能硬件等。产品必须使用百度提供的理解与交互技术平台（以下简称&gt; &gt; UNIT平台）实现核心对话能力，可以使用百度AI平台开放的其他技术能力作为辅助，其余百度未提供的功&gt; 能与能力，实现形式不限。 刚开始接触感觉挺难的，想找些队员，在群里吼了两嗓子，但只有两个妹子回应，最后也就没有了下文。正好自己又忙着开题，也就打算放弃了。就这样，7月28号，这次百度之星的组织者吧，听声音应该是个小姐姐直接打电话过来了。说是资格赛不是很难，尽量还是参与一下，比较容易可以进正赛。还举例子说东北大学、大连理工的几位同学两天就搞到了60、70分。后来，想了下，觉得也是，小姐姐都打电话过来了。抱着试一试的心态，搞了将近两天吧，69.06分，第9名。 头半天在及格线附近，又连忙按着使用手册搞了一天，标注了快3000条样本才把分数提上去。下回写一下具体优化过程，感觉UNIT平台以后要是集成语音交互功能，再加上百度的中文处理资源，还是有一定钱途的。]]></content>
      <categories>
        <category>UNIT</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>UNIT</tag>
        <tag>百度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[台湾国立清华大学彭明辉研究生手册【转】]]></title>
    <url>%2F2017%2F07%2F27%2F%E5%8F%B0%E6%B9%BE%E5%9B%BD%E7%AB%8B%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E5%BD%AD%E6%98%8E%E8%BE%89%E7%A0%94%E7%A9%B6%E7%94%9F%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[#一、论文的要求 # 我对硕士论文的基本要求是： （1）论文的主要内容，是叙述一套方法在一个特定场合中的应用。 （2）这套方法必须要有所创新或突破，并因而对学术界有所贡献。因此，它或者是解决既有问题的新方法，或者是既有方法的新应用，或者是以一个新的方法开启一整片新的应用领域。 （3）在论文中，你必须要有能力提出足够的证据来让读者信服说：针对这个应用场合，你所提出来的方法确实有比文献中一切既有方法更优越之处。 （4）此外，你必须要能清楚指出这个方法在应用上的限制，并且提出充分证据来说服读者：任何应用场合，只要能够满足你所提出来的假设（前提）条件，你的方法就一定适用，而且你所描述的优点就一定会存在。 （5）你还必须要在论文中清楚指出这个方法的限制和可能的缺点（相对于其它文献上的既有方法，或者在其它应用场合里）。假如这个方法有任何重大缺点，在口试时才被口试委员指出来，其后果有可能是论文无法通过。 （6）行文风格上，它是一篇论证严谨，逻辑关系清晰，而且结构有条理的专业论述。也就是说，在叙述你的方法的过程，你必须要清清楚楚地交代这个方法的应用程序以及所有仿真或实验结果的过程，使得这个专业领域内的任何读者，都有办法根据你的描述，在他的实验室下复制出你的研究成果，以便确定你的结论确实是可以「在任何时间、任何地点、任何人」都具有可重复性（可重复性是「科学」的根本要求）。 （7）而且，你对这个方法的每一个步骤都必须要提供充分的理由说明「为什么非如此不可」。 （8）最后，你的论文必须要在适当位置清楚注明所有和你所研究之题目相关的文献。而且，你必须要记得：只要是和你所研究的问题相关的学术文献（尤其是学术期刊论文），你都有必要全部找出来（如果漏掉就是你的过失），仔细读过。假如你在学位论文口试时，有口试委员指出有一篇既有文献，在你所讨论的问题中处理得比你的方法还好，这就构成你论文无法及格的充分理由。 （9）第（2）款所谓「对学术界的贡献」，指的是：把你的所有研究成果扣除掉学术界已经发表过的所有成果（不管你实际上有没有参考过，没有参考过也算是你的重大过失），剩下的就是你的贡献。假如这个贡献太少，也构成你论文无法及格的充分理由。 上面所叙述的九款要件中，除第（2）款之外，通通都是必须要做到的，因此没有好坏之分。一篇硕士论文的好坏（以及成绩的评定标准），主要是看第（2）款所谓「对学术界的贡献」的多寡与重要性而定。假如你要申请国外的博士班，最重要的也是看你的硕士论文有什么「贡献」而定（这往往比TOFEL、GRE、GPA还重要）。 一个判断硕士论文的好坏有一个粗浅办法：假如你的研究成果可以在国外著名学术期刊（journals，而非 magazines）上发表，通常就比一篇只能在国外学术会议（conferences）上发表的硕士论文贡献多；一篇国外学术会议的论文又通常比无法发表的论文贡献多；在国际顶尖学术期刊上发表的论文通常比一篇二流的学术期刊论文贡献多。SCI有一种叫做 Impact Factor 的指数，统计一个期刊每篇论文被引述的次数。通常这个次数（或指数）愈高，对学术界的影响力就愈大。以机械视觉相关领域的期刊而言，Impact Factor 在 1.0 以上的期刊，都算是顶尖的期刊。这些期刊论文的作者，通常是国外顶尖学府的著名教授指导全球一流的博士生做出来的研究成果。 二、完成硕士论文所需要的能力 从前面的叙述可以归纳出来，完成硕士论文所需要的能力包括以下数项，依它们的培养先后次序逐项讨论。 （1）资料检索的能力：在给定（或自己拟定）的题目范围内，你必须有能力利用文资料索引系统，查出所有相关的论文，而无任何遗漏（否则你可能在论文口试时才发现同一个题目已经有人发表过了）。你到底要用什么样的关键词和查所程序去保证你已经找出所有相关的文献？这是第一个大的挑战。每一组关键词（包含联集与交集）代表一个论文所构成的集合，假如你用的关键词不恰当，你可能找到的集合太小，没有涵盖所有的相关文献；假如你用的关键词太一般化（譬如「image」），通常你找到的集合会太大，除了所有相关文献之外还加上好几十倍的毫不相关的文献。 （2）资料筛选的能力：即使你使用了恰当的搜寻策略，通常找到的文献集合都还是明显地比你所需要的集合大，而且通常文献比数大概在一两百篇或数百篇之间，而其中会和你的的研究子题直接且密切相关的论文，通常只有廿、卅篇左右。你如何可以只读论文的题目、摘要、简介和结论，而还没有完全看懂内文，就准确地判断出这篇论文中是否有值得你进一步参考的内容，以便快速地把需要仔细读完的论文从数百篇降低到廿、卅篇？这考验着你从事资料筛选的能力。 （3）期刊论文的阅读能力：期刊论文和大学部的课本截然不同。大学部的课本是寻次渐进地从最基本的知识背景逐步交代出整套有系统的知识，中间没有任何的跳跃，只要你逐页读下去，就可以整本都读懂，不需要在去别的地方找参考资料。但是期刊论文是没头没尾的十几页文献，只交代最核心的创意，并援引许多其它论文的研究成果（但只注明文献出处，而完全没有交代其内容）。因此，要读懂一篇论文，一定要同时读懂数篇或十数篇被援引的其它论文。偏偏，这十几篇被援引的论文又各自援引十数篇其它论文。因此，相对于大学部的教科书而言，期刊论文是一个极端没有系统的知识，必须要靠读者自己从几十篇论文中撷取出相关的片段，自己组织成一个有系统的知识，然后才有办法开始阅读与吸收。要培养出这种自己组织知识的能力，需要在学校靠着大量而持续的时间去摸索、体会，而不可能只利用业余的零星时间去培养。因此，一个大学毕业后就不再念研究所的学生，不管他在毕业生和大学毕业生最大的差别，就是：学士只学习过吸收系统知识的能力（也就是读别人整理、组织好的知识，典型的就是课本）；但硕士则学习过自己从无组织的知识中检索、筛选、组织知识的能力。 （4）期刊论文的分析能力：为了确定你的学位论文研究成果确实比所有相关的学术期刊论文都更适合处理你所拟定的应用场域，首先你必须要有能力逐篇分析出所有相关期刊论文的优点与缺点，以及自己的研究成果的优点与缺点，然后再拿他们来做比较，总结出你的论文的优点和缺点（限制）。但是，好的期刊论文往往是国外著名学府的名师和一流的博士生共同的研究成果，假如你要在锁定的应用场域上「打败」他们，突出自己的优点，这基本上是一个极端困难的挑战。即使只是要找出他们的缺点，都已经是一个相当困难的工作了。一个大学毕业生，四年下来都是假定「课本是对的」这样地学下来的，从来没有学习如何分析课本知识的优缺点，也就是「只有理解的能力，而没有批判的能力」。硕士生则必须要有「对一切既有进行精确批判」的能力。但是，这个批判并非个人好恶或情绪化的批判，而是真的找得到充分理由去支持的批判。这个批判的能力，让你有能力自己找到自己的优、缺点，因此也有机会自己精益求精。所以，一个大学毕业生在业界做事的时候，需要有人指导他（从事批判性检验），帮他找出缺点和建议改进的可能性。但是，一个严格训练过的合格硕士，他做事的时候应该是不需要有人在背后替他做检证，他自己就应该要有能力分析自己的优、缺点，主动向上级或平行单位要求支持。其实，至少要能够完成这个能力，才勉强可以说你是有「独立自主的判断能力」。 （5）创新的能力：许多大学毕业的工程师也能创新，但是硕士的创新是和全世界同一个学术团体内所有的名师和博士生挑战。因此，两者是站在不同的比较基础上在进行的：前者往往是一个企业内部的「闭门造车」，后者是一个全球的开放性竞争。其次，工程师的创新往往是无法加以明确证明其适用条件，但是学术的创新却必须要能够在创新的同时厘清这个创新的有效条件。因此，大学毕业生的主要能力是吸收既有知识，但硕士毕业生却应该要有能力创造知识。此外，台湾历年来工业产品的价位偏低，这一部分是因为国际大厂的打压以及国际消费者的信任不易建立。但是，另一方面，这是因为台湾的产品在品质上无法控制，因此只好被当作最粗糙的商品来贩卖。台湾的产品之所以无法有稳定的品质，背后的技术原因就是：各种创新都是只凭一时偶然的巧思，却没有办法进一步有系统地厘清这些巧思背后可以成立的条件。但是，创新其实是可以有一套「有迹可寻」的程序的，这是我最得意的心得，也是我最想教的。 三、为什么要坚持培养阅读与分析期刊论文的能力 我所以一直坚持要训练研究生阅读与分析期刊论文的能力，主要是为了学生毕业后中长期的竞争力着想。 台湾从来都只生产国外已经有的产品，而不事创新。假如国外企业界比国外学术的技术落后三年，而台湾的技术比国外技术落后五年，则台湾业界所需要的所有技术都可以在国外学术期刊上找到主要的理论依据和技术核心构想（除了一些技术的细节和 know how 之外）。因此，阅读期刊的能力是台湾想要保持领先大陆技术的必备条件。 此外，只要能够充分掌握阅读与分析期刊论文的技巧，就可以水到渠成地轻松进行「创新」的工作。所以，只要深入掌握到阅读与分析期刊论文的技巧，就可以掌握到大学生不曾研习过的三种能力：（1）自己从无组织的知识中检索、筛选、组织知识的能力、（2）对一切既有进行精确批判的独立自主判断能力、（3）创造新知识的能力。 创新的能力在台湾一直很少被需要（因为台湾只会从国外买整套设备、制程和设计与制造的技术）。但是，大陆已经成为全球廉价品制造中心，而台商为了降低成本也主动带技术到大陆设厂（包括现在的晶元代工），因此整个不具关键性技术的制造业都会持续往大陆移动；甚至 IC 的设计（尤其数字的部分）也无可避免地会迅速朝向「台湾开系统规格，进行系统整合，大陆在前述架构下开发特定数位模块」的设计代工发展。因此，未来台湾将必然会被逼着朝愈来愈创意密集的创意中心走（包括商务创意、经营创意、产品创意、与技术创新）。因此，不能因为今天台湾的业界不需要创新的能力，就误以为自己一辈子都不需要拥有创新的能力。 我在协助民间企业发展技术研发的过程中，碰到过一位三十多岁的厂长。他很聪明，但从小家穷，被环境逼着去念高工，然后上夜校读完工专。和动态性能（ bandwidth、response speed等）无关的技术他都很深入，也因为产品升级的需要而认真向我求教有关动态性能的基本观念。但是，怎么教他都不懂，就只因为他不懂工程数学。偏偏，工程数学不是可以在工厂里靠自修读会的。一个那么聪明的人，只因为不懂工数，就注定从三十岁以后一辈子无法在专业上继续成长！他高工毕业后没几年，廿多岁就当课长，家人与师长都以他为荣；卅岁当厂长，公司还给他技术股，前途无量；谁想得到他会在卅岁以后被逼着「或者升级，或者去大陆，或者失业」？ 每次想起这位厂长，看着迫不急待地要到台积电去「七年赚两千万退休金」的学生，或者只想学现成可用的技术而不想学研究方法的学生，我总忍禁不住地要想：十年后，我教过的学生里，会不会有一堆人就只因为不会读期刊论文而被逼提前退休？ 再者，技术的创新并不是全靠聪明。我熟谙一套技术创新的方法，只要学会分析期刊论文的优缺点，就可拿这套方法分析竞争对手产品的优缺点；而且，只要再稍微加工，就可以从这套优缺点的清单里找到突破瓶颈所需的关键性创意。这套创新程序，可以把「创新」变成不需要太多天分便可以完成的事，从而减轻创意的不定性与风险性。因此，只要会分析论文，几乎就可以轻易地组合出你所需要的绝大部分创意。聪明是不可能教的，但这套技巧却是可以教的；而且只要用心，绝大部分硕士生都可以学会。 就是因为这个原因，我的实验室整个训练的重心只有一个：通过每周一次的 group meeting，培养学生深入掌握阅读与分析期刊论文的技巧，进而培养他们在关键问题上突破与创新的能力。 四、期刊论文的分析技巧与程序 一般来讲，好的期刊论文有较多的创意。虽然读起来较累，但收获较多而深入，因此比较值得花心思去分析。读论文之前，参考SCI Impact Factor 及学长的意见是必要的。 一篇期刊论文，主要分成四个部分。 （1）Abstract： 说明这篇论文的主要贡献、方法特色与主要内容。最慢硕二上学期必须要学会只看 Abstract 和Introduction便可以判断出这篇论文的重点和你的研究有没有直接关连，从而决定要不要把它给读完。假如你有能力每三十篇论文只根据摘要和简介便能筛选出其中最密切相关的五篇论文，你就比别人的效率高五倍以上。以后不管是做事或做学术研究，都比别人有能力从更广泛的文献中挑出最值得参考的资料。 （2）Introduction： Introduction 的功能是介绍问题的背景和起源，交代前人在这个题目上已经有过的主要贡献，说清楚前人留下来的未解问题，以及在这个背景下这篇论文的想解决的问题和它的重要性。对初学的学生而言，从这里可以了解以前研究的概况。通常我会建议初学的学生，对你的题目不熟时，先把跟你题目可能相关的论文收集个 30～40篇，每篇都只读Abstract 和 Introduction，而不要读 Main Body（本文），只在必要时稍微参考一下后面的 Illustrative examples和 Conclusions，直到你能回答下面这三个问题：（2A）在这领域内最常被引述的方法有哪些？（2B）这些方法可以分成哪些主要派别？（2C）每个派别的主要特色（含优点和缺点）是什么？ 问题是，你怎么去找到这最初的30～40篇论文？有一种期刊论文叫做「review paper」，专门在一个题目下面整理出所有相关的论文，并且做简单的回顾。你可以在搜寻 Compendex 时在 keywords 中加一个「review」而筛选出这类论文。然后从相关的数篇review paper 开始，从中根据 title 与 Abstract 找出你认为跟你研究题目较相关的30～40篇论文。 通常只要你反复读过该领域内30～40篇论文的Abstract 和 Introduction，你就应该可以从Introduction的评论中回答（2A）和（2B）这两个问题。尤其要记得，当你阅读的目的是要回答（2A）和（2B）这两个问题时，你一定要先挑那些 Introduction写得比较有观念的论文念（很多论文的Introduction 写得像流水帐，没有观念，这种论文刚开始时不要去读它）。假如你读过假如30～40篇论文的 Abstract 和 Introduction之后，还是回答不了（2C），先做下述的工作。 你先根据（2A）的答案，把这领域内最常被引述的论文找齐，再把他们根据（2B）的答案分成派别，每个派别按日期先后次序排好。然后，你每次只重新读一派的 Abstract 和 Introduction（必要时简略参考内文，但目的只是读懂Introduction内与这派有关的陈述，而不需要真的看懂所有内文），照日期先后读 ，读的时候只企图回答一个问题：这一派的创意与主要诉求是什么？这样，你逐派逐派地把每一派的Abstract 和 Introduction 给读完，总结出这一派主要的诉求 、方法特色和优点（每一篇论文都会说出自己的优点，仔细读就不会漏掉）。 其次，你再把这些论文拿出来，但是只读Introduction，认真回答下述问题：「每篇论文对其它派别有什么批评？」然后你把读到的重点逐一记录到各派别的「缺点」栏内。 通过以上程序，你就应该可以掌握到（2A）、（2B）、和（2C）三个问题的答案。这时你对该领域内主要方法、文献之间的关系算是相当熟捻了，但是你还是只仔细 读完Abstract 和 Introduction而已，内文则只是笼统读过。 这时候，你已经掌握到这领域主要的论文，你可以用这些论文测试看看你用来搜寻这领域论文的 keywords 到底恰不恰当，并且用修正过的 keywords 再搜寻一次论文，把这领域的主要文献补齐，也把原来30～40篇论文中后来发现关系较远的论文给筛选掉，只保留大概20篇左右确定跟你关系较近的文献。如果有把握，可以甚至删除一两个你不想用的派别（要有充分的理由），只保留两、三个派别（也要有充分的理由）继续做完以下工作。 然后你应该利用（2C）的答案，再进一步回答一个问题（2D）：「这个领域内大家认为重要的关键问题有哪些？有哪些特性是大家重视的优点？有哪些特性是大家在意的缺点？这些优点与缺点通常在哪些应用场合时会比较被重视？在哪些应用场合时比较不会被重视？」然后，你就可以整理出这个领域（研究题目）主要的应用场合，以及这些应用场合上该注意的事项。 最后，在你真正开始念论文的 main body 之前，你应该要先根据（2A）和（2C的答案，把各派别内的论文整理在同一个档案夹里，并照时间先后次序排好。然后依照这些派别与你的研究方向的关系远近，一个派别一个派别地逐一把各派一次念完一派的 main bodies。 （3）Main body（含simulation and/or experimental examples）： 在你第一次有系统地念某派别的论文 main bodies 时，你只需要念懂：（3A）这篇论文的主要假设是什么（在什么条件下它是有效的），并且评估一下这些假设在现实条件下有多容易（或多难）成立。愈难成立的假设，愈不好用，参考价值也愈低。（3B）在这些假设下，这篇论文主要有什么好处。（3C）这些好处主要表现在哪些公式的哪些项目的简化上。至于整篇论文详细的推导过程，你不需要懂。除了三、五个关键的公式（最后在应用上要使用的公式，你可以从这里评估出这个方法使用上的方便程度或计算效率，以及在非理想情境下这些公式使用起来的可靠度或稳定性）之外，其它公式都不懂也没关系，公式之间的恒等式推导过程可以完全略过去。假如你要看公式，重点是看公式推导过程中引入的假设条件，而不是恒等式的转换。 但是，在你开始根据前述问题念论文之前，你应该先把这派别所有的论文都拿出来，逐篇粗略地浏览过去（不要勉强自己每篇或每行都弄到懂，而是轻松地读，能懂就懂，不懂就不懂），从中挑出容易念懂的 papers，以及经常被引述的论文。然后把这些论文照时间先后次序依序念下去。记得：你念的时候只要回答（3A）、（ 3B）、（3C）三个问题就好，不要念太细。 这样念完以后，你应该把这一派的主要发展过程，主要假设、主要理论依据、以及主要的成果做一个完整的整理。其次，你还要在根据（2D）的答案以及这一派的主要假设，进一步回答下一个问题：（3D）这一派主要的缺点有哪些。最后，根据（ 3A）、（3B）、（3C）、（3D）的答案综合整理出：这一派最适合什么时候使用，最不适合什么场合使用。 记住：回答完这些问题时，你还是不应该知道恒等式是怎么导出来的！ 当你是生手的时候，你要评估一个方法的优缺点时，往往必须要参考它Examples。但是，要记得：老练的论文写作高手会故意只 present 成功的案例而遮掩失败的案例。所以，simulation examples and/or experiments 很棒不一定表示这方法真的很好。你必须要回到这个方法的基本假设上去，以及他在应用时所使用的主要公式（resultant equations）去，凭自己的思考能力， 并且参考（2C）和（2D）的答案，自己问问看：当某某假设在某些实用场合上无法成立时，这个方法会不会出什么状况？猜一猜，预测一下这个方法应该会在哪些条件下（应用场合）表现优异，又会在哪些条件下（应用场合）出状况？根据这个猜测再检验一次simulation examples and/or experiments，看它的长处与短处是不是确实在这些examples 中充分被检验，且充分表现出来。 那么，你什么时候才需要弄懂一篇论文所有的恒等式推导过程，或者把整篇论文细细读完？NEVER！你只需要把确定会用到的部分给完全搞懂就好，不确定会不会用到的部分，只需要了解它主要的点子就够了。 硕士生和大学生最主要的差别：大学生读什么都必须要从头到尾都懂，硕士生只需要懂他用得着的部分就好了！大学生因为面对的知识是有固定的范围，所以他那样念。硕士生面对的知识是没有范围的，因此他只需要懂他所需要的细腻度就够了。硕士生必须学会选择性的阅读，而且必须锻炼出他选择时的准确度以及选择的速度，不要浪费时间在学用不着的细节知识！多吸收「点子」比较重要，而不是细部的知识。 五、方法与应用场合特性表（有迹可寻的创意程序） 试着想象说你从上图中论文阅读步骤的第（4）与（5）步骤分别获得以下两张表：譬如，当你的题目是「如何标定fiducial mark 之中心位置」，你就必须要仔细搜寻出文献上所有可能可以用来做这一个工作的方法。或许你找到的方法一共有四种，依序如下。譬如（随便乱举例），「方法一」可能表示：「以面积形心标定 fiducial mark 之中心位置」，「方法二」可能表示「以 Hugh transform标定 fiducial mark 之中心位置」，「方法三」可能表示：「以局部弧形 matching 的方法标定fiducial mark 之中心位置」，「方法四」可能表示：「以 ring code标定fiducial mark 之中心位置」。 这些方法各有它的特色（优缺点），譬如（随便乱举例），特性1可能表示「计算速度」（因此，根据上表左边第一个 row，可以发现：方法一的计算速度很快，方法二与方法三的计算速度很慢，而方法四的计算速度普通。其次，特性2可能代表「光源亮度不稳定时计算位置的误差大小」，特性3可能代表「噪声对计算出的位置干扰多大」，特性4可能代表「图形边缘有破损时计算的可靠度」，特性5可能代表「对象有彼此的遮蔽时方法的适用性」等等。所以，以上左图中第五个row为例，可以发现：当对象有彼此的遮蔽时，除方法二之外其它三个方法的适用性都很好。 但是，同样一个方法可能有许多不同的应用场合，而不同应用场合可能会对适用（或最佳）的方法有不同要求。所以，让我们来看右边的「问题特性分析表」。譬如（随便乱举例），应用甲可能是「标定fiducial mark 之中心位置」的方法在「电路插件组装（SMT）」里的应用，应用乙可能是「标定fiducial mark 之中心位置」的方法在「生物检验自动化影像处理」里的应用，而应用丙则可能是「标定 fiducial mark 之中心位置」的方法在「巡乂飞弹目标搜寻」里的应用。这三种应用场合更有其关注的特性。譬如，根据上面右表第二个 row 的资料，三种应用场合对特性2（光源亮度不稳定时计算位置的误差大小）都很在意。再譬如，根据上面右表第四个 row 的资料，三种应用场合中除了应用甲（电路插件组装（SMT））之外，其它两种应用场合对特性4（图形边缘有破损时计算的可靠度）都很在意。 那么，四个方法中哪个方法最好？你可能会回答说：「方法二！因为它的优点最多，缺点最少。」但是，这样的回答是错的！一个方法只有优缺点，而没有好坏。当它被用在一个适合表现其优点而不在乎其缺点的场合里，它就显得很好；但是，当它被用在一个不适合表现其优点而很在乎其缺点的场合里，它就显得很糟。譬如，方法二在应用场合乙，它的表现会非常出色（因为所有的优点刚好那个应用场合都在意，而所有的缺点刚好那个应用场合都不在意）；但是，方法二在应用场合甲里它的表现却会非常糟糕（它所有的缺点刚好那个应用场合都很在意，而它大部分的优点刚好那个应用场合却都不在意）。所以，必须要学会的第一件是就是：方法没有好坏，只有相对优缺点点；只有当方法的特性与应用场合的特性不合时，才能下结论说这方法「不适用」；二当当方法的特性与应用场合的特性吻合时，则下结论说这方法「很适用」。因此，一定要同时有方法特性表与应用场合特性分析表放在一起后，才能判断一个方法的适用性。 更重要的是：上面的方法与问题分析对照表还可以用来把「突破瓶颈所需的创意」简化成一种「有迹可寻」的工作。譬如，假定我们要针对应用甲发展一套适用的方法，首先我们要先从上右表中标定这个应用场合关心哪些问题特性。根据上右表第一个 column，甲应用场合只关心四个特性：特性1、2、3、5（即「计算速度」、「光源亮度不稳定时计算位置的误差大小」、「噪声对计算出的位置的干扰」、「对象有彼此的遮蔽时方法的适用性」）。那么，哪个方法最适用呢？看起来是方法 一，它除了特性2表现普通之外，其它三个特性的表现都很出色。但是，假如我们对方法一的表现仍不够满意，怎么去改善它？最简单的办法就是从上左表找现成的方法和方法一结合，产生出一个更适用的方法。因为方法一只有在特性2上面表现不够令人满意，所以我们就优先针对在特性2上面表现出色的其它方法加以研究。根据上左表，在特性2上面表现出色的方法有方法二和方法四，所以我们就去研究这两个方法和方法一结合的可能性。或许（随便举例）方法四的创意刚好可以被结合进方法一而改善方法一在特性2上面的表现，那么，我们就可以因此轻易地获得一个方法一的改良，从而突破甲应用场合没有适用方法的瓶颈。 有没有可能说单纯常识结合既有方法优点仍无法突破技术瓶颈的状况？可能有。这时候真的需要完全新颖的创意了。但是，这种时候很罕见。多半时候只要应用上一段的分析技巧就可以产生足以解决实用问题的创意了。至少，要产生出一篇学术期刊论文并非那么困难。 六、论文阅读的补充说明 硕士生开始学读期刊论文时，就容易犯的毛病就是戒除不掉大学部的习惯：（1）老是想逐行读懂，有一行读不懂就受不了。（2）不敢发挥自己的想象，读论文像在读教科书，论文没写的就不会，瘫痪在那里；被我逼着去自己猜测或想象时，老怕弄错作者的意思，神经绷紧，脑筋根本动不了。 大学毕业后（不管是念硕、博士或工作），可以参考的资料都没有秩序地交错成一团，而且永远都读不完。用大学生的心态读书，结果一定时间永远不够用。因此，每次读论文都一定要带着问题去读，每次读的时候都只是图回答你要回答的问题。因此，一定是选择性地阅读，一定要逐渐由粗而细地一层一层去了解。上面所规划的读论文的次序，就是由粗而细，每读完一轮，你对这问题的知识就增加一层。根据这一层知识就可以问出下一层更细致的问题，再根据这些更细致的问题去重读，就可以理解到更多的内容。因此，一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。 这样读还有一个好处：第一轮读完后，可以根据第一轮所获得的知识判断出哪些论文与你的议题不相关，不相关的就不需要再读下去了。这样才可以从广泛的论文里逐层准确地筛选出你真正非懂不可的部分。不要读不会用到的东西，白费的力气必须被极小化！其实，绝大部分论文都只需要了解它的主要观念（这往往比较容易），而不需要了解它的详细推导过程（这反而比较费时）。 其次，一整批一起读还有一个好处：同一派的观念，有的作者说得较易懂，有的说得不清楚。整批读略过一次之后，就可以规划出一个你以为比较容易懂的阅读次序，而不要硬碰硬地在那里撞墙壁。你可以从甲论文帮你弄懂以论文的一个段落，没人说读懂甲论文只能靠甲论文的信息。所以，整批阅读很像在玩跳棋，你要去规划出你自己阅读时的「最省力路径」。 大学部学生读东西一定要循规蹈矩，你还没修过机械视觉相关课程之前可能也只好循规蹈矩地逐行去念。但是一旦修过机械视觉相关课程，许多论文中没被交代的段落你也已经可以有一些属于你的想象（虽然有可能猜错，尤其刚开始时经常猜错，但没关系，下面详述）。这些想象往往补足论文跳跃处最快速的解决方案。其实，一个大学毕业生所学已经很多了，对许多是都可以有一个不太离谱的想象能力。但是大部分学生却根本不敢去想象。我读论文远比学生快，分析远比学生深入，主要的是我敢想象与猜测，而且多年训练下来想象与猜测的准确度很高。所以，许多论文我根本不是「读懂」的，而是「猜对」了！ 假如猜错了怎么办？不用怕！猜完一后要根据你的猜测在论文里找证据，用以判断你的猜测对不对。猜对了，就用你的猜测（其实是你的推理架构）去吸收作者的资讯与创意（这会比从头硬生生地去迁就作者的思路轻松而容易）；猜错了，论文理会有一些信息告诉你说你错了，而且因为猜错所以你读到对的答案时反而印象更深刻。 七、论文报告的要求与技巧 报告一篇论文，我要求做到以下部分（依报告次序排列）： （1） 投影片第一页必须列出论文的题目、作者、论文出处与年份。 （2） 以下每一页投影片只能讲一个观念，不可以在一张投影片里讲两个观念。 （3） 说明这篇论文所研究的问题的重点，以及这个问题可能和工业界的哪些应用相关。 （4） 清楚交代这篇论文的主要假设，主要公式，与主要应用方式（以及应用上可能的解题流程）。 （5） 说明这篇论文的范例（simulation examples and/or experiments），预测这个方法在不同场合时可能会有的准确度或好用的程度 （6） 你个人的分析、评价与批评，包括：（6A）这篇论文最主要的创意是什么？（6B）这些创意在应用上有什么好处？（6C）这些创意和应用上的好处是在哪些条件下才能成立？（6D）这篇论文最主要的缺点或局限是什么？（6E）这些缺点或局限在应用上有什么坏处？（6F）这些缺点和应用上的坏处是因为哪些因素而引入的？（6G）你建议学长学弟什么时候参考这篇论文的哪些部分（点子）？ 一般来讲，刚开始报告论文（硕一上学期）时只要做到能把前四项要素说清楚就好了，但是硕一结束后（暑假开始）必须要设法做到六项要素都能触及。硕二下学期开始的时候，必须要做到六项都能说清楚。 注意：读论文和报告论文时，最重要的是它的创意和观念架构，而不是数学上恒等式推导过程的细节（顶多只要抓出关键的 equation 去弩懂以及说明清楚即可）。你报告观念与分析创意，别人容易听懂又觉得有趣；你讲恒等式，大家不耐烦又浪费时间。]]></content>
      <categories>
        <category>鸡汤</category>
      </categories>
      <tags>
        <tag>day day up</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Complete Tutorial to Learn Data Science with Python from Scratch(3)]]></title>
    <url>%2F2017%2F07%2F27%2FA%20Complete%20Tutorial(3)%2F</url>
    <content type="text"><![CDATA[4. Data Munging in Python : Using PandasFor those, who have been following, here are your must wear shoes to start running. Data munging – recap of the need While our exploration of the data, we found a few problems in the data set, which needs to be solved before the data is ready for a good model. This exercise is typically referred as “Data Munging”. Here are the problems, we are already aware of: There are missing values in some variables. We should estimate those values wisely depending on the amount of missing values and the expected importance of variables.While looking at the distributions, we saw that ApplicantIncome and LoanAmount seemed to contain extreme values at either end. Though they might make intuitive sense, but should be treated appropriately.In addition to these problems with numerical fields, we should also look at the non-numerical fields i.e. Gender, Property_Area, Married, Education and Dependents to see, if they contain any useful information. If you are new to Pandas, I would recommend reading this article(Pandas的12种奇淫异巧) before moving on. It details some useful techniques of data manipulation. 数据修复 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1df=pd.read_csv("G:/Datahack/Loanprediction/TrainFile/train.csv") #读取数据 Check missing values in the datasetLet us look at missing values in all the variables because most of the models don’t work with missing data and even if they do, imputing them helps more often than not. So, let us check the number of nulls / NaNs in the dataset.(检查一下数据集的缺失值) 12df.apply(lambda x:sum(x.isnull()),axis=0) # 匿名函数，axis=0表示针对每列应该用，可以看看#与 df.apply(lambda x:sum(x.isnull()),axis=1)的区别 Loan_ID 0 Gender 13 Married 3 Dependents 15 Education 0 Self_Employed 32 ApplicantIncome 0 CoapplicantIncome 0 LoanAmount 22 Loan_Amount_Term 14 Credit_History 50 Property_Area 0 Loan_Status 0 dtype: int64 可以看到Gender丢失13个，Married丢失3个，Dependents丢失15个，Self_Employed丢失32个，LoanAmount丢失22个，Loan_Amount_Term丢失14个，Credit_History丢失50个。 Though the missing values are not very high in number, but many variables have them and each one of these should be estimated and added in the data. Get a detailed view on different imputation techniques through this article(A Comprehensive Guide to Data Exploration). Note: Remember that missing values may not always be NaNs. For instance, if the Loan_Amount_Term is 0, does it makes sense or would you consider that missing? I suppose your answer is missing and you’re right. So we should check for values which are unpractical.(值得注意的是，并非所有丢失值都是NaN,也可能是0或者其他不合理的值) How to fill missing values in LoanAmount?如何填充丢失值是一个问题，下面以LoanAmount为例。 There are numerous ways to fill the missing values of loan amount – the simplest being replacement by mean, which can be done by following code:（最简单的方法，以平均值进行填充） 1df['LoanAmount'].fillna(df['LoanAmount'].mean(),inplace=True)# 平均值填充 The other extreme could be to build a supervised learning model to predict loan amount on the basis of other variables and then use age along with other variables to predict survival.(另一个方法是建立一个有监督学习模型利用其他变量来预测未知值） Since, the purpose now is to bring out the steps in data munging, I’ll rather take an approach, which lies some where in between these 2 extremes. A key hypothesis is that the whether a person is educated or self-employed can combine to give a good estimate of loan amount.（但这里只是一个入门教程，采用一种介于上述两种方法之间的手段来填充丢失值） First, let’s look at the boxplot to see if a trend exists: 1df.boxplot(column='LoanAmount',by=['Education','Self_Employed']) &lt;matplotlib.axes._subplots.AxesSubplot at 0x9d956d8&gt; Thus we see some variations in the median of loan amount for each group and this can be used to impute the values. But first, we have to ensure that each of Self_Employed and Education variables should not have a missing values.(由上面的Boxplot可以看出，不同Education和Self_Employed的组合对LoanAmount的中位数还是有影响的。因此，可以用Education和Self_Employed来进行推断。但在此之前，必须保证Education和Self_Employed是完整的) As we say earlier, Self_Employed has some missing values. Let’s look at the frequency table: 1df['Self_Employed'].value_counts() No 500 Yes 82 Name: Self_Employed, dtype: int64 Since ~86% values are “No”, it is safe to impute the missing values as “No” as there is a high probability of success. This can be done using the following code: 1df['Self_Employed'].fillna('No',inplace=True) Now, we will create a Pivot table, which provides us median values for all the groups of unique values of Self_Employed and Education features. Next, we define a function, which returns the values of these cells and apply it to fill the missing values of loan amount:(由Education和Self_Employed两列所确定的LoanAmount的中位数来填充LoanAmount的丢失值) 1234567table = df.pivot_table(values='LoanAmount', index='Self_Employed' , columns='Education', aggfunc=np.median)# Define function to return value of this pivot_tabledef fage(x): return table.loc[x['Self_Employed'],x['Education']]# Replace missing valuesdf['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True) How to treat for extreme values in distribution of LoanAmount and ApplicantIncome ?Let’s analyze LoanAmount first. Since the extreme values are practically possible, i.e. some people might apply for high value loans due to specific needs. So instead of treating them as outliers, let’s try a log transformation to nullify their effect:(使用Log函数消除极端值的影响） 12df['LoanAmount_log']=np.log(df['LoanAmount'])df['LoanAmount_log'].hist(bins=20) &lt;matplotlib.axes._subplots.AxesSubplot at 0x9f000f0&gt; Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided. Coming to ApplicantIncome. One intuition can be that some applicants have lower income but strong support Co-applicants. So it might be a good idea to combine both incomes as total income and take a log transformation of the same. 123df['TotalIncome']=df['ApplicantIncome']+df['CoapplicantIncome']df['TotalIncome_log']=np.log(df['TotalIncome'])df['LoanAmount_log'].hist(bins=20) &lt;matplotlib.axes._subplots.AxesSubplot at 0xa007da0&gt; Now we see that the distribution is much better than before. I will leave it upto you to impute the missing values for Gender, Married, Dependents, Loan_Amount_Term, Credit_History. Also, I encourage you to think about possible additional information which can be derived from the data. For example, creating a column for LoanAmount/TotalIncome might make sense as it gives an idea of how well the applicant is suited to pay back his loan. 1df.apply(lambda x:sum(x.isnull()),axis=0) Loan_ID 0 Gender 13 Married 3 Dependents 15 Education 0 Self_Employed 0 ApplicantIncome 0 CoapplicantIncome 0 LoanAmount 0 Loan_Amount_Term 14 Credit_History 50 Property_Area 0 Loan_Status 0 LoanAmount_log 0 TotalIncome 0 TotalIncome_log 0 dtype: int64 丢失的还有Gender,Married,Dependents,Loan_Amount_Term,Credit_History5个。其中Married比较好处理，直接以大多数类别填充，先把它处理啦。 1df['Married'].value_counts() Yes 398 No 213 Name: Married, dtype: int64 1df['Married'].fillna('Yes',inplace=True) 接着处理Gender(性别),缺了13个。直观上，先看看Gender和Married、Education、Self_Employed、Property_Area的关系。 1234def percConvert(ser): return ser/float(ser[-1])pd.crosstab(df['Married'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Married No 0.380952 0.619048 1.0 Yes 0.081841 0.918159 1.0 All 0.186356 0.813644 1.0 上表可以看出已婚人士中，Male占0.918，那假如某人性别未知，只知其已婚，是不是有0.918的概率判断其为男性呢？ 1pd.crosstab(df['Education'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Education Graduate 0.196581 0.803419 1.0 Not Graduate 0.150376 0.849624 1.0 All 0.186356 0.813644 1.0 上表可以看出性别与其Education关系好像并不大。 1pd.crosstab(df['Self_Employed'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Self_Employed No 0.185468 0.814532 1.0 Yes 0.192308 0.807692 1.0 All 0.186356 0.813644 1.0 上表可以看出性别与其Self_Employed关系好像也并不大。 1pd.crosstab(df['Dependents'],df['Gender'],margins=True).apply(percConvert,axis=1) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Gender Female Male All Dependents 0 0.236686 0.763314 1.0 1 0.188119 0.811881 1.0 2 0.070707 0.929293 1.0 3+ 0.062500 0.937500 1.0 All 0.186007 0.813993 1.0 1234567def coding(col,codeDict): colCoded=pd.Series(col,copy=True) for key,value in codeDict.items(): colCoded.replace(key,value,inplace=True) return colCodeddf['Gender']=coding(df['Gender'],&#123;'Male':1,'Female':0&#125;) #将‘Gender'转换为0,1表示（男士：1，女生：0）table2=df.pivot_table(values=['Gender'],index=['Married','Dependents'],aggfunc=np.median) 1234567#pd.crosstab(df['Married'],df['Dependents'],margins=True).apply(percConvert,axis=1)def dependent(x): if x['Married']=='No': return '0' else: return '1'df['Dependents'].fillna(df[df['Dependents'].isnull()].apply(dependent,axis=1),inplace=True) 12345678# Define function to return value of this pivot_tabledef fage(x): if table2.loc[x['Married'],x['Dependents']].values[0]==1.0: return 1.0 else: return 0.0# Replace missing valuesdf['Gender'].fillna(df[df['Gender'].isnull()].apply(fage, axis=1), inplace=True) 1df.apply(lambda x:sum(x.isnull()),axis=0) Loan_ID 0 Gender 0 Married 0 Dependents 0 Education 0 Self_Employed 0 ApplicantIncome 0 CoapplicantIncome 0 LoanAmount 0 Loan_Amount_Term 14 Credit_History 50 Property_Area 0 Loan_Status 0 LoanAmount_log 0 TotalIncome 0 TotalIncome_log 0 dtype: int64 可以看到只剩下Loan_Amount_Term和Credit_History没有处理啦，因为Credit_History由前面分析，对于结果来说是非常重要的，那么就放在最后处理好了。现在来对Loan_Amount_Term进行分析，Loan_Amount_Term是数值型。 1df.boxplot(column='Loan_Amount_Term',by=['Education','Self_Employed']) &lt;matplotlib.axes._subplots.AxesSubplot at 0xbc43d30&gt; 12termMedian=df['Loan_Amount_Term'].median()df['Loan_Amount_Term'].fillna(termMedian,inplace=True)#直接由中位数填充好了…… 按理说，Credit_History应该是根据一个人的性别，婚姻，家属，教育等申请之前的状态有很大关系，而与当前该次申请贷款的数额、期限等业务关系不大。 1df.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 614 entries, 0 to 613 Data columns (total 17 columns): Loan_ID 614 non-null object Gender 614 non-null float64 Married 614 non-null object Dependents 614 non-null object Education 614 non-null object Self_Employed 614 non-null object ApplicantIncome 614 non-null int64 CoapplicantIncome 614 non-null float64 LoanAmount 614 non-null float64 Loan_Amount_Term 614 non-null object Credit_History 564 non-null float64 Property_Area 614 non-null object Loan_Status 614 non-null object LoanAmount_log 614 non-null float64 TotalIncome 614 non-null float64 TotalIncome_log 614 non-null float64 LoanAmount/TotalIncome 614 non-null float64 dtypes: float64(8), int64(1), object(8) memory usage: 81.6+ KB 123456from sklearn.preprocessing import LabelEncodervar_mod=['Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']le=LabelEncoder()for i in var_mod: df[i]=le.fit_transform(df[i])df.dtypes#将以上非数值型，转化为数值型量 Loan_ID object Gender float64 Married int64 Dependents int64 Education int64 Self_Employed int64 ApplicantIncome int64 CoapplicantIncome float64 LoanAmount float64 Loan_Amount_Term object Credit_History float64 Property_Area int64 Loan_Status int64 LoanAmount_log float64 TotalIncome float64 TotalIncome_log float64 LoanAmount/TotalIncome float64 dtype: object 5. Building a Predictive Model in PythonAfter, we have made the data useful for modeling, let’s now look at the python code to create a predictive model on our data set. Skicit-Learn (sklearn) is the most commonly used library in Python for this purpose and we will follow the trail. I encourage you to get a refresher on sklearn through this article. Next, we will import the required modules. Then we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores. Since this is an introductory article, I will not go into the details of coding. Please refer to this article for getting details of the algorithms with R and Python codes. Also, it’ll be good to get a refresher on cross-validation through this article, as it is a very important measure of power performance. 123456#Import models from scikit learn module:from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import KFoldfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifier, export_graphvizfrom sklearn import metrics 1234567891011121314151617181920212223242526272829303132#Generic function for making a classification model and accessing performance:def classification_model(model,data,predictors,outcome): #Fit the model: model.fit(data[predictors],data[outcome]) #Make predictions on training set: predictions=model.predict(data[predictors]) #Print accuracy accuracy=metrics.accuracy_score(predictions,data[outcome]) print("Accuracy : %s" % "&#123;0:.3%&#125;".format(accuracy)) #Perform k-fold cross-validation with 5 folds kf=KFold(n_splits=5).split(data) error=[] for train,test in kf: #Filter training data train_predictors=(data[predictors].iloc[train,:]) #The target we're using to train the algorithm. train_target=data[outcome].iloc[train] #Training the algorithm using the predictors and target. model.fit(train_predictors,train_target) #Record error from each cross-validation run error.append(model.score(data[predictors].iloc[test,:],data[outcome].iloc[test])) print("Cross-Validation Score : %s" % "&#123;0:.3%&#125;".format(np.mean(error))) #Fit the model again so that it can be refered outside the function: model.fit(data[predictors],data[outcome]) 1creditTrain=df[df['Credit_History'].notnull()] #以Credit_History为输出进行预测 1234outcome_var='Credit_History'model=LogisticRegression()#模型选用普通的线性回归predictor_var=['Gender','Married','Education','TotalIncome']classification_model(model,creditTrain,predictor_var,outcome_var) Accuracy : 84.220% Cross-Validation Score : 84.218% 1234outcome_var='Credit_History'model=DecisionTreeClassifier()#模型改为决策树predictor_var=['Gender','Married','Education','TotalIncome']classification_model(model,creditTrain,predictor_var,outcome_var) Accuracy : 99.113% Cross-Validation Score : 74.651% 1234outcome_var='Credit_History'model=RandomForestClassifier(n_estimators=25,min_samples_split=25,max_depth=7,max_features=1)#模型改为Random Forestpredictor_var=['Gender','Married','Education','TotalIncome']classification_model(model,creditTrain,predictor_var,outcome_var) Accuracy : 84.929% Cross-Validation Score : 83.864% 12creditTest=df[df['Credit_History'].isnull()]#以Credit_History丢失值作为测试集，进行预测填充丢失值creditPredictions=model.predict(creditTest[predictor_var]) 1df.loc[df['Credit_History'].isnull(),'Credit_History']=creditPredictions #以预测值进行填充 Logistic RegressionLet’s make our first Logistic Regression model. One way would be to take all the variables into the model but this might result in overfitting (don’t worry if you’re unaware of this terminology yet). In simple words, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well. Read more about Logistic Regression. We can easily make some intuitive hypothesis to set the ball rolling. The chances of getting a loan will be higher for: Applicants having a credit history (remember we observed this in exploration?) Applicants with higher applicant and co-applicant incomes Applicants with higher education level Properties in urban areas with high growth perspectives So let’s make our first model with ‘Credit_History’. 1234outcome_var='Loan_Status'model=LogisticRegression()predictor_var=['Credit_History']#仅仅使用一个特征来预测classification_model(model,df,predictor_var,outcome_var) Accuracy : 80.945% Cross-Validation Score : 80.946% 123#再加上一些特征predictor_var=['Credit_History','Education','Married','Self_Employed','Property_Area','LoanAmount/TotalIncome']classification_model(model,df,predictor_var,outcome_var) Accuracy : 80.945% Cross-Validation Score : 80.946% Generally we expect the accuracy to increase on adding variables. But this is a more challenging case. The accuracy and cross-validation score are not getting impacted by less important variables. Credit_History is dominating the mode. We have two options now:(可以看到使用线性回归方法，在增加更多特征的时候，预测效果并没有很好改变，几乎和原来一样。这时候就需要考虑以下两种方法。） Feature Engineering: dereive new information and try to predict those. I will leave this to your creativity. Better modeling techniques. Let’s explore this next. Decision TreeDecision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model. Read more about Decision Trees. 123model=DecisionTreeClassifier()#换模型predictor_var=['Credit_History','Gender','Married','Education']classification_model(model,df,predictor_var,outcome_var) Accuracy : 80.945% Cross-Validation Score : 80.946% Here the model based on categorical variables is unable to have an impact because Credit History is dominating over them. Let’s try a few numerical variables: 12predictor_var=['Credit_History','Loan_Amount_Term','LoanAmount_log']classification_model(model,df,predictor_var,outcome_var) Accuracy : 88.925% Cross-Validation Score : 69.371% Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Let’s try an even more sophisticated algorithm and see if it helps: Random ForestRandom forest is another algorithm for solving the classification problem. Read more about Random Forest. An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features. 1234model=RandomForestClassifier(n_estimators=100)predictor_var=['Gender','Married','Dependents','Education','Self_Employed','Loan_Amount_Term','Credit_History', 'Property_Area','LoanAmount_log','TotalIncome_log','LoanAmount/TotalIncome']classification_model(model,df,predictor_var,outcome_var) Accuracy : 100.000% Cross-Validation Score : 78.665% Here we see that the accuracy is 100% for the training set(显然过拟合). This is the ultimate case of overfitting and can be resolved in two ways: Reducing the number of predictors Tuning the model parameters Let’s try both of these. First we see the feature importance matrix from which we’ll take the most important features. 123#Create a series with feature importances:featimp=pd.Series(model.feature_importances_,index=predictor_var).sort_values(ascending=False)print(featimp) Credit_History 0.263870 LoanAmount/TotalIncome 0.192546 TotalIncome_log 0.182355 LoanAmount_log 0.164331 Property_Area 0.041543 Dependents 0.041328 Loan_Amount_Term 0.035326 Married 0.023571 Gender 0.019269 Education 0.018238 Self_Employed 0.017623 dtype: float64 Let’s use the top 6 variables for creating a model. Also, we will modify the parameters of random forest model a little bit: 1234model=RandomForestClassifier(n_estimators=25,min_samples_split=25,max_depth=7,max_features=1)predictor_var=['TotalIncome_log','LoanAmount_log','Credit_History','LoanAmount/TotalIncome','Property_Area', 'Dependents']classification_model(model,df,predictor_var,outcome_var) Accuracy : 83.550% Cross-Validation Score : 80.782% Notice that although accuracy reduced, but the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. But the output should stay in the ballpark. You would have noticed that even after some basic parameter tuning on random forest, we have reached a cross-validation accuracy only slightly better than the original logistic regression model. This exercise gives us some very interesting and unique learning: Using a more sophisticated model does not guarantee better results. Avoid using complex modeling techniques as a black box without understanding the underlying concepts. Doing so would increase the tendency of overfitting thus making your models less interpretable Feature Engineering is the key to success. Everyone can use an Xgboost models but the real art and creativity lies in enhancing your features to better suit the model. So are you ready to take on the challenge? Start your data science journey with Loan Prediction Problem. 参考文章：https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Complete Tutorial to Learn Data Science with Python from Scratch(1)]]></title>
    <url>%2F2017%2F07%2F25%2FA%20Complete%20Tutorail(1)%2F</url>
    <content type="text"><![CDATA[最近一段时间进行大论文研究方向的调研，主要是关于电子系统或设备故障预测和健康管理（PHM）这一块。也跟老板沟通了，大致就做这一块了。研一在实验室除了上理论课之外，就负责实验室项目中上位机软件编程，也没掌握啥儿高大上的东西。现在论文大方向基本定了，趁着暑假想掌握一些数据分析(Data Analysis)方面的基本技术，包括（Python及其相关库、机器学习、数据挖掘算法等）。 在https://www.analyticsvidhya.com网站里看到一篇很不错的博客，跟着学习了一下。算是对前一段时间Numpy、Pandas、Scipy库的复习运用吧，整体上对用Python进行机器学习过程有一个把握。 目录(Table of Contents) Python基础知识(Basics of Python for Data Analysis) Why learn Python for data analysis? Python 2.7 vs 3.4 How to install Python? Running a few simple program in Python Python库和数据结构(Python libraries and data structures) Python Data Structures Python Iteration and Conditional Constructs Python Libraries 使用Pandas库探索分析(Exploratory analysis in Python using Pandas) Introduction to series and dataframes Analytics Vidhy a dataset- Loan Prediction Problem 使用Pandas数据修复(Data Munging in Python using Pandas) 建立预测模型(Building a Predictive Model in Python) Logistic Regression Decision Tree Random Forest 1、Python基础知识Why learn Python for data analysis? - 开源、免费安装 - 丰富的在线社区论坛 - 简单易学 - 可以成为Data Science的一种标准语言 但是缺点也显而易见， - 解释型语言，速度慢 Python 2.7 vs 3.4 这是一个非常具有争议性的话题，但是，我个人感觉，如果现在入门学习一定要用Python3了。 How to install Python? 有两种方法： -直接从Python官网上下载安装包，一步一步配置环境，需要什么包装什么包 -直接安装Anaconda,基本库都预安装了，啥事儿没有，不过要自己百度Anaconda教程，看看如何使用，挺简单的！ 推荐使用第二种方法，特别是国内&amp;Windows系统，自己配置Python环境，安装相关库真的是非常麻烦的！ 2、Python库和数据结构Python Data Structures Lists-Lists 是Python中使用最为频繁，功能也最为丰富的数据结构。可以简单的由逗号和中括号定义。Lists可以包含不同类型的元素，但是使用时基本都是同类型的。Python中lists是可变的，list中的每个元素也都可以改写。 ListsA list can be simply defined by writing comma separated values in square brackets. 1squares_list=[0,1,4,9,16,25] 1squares_list [0, 1, 4, 9, 16, 25] Individual elements of a list can be accessed by writing the index number in square bracket. Please note that the first index of a list is 0 and not 1 1squares_list[0]#Indexing returns the item 0 A range of script can be accessed by having first index and last index 1squares_list[2:4] #Slicing returns a new list [4, 9] A Negative index access the list from end 1squares_list[-2] #It should return the second last element in the list 16 A few common methods applicable to lists include: append(),extend(),insert(),remove(),pop(),count(),sort(),reverse() Strings- Strings 可以简单的用单引号(‘),双引号(“),三引号( ”’)表示。 Strings用三引号时可以包含多行，因此在Python docstrings中使用较多。‘\’是一个换行符，另起一行。但是需要注意Python strings是不可变的，不能改变strings! StringsA string can be simply defined by using single(‘),double(‘’) or triple(‘’’) quotation 1234greeting='Hello'print(greeting[1])print(len(greeting))print(greeting+'World') e 5 HelloWorld Raw strings can be used to pass on string as is. Python interpretter does not alter the string, if you specify a string to be raw. Raw string can be defined by adding r to the string 12stmt=r'\n is a newline character by default.'print(stmt) \n is a newline character by default. Python strings are immutable and hence can be changed. Doing so will result in an error. Common string methods include lower(),strip(),isdigit(),isspace(),find(),replace(),split() and join(). These are usually very helpful when you need to perform data mainpulation or cleaning on text fields. Tuples - 元组可以用逗号和圆括号表示。元组是不可变的，输出由圆括号包围。尽管元组是不可变的，但是可以包含可变元素。 因为元组是不可变的，在处理上相对于List更快。可以使用tuple代替使用不会更改的list. TuplesA tuple is represented by a number of values separated by commas. 1tuple_example=0,1,4,9,16,25 1tuple_example (0, 1, 4, 9, 16, 25) 1tuple_example[2] 4 1tuple_example[2]=6 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-12-59fc5567f2da&gt; in &lt;module&gt;() ----&gt; 1 tuple_example[2]=6 TypeError: &apos;tuple&apos; object does not support item assignment Dictionary -Dictionary是一种无序的key-value对集合，同一Dictionary中Key必须是唯一的。用大括号变可以建立一个空的Dictionary. DictionaryA dictionary is an unordered set of key: value pairs, with the requirement that the keys are unique (within one dictionary). A pair of braces creates an empty dictionary:{} 12extensions=&#123;'Kunal':9073,'Tavish':9128,'Sunil':9223,'Nitin':9330&#125;extensions {&apos;Kunal&apos;: 9073, &apos;Nitin&apos;: 9330, &apos;Sunil&apos;: 9223, &apos;Tavish&apos;: 9128} 12extensions['Mukesh']=9150extensions {&apos;Kunal&apos;: 9073, &apos;Mukesh&apos;: 9150, &apos;Nitin&apos;: 9330, &apos;Sunil&apos;: 9223, &apos;Tavish&apos;: 9128} 1extensions.keys() dict_keys([&apos;Kunal&apos;, &apos;Tavish&apos;, &apos;Sunil&apos;, &apos;Nitin&apos;, &apos;Mukesh&apos;]) Python Iteration and Conditional ConstructsLike most languages, Python also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax: 12for i in [Python Iterable]: expression(i) Here “Python Iterable” can be a list, tuple or other advanced data structures which we will explore in later sections. Let’s take a look at a simple example, determining the factorial of a number. 123fact=1for i in range(1,N+1): fact *=i Coming to conditional statements, these are used to execute code fragments based on a condition. The most commonly used construct is if-else, with following syntax: 1234if [condition]: __execution if true__else: __execution if false__ For instance, if we want to print whether the number N is even or odd: 1234if N%2 == 0: print('Even')else: print('Odd') Now that you are familiar with Python fundamentals, let’s take a step further. Python LibrariesLets take one step ahead in our journey to learn Python by getting acquainted with some useful libraries. The first step is obviously to learn to import them into our environment. There are several ways of doing so in Python: 1import math as m 1from math import * In the first manner, we have defined an alias m to library math. We can now use various functions from math library (e.g. factorial) by referencing it using the alias m.factorial(). In the second manner, you have imported the entire name space in math i.e. you can directly use factorial() without referring to math. 推荐使用前一种导入方法。 科学计算和数据分析常用的库： NumPy stands for Numerical Python. The most powerful feature of NumPy is n-dimensional array. This library also contains basic linear algebra functions, Fourier transforms, advanced random number capabilities and tools for integration with other low level languages like Fortran, C and C++ SciPy stands for Scientific Python. SciPy is built on NumPy. It is one of the most useful library for variety of high level science and engineering modules like discrete Fourier transform, Linear Algebra, Optimization and Sparse matrices. Matplotlib for plotting vast variety of graphs, starting from histograms to line plots to heat plots.. You can use Pylab feature in ipython notebook (ipython notebook –pylab = inline) to use these plotting features inline. If you ignore the inline option, then pylab converts ipython environment to an environment, very similar to Matlab. You can also use Latex commands to add math to your plot. Pandas for structured data operations and manipulations. It is extensively used for data munging and preparation. Pandas were added relatively recently to Python and have been instrumental in boosting Python’s usage in data scientist community. Scikit Learn for machine learning. Built on NumPy, SciPy and matplotlib, this library contains a lot of effiecient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction. Statsmodels for statistical modeling. Statsmodels is a Python module that allows users to explore data, estimate statistical models, and perform statistical tests. An extensive list of descriptive statistics, statistical tests, plotting functions, and result statistics are available for different types of data and each estimator. Seaborn for statistical data visualization. Seaborn is a library for making attractive and informative statistical graphics in Python. It is based on matplotlib. Seaborn aims to make visualization a central part of exploring and understanding data. Bokeh for creating interactive plots, dashboards and data applications on modern web-browsers. It empowers the user to generate elegant and concise graphics in the style of D3.js. Moreover, it has the capability of high-performance interactivity over very large or streaming datasets. Blaze for extending the capability of Numpy and Pandas to distributed and streaming datasets. It can be used to access data from a multitude of sources including Bcolz, MongoDB, SQLAlchemy, Apache Spark, PyTables, etc. Together with Bokeh, Blaze can act as a very powerful tool for creating effective visualizations and dashboards on huge chunks of data. Scrapy for web crawling. It is a very useful framework for getting specific patterns of data. It has the capability to start at a website home url and then dig through web-pages within the website to gather information. SymPy for symbolic computation. It has wide-ranging capabilities from basic symbolic arithmetic to calculus, algebra, discrete mathematics and quantum physics. Another useful feature is the capability of formatting the result of the computations as LaTeX code. Requests for accessing the web. It works similar to the the standard python library urllib2 but is much easier to code. You will find subtle differences with urllib2 but for beginners, Requests might be more convenient. 还有一些其他的库可能会用到： os for Operating system and file operations networkx and igraph for graph based data manipulations regular expressions for finding patterns in text data BeautifulSoup for scrapping web. It is inferior to Scrapy as it will extract information from just a single webpage in a run. Now that we are familiar with Python fundamentals and additional libraries, lets take a deep dive into problem solving through Python. Yes I mean making a predictive model! In the process, we use some powerful libraries and also come across the next level of data structures. We will take you through the 3 key phases: Data Exploration (分析数据） – finding out more about the data we have Data Munging （数据修复） – cleaning the data and playing with it to make it better suit statistical modeling Predictive Modeling （预测模型） – running the actual algorithms and having fun]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Complete Tutorial to Learn Data Science with Python from Scratch(2)]]></title>
    <url>%2F2017%2F07%2F25%2FA%20Complete%20Tutorail(2)%2F</url>
    <content type="text"><![CDATA[3. Exploratory analysis in Python using PandasPandas是Python中最为实用的一种数据分析库，接下来会简单介绍Pandas，然后利用Pandas读取Analytics Vidhya竞赛中的一个数据集进行探索分析，建立一个简单的分类算法来解决这个问题。 Introduction to Series and Dataframe开始读取数据集之前，先来理解Pandas的两个关键数据结构-Series &amp; DataFrame Series 可以理解为一个1维带标签（labelled）或索引(indexed)的数组,可以通过labels来读取series中的元素（elements). DataFrame 则与Excel表格很相像，可以通过Column names来引用某一列，可以使用Row numbers 来引用某一行。区别在于，在DataFrame中，Column names被称为Column,Row numbers被称为row index. Series and dataframes form the core data model for Pandas in Python. The data sets are first read into these dataframes and then various operations (e.g. group by, aggregation etc.) can be applied very easily to its columns. More: + 10 Minutes to Pandas(亲身经历，感觉10分钟根本没看完，可能英语渣吧……) Practice data set — Loan Prediction Problem 数据集下载地址 貌似是Analytics Vidhya举办的一个竞赛吧，贷款预测问题（Loan Prediction Problem). 1234567891011121314151617181920212223242526272829VARIABLE DESCRIPTIONS:(数据集变量描述)Variable DescriptionLoan_ID(贷款人标识) Unique Loan IDGender(性别) Male/ FemaleMarried(婚姻) Applicant married (Y/N)Dependents(家属) Number of dependentsEducation(教育) Applicant Education (Graduate/ Under Graduate)Self_Employed(自营) Self employed (Y/N)ApplicantIncome(申请人收入) Applicant incomeCoapplicantIncome Coapplicant incomeLoanAmount(贷款金额) Loan amount in thousandsLoan_Amount_Term(贷款期限) Term of loan in monthsCredit_History(信用记录) credit history meets guidelinesProperty_Area(居住地) Urban/ Semi Urban/ RuralLoan_Status(贷款状况) Loan approved (Y/N) Importing libraries and the data set numpy matploylib pandas 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf=pd.read_csv("G:/Datahack/Loanprediction/TrainFile/train.csv") Quick Data ExplorationOnce you have read the dataset, you can have a look at few top rows by using the function head() 1df.head() #print first 5 rows of dataset .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Loan_ID Gender Married Dependents Education Self_Employed ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History Property_Area Loan_Status 0 LP001002 Male No 0 Graduate No 5849 0.0 NaN 360.0 1.0 Urban Y 1 LP001003 Male Yes 1 Graduate No 4583 1508.0 128.0 360.0 1.0 Rural N 2 LP001005 Male Yes 0 Graduate Yes 3000 0.0 66.0 360.0 1.0 Urban Y 3 LP001006 Male Yes 0 Not Graduate No 2583 2358.0 120.0 360.0 1.0 Urban Y 4 LP001008 Male No 0 Graduate No 6000 0.0 141.0 360.0 1.0 Urban Y Next, you can look at summary of numerical fields by using describe() function. describe() function would provide count,mean,standard deviation(std),min,quartiles and max in its output. More: + basic statistics to understand population distribution 同时，可以看到部分列还有缺失值。 1df.describe() #Get summary of numerical variables .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History count 614.000000 614.000000 592.000000 600.00000 564.000000 mean 5403.459283 1621.245798 146.412162 342.00000 0.842199 std 6109.041673 2926.248369 85.587325 65.12041 0.364878 min 150.000000 0.000000 9.000000 12.00000 0.000000 25% 2877.500000 0.000000 100.000000 360.00000 1.000000 50% 3812.500000 1188.500000 128.000000 360.00000 1.000000 75% 5795.000000 2297.250000 168.000000 360.00000 1.000000 max 81000.000000 41667.000000 700.000000 480.00000 1.000000 For the non-numerical values, we can look at frequency distribution to understand whether they make sense or not. The frequency table can be printed by following command: 1df['Property_Area'].value_counts() Semiurban 233 Urban 202 Rural 179 Name: Property_Area, dtype: int64 Distribution analysis通过前面的操作已经熟悉了基本数据特征，下面开始研究各个变量的分布情况。 Lets start by plotting the histogram of ApplicantIncome using the follow commands: 12df['ApplicantIncome'].hist(bins=50,figsize=(15,5))plt.show() 可以看到上图中有少数的极端值。 Next, we look at box plots to understand the distributions. 12df.boxplot(column='ApplicantIncome')plt.show() This confirms the presence of a lot of outliers/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education: 12df.boxplot(column='ApplicantIncome',by='Education')plt.show() We can see that there is no substantial different between the mean income of graduate and non-graduates. But there are a higher number of graduates with very high incomes, which are appearing to be the outliers.可以看出，大学毕业和大学未毕业人数平均收入是差不多的，但是大学毕业会有部分高收入值，形成离群值。 Now, Let’s look at the histogram and boxplot of LoanAmount using the following command: 12df['LoanAmount'].hist(bins=50,figsize=(15,5))plt.show() 12df.boxplot(column='LoanAmount')plt.show() Again, there are some extreme values. Clearly, both ApplicantIncome and LoanAmount require some amount of data munging.（小白表示懵逼啊，有很多离群值就不能用了？需要数据修复？好像吴恩达的视频是提到过） LoanAmount has missing and well as extreme values values, while ApplicantIncome has a few extreme values, which demand deeper understanding. We will take this up in coming sections. Categorical variable analysisNow that we understand distributions for ApplicantIncome and LoanIncome, let us understand categorical variables in more details. We will use Excel style pivot table and cross-tabulation. For instance, let us look at the chances of getting a loan based on credit history. This can be achieved in MS Excel using a pivot table as: Now we will look at the steps required to generate a similar insight using Python. Please refer to this article(12种奇淫异巧) for getting a hang of the different data manipulation techniques in Pandas. 1234567temp1=df['Credit_History'].value_counts(ascending=True)temp2=df.pivot_table(values='Loan_Status',index=['Credit_History'], aggfunc=lambda x:x.map(&#123;'Y':1,'N':0&#125;).mean())print("Frequency Table for Credit History:")print(temp1)print("\nProbility of getting loan for each Credit History class:")print(temp2) Frequency Table for Credit History: 0.0 89 1.0 475 Name: Credit_History, dtype: int64 Probility of getting loan for each Credit History class: Loan_Status Credit_History 0.0 0.078652 1.0 0.795789 1234567891011temp1.plot(kind='bar')plt.xlabel('Credit_History')plt.ylabel('Count of Applicants')plt.title('Applicants by Credit_History')temp2.plot(kind='bar')plt.xlabel('Credit_History')plt.ylabel('Probability of getting loan')plt.title('Probability of getting loan by credit history')plt.show() This shows that the chances of getting a loan are eight-fold if the applicant has a valid credit history.(在有Credit_History的人获得贷款的概率是无Credit_History的8倍)You can plot similar graphs by Married, Self-Employed, Property_Area, etc. Alternately, these two plots can also be visualized by combining them in a stacked chart:: 123temp3=pd.crosstab(df['Credit_History'],df['Loan_Status'])temp3.plot(kind='bar',stacked=True,color=['red','blue'],grid=False)plt.show() You can also add gender into the mix. If you have not realized already, we have just created two basic classification algorithms here, one based on credit history, while other on 2 categorical variables (including gender). You can quickly code this to create your first submission on AV Datahacks. We just saw how we can do exploratory analysis in Python using Pandas. I hope your love for pandas (the animal) would have increased by now – given the amount of help, the library can provide you in analyzing datasets. Next let’s explore ApplicantIncome and LoanStatus variables further, perform data munging and create a dataset for applying various modeling techniques. I would strongly urge that you take another dataset and problem and go through an independent example before reading further.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习书单【转】]]></title>
    <url>%2F2017%2F07%2F13%2FPython%E5%AD%A6%E4%B9%A0%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[In my fifteen years as a Python developer and author, one question seems to come up over and over again: “Can anyone recommend a good book on Python?” To book authors, this is pretty much a conflict-of-interest question. “Why, mine!” is what we would all say. The problem is that there is no correct answer. The right answer depends on the reader’s level and skillset, as well as the style of learning that they find most compatible. This means the best book is different for different people. As lofty as it may sound, my main goal is the good of the community. If my book is the right one for certain classes of users, that’s great. If not, I’m happy to recommend others which may be a better fit. Before we get started, I’ve have one comment on another popular question: Python 2 or Python 3? While Python 3 has been around for more than 3 years, many libraries and packages have been ported, and a new 3.3 release forthcoming late summer, most of the world still runs on Python 2. If you have no old software to port and are getting into Python now, you can start with 3.x. If you have existing code that still runs under 2.x, start there, because the changes are mostly cosmetic (albeit backwards-incompatible). Once you learn one, you can get up-to-speed with the other quickly. Now let’s get reading! In this article, I’ll go over three different reading lists for three different audiences. The first audience is existing programmers who need to learn Python. For Programmers New to Python Dive into Python 3 by Mark Pilgrim, Apress Core Python Programming by Wesley Chun, Prentice Hall The Quick Python Book by Naomi Ceder, Manning Beginning Python: From Novice to Professional by Magnus Lie Hetland, Apress Learn Python the Hard Way by Zed Shaw by Mark Pilgrim, Apress, 2009 One of the most popular Python books has been Dive into Python. Originally published in 2004, a new version for Python 3 was published in 2009. For developers who prefer to learn by just diving into code, this is one of your top choices, even more so since the author is one of my co-workers! However, if you prefer to learn a lot more before venturing into programming, there are other options for you.. by Wesley Chun, Prentice Hall, 2006 Core Python Programming is pretty much the opposite of Dive into Python. Instead of a “quick dive,” I would call it a deep dive into the Python language. The goal of this book is to teach you Python as quickly but as comprehensively as possible. There are plenty of code samples to look at and try during your reading, so you don’t have to read that much before getting started. Even better are the exercises at the end of every chapter to help you put what you learned into practice. Furthermore, a healthy dose of charts and tables provides reference material for readers. In 2009, I added two new appendices on Python 2.6 (also applicable for 2.7) and 3.x to keep the book contemporary. These appendices are available in the 5th and newer printings. All other readers can download both appendices as well as a cleaned up index on the book’s Web site at corepython.com. second edition by Naomi Ceder, Manning, 2010 The Quick Python Book is similar to Dive into Python, but originally published well before the latter except that it goes into a bit more detail than Dive. Its reviews are just as good as its newer brother. A few years ago, it was updated to Python 3. second edition by Magnus Lie Hetland, Apress, 2008 The Beginning Python book also goes more into detail than Apress’ fellow book, Dive into Python. It is very readable and user-friendly; however, like the Quick Python Book, it doesn’t dive in as deeply as Core Python Programming. It’s just right in the middle and thus could be your cup of tea. There’s even a companion Web site for the book. third edition by Zed Shaw, 2013 This series takes a completely different approach: the author forces you to code and code correctly, then explains what you did and why. But since you already had to experience it, you pick up programming skills more quickly. This book is also suitable for those who have never programmed before, and is “brutally-friendly” for these readers. ReferencesThe final list of books to look at are the references–those companion tomes that you should also have on your shelf. The best ones are those that you can just pull off the shelf, look up something, then put away. Python Essential Reference fourth edition by David Beazley, Addison-Wesley Python in a Nutshell second edition by Alex Martelli, O’Reilly Python Cookbook third edition by David Beazley and Brian K. Jones, O’Reilly Python Standard Library by Example by Doug Hellmann, Addison-Wesley Python Essential Reference fourth edition by David Beazley, Addison-Wesley, 2009 The first book in this list is the classic “PER” (Python Essential Reference). It was the very first one (at least its original edition was). Back in the Python 1.5 days, the only real reference Python programmers had was the Standard Library Reference online documentation. Printing it out was enormous: about an inch thick double-sided! Developers craved a “library reference to take home.” Python Essential Reference alleviated that need and represented exactly that: a small, portable version of the library reference. It has since been updated regularly by jazz musician and mad (computer) scientist, David Beazley. Python in a Nutshell second edition by Alex Martelli, O’Reilly, 2006 Several years later, a second reference book came out, this one from O’Reilly as part of their classic Nutshell reference series written by the incomparable Alex Martelli, another co-worker of mine. Both the Nutshell and PER references are written by luminaries in the Python world, and both books are similar. The best suggestion I can offer you is to flip through several pages of both and find which writing style suits you best. Python Cookbook third edition by David Beazley and Brian K. Jones, O’Reilly, 2013 The final two books are not references as much as the first two, but they are still references to consider if you want to go beyond the pure lookup reference guides. This book is based on the online Python Cookbook, a series of “recipes” that are Python snippets of code that “do something.” You can find all the recipes here at http://code.activestate.com/recipes/langs/python, but the book contains “the best” ones, plus additional commentary by the editors. Python Standard Library by Example by Doug Hellmann, Addison-Wesley, 2011 If you can imagine one of the earlier references along with many more code samples, you’ll arrive here. Rather than covering every single module and package in the standard library, this book takes the most popular ones that are used by developers today. It is based on the popular blog series, PyMotW (Python Module of the Week), maintained by the author himself. The Next StepWhat do you do after you’ve learned Python? You may have read my book or others like Dive into Python, Beginning Python, or Learning Python and have written some basic tools/apps. However, to go to the next level, you have nowhere to turn other than dive deeply into specific topics with books about game programming, databases, graphics/multimedia, GUIs, scientific programming, networking, etc. There are definitely books on advanced topics such as these and more, don’t get me wrong. But if you want to develop more than one of these skills, you’d have to buy a book on every topic of interest. This is overkill, especially if you’re only looking to expand your skillset. In that case, you’ll want to reach for this book: Core Python Applications Programming by Wesley Chun, Prentice Hall, 2012 Those of you who have read Core Python Programming will recognize much of the material in this book, because it comes from Part II of Core Python Programming. In the original book, I felt I had done a good enough job of teaching Python to readers, but didn’t have the room to get into any details about what you could build with it. Now that that material has expanded beyond the borders of an introductory book, it was time to split out this intermediate/advanced material into its own volume. Thus I am pleased to announce that those chapters have been extracted to form their own book, Core Python Applications Programming! The contents have been cleaned up and retrofitted with Python 3 examples paired w/their 2.x friends as a hybrid to help you learn both 2.x &amp; 3.x. There is plenty of new material added to existing chapters, as well as completely brand new ones on Web framework development using Django, an introduction to cloud computing with Google App Engine, and text processing with CSV, JSON, and XML. The purpose is to provide comprehensive introductions to each of these areas of application development, hence the title. I hope you are as excited about the new book as I am! Missing Lists?As some astute readers have noticed, I’m missing one or two lists I would have liked to include. One is a list of advanced Python books. This category is, unfortunately, pretty small as there are just a handful of them, including Expert Python Programming and Pro Python. Which of these two or other advanced books have you read, which have you liked and why? The other missing list I’d like to consider is that for scientific computing in Python. Send me your favorites and why. I can be reached via Google+ (+wescpy) or Twitter (@wescpy). ConclusionNow that you’ve seen a variety of ways to address that question of which book with the possible correct answer, we hope that you’re able to use this article to get the right Python book(s) for you. While I hope you find that my books best fit your needs, I’m more happy that you get what you need to build great applications in Python. If you’re new to Python, welcome to our family!**]]></content>
      <categories>
        <category>book</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学的完整学习路径【转】]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%9A%84%E5%AE%8C%E6%95%B4%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[从Python菜鸟到Python Kaggler的旅程。 假如你想成为一个数据科学家，或者已经是数据科学家的你想扩展你的技能，那么你已经来对地方了。本文的目的就是给数据分析方面的Python新手提供一个完整的学习路径。该路径提供了你需要学习的利用Python进行数据分析的所有步骤的完整概述。如果你已经有一些相关的背景知识，或者你不需要路径中的所有内容，你可以随意调整你自己的学习路径，并且让大家知道你是如何调整的。 步骤0：热身 开始学习旅程之前，先回答第一个问题：为什么使用Python？或者，Python如何发挥作用？观看DataRobot创始人Jeremy在PyCon Ukraine 2014上的30分钟演讲，来了解Python是多么的有用。 步骤1：设置你的机器环境 现在你已经决心要好好学习了，也是时候设置你的机器环境了。最简单的方法就是从Continuum.io上下载分发包Anaconda。Anaconda将你以后可能会用到的大部分的东西进行了打包。采用这个方法的主要缺点是，即使可能已经有了可用的底层库的更新，你仍然需要等待Continuum去更新Anaconda包。当然如果你是一个初学者，这应该没什么问题。 如果你在安装过程中遇到任何问题，你可以在这里找到不同操作系统下更详细的安装说明。 步骤2：学习Python语言的基础知识 你应该先去了解Python语言的基础知识、库和数据结构。Codecademy上的Python课程是你最好的选择之一。完成这个课程后，你就能轻松的利用Python写一些小脚本，同时也能理解Python中的类和对象。 具体学习内容：列表Lists，元组Tuples，字典Dictionaries，列表推导式，字典推导式。任务：解决HackerRank上的一些Python教程题，这些题能让你更好的用Python脚本的方式去思考问题。 替代资源：如果你不喜欢交互编码这种学习方式，你也可以学习谷歌的Python课程。这个2天的课程系列不但包含前边提到的Python知识，还包含了一些后边将要讨论的东西。 步骤3：学习Python语言中的正则表达式 你会经常用到正则表达式来进行数据清理，尤其是当你处理文本数据的时候。学习正则表达式的最好方法是参加谷歌的Python课程，它会让你能更容易的使用正则表达式。 任务：做关于小孩名字的正则表达式练习。 如果你还需要更多的练习，你可以参与这个文本清理的教程。数据预处理中涉及到的各个处理步骤对你来说都会是不小的挑战。 步骤4：学习Python中的科学库—NumPy, SciPy, Matplotlib以及Pandas 从这步开始，学习旅程将要变得有趣了。下边是对各个库的简介，你可以进行一些常用的操作： •根据NumPy教程进行完整的练习，特别要练习数组arrays。这将会为下边的学习旅程打好基础。 •接下来学习Scipy教程。看完Scipy介绍和基础知识后，你可以根据自己的需要学习剩余的内容。 •这里并不需要学习Matplotlib教程。对于我们这里的需求来说，Matplotlib的内容过于广泛。取而代之的是你可以学习这个笔记中前68行的内容。 •最后学习Pandas。Pandas为Python提供DataFrame功能（类似于R）。这也是你应该花更多的时间练习的地方。Pandas会成为所有中等规模数据分析的最有效的工具。作为开始，你可以先看一个关于Pandas的10分钟简短介绍，然后学习一个更详细的Pandas教程。 您还可以学习两篇博客Exploratory Data Analysis with Pandas和Data munging with Pandas中的内容。 额外资源： •如果你需要一本关于Pandas和Numpy的书，建议Wes McKinney写的“Python for Data Analysis”。 •在Pandas的文档中，也有很多Pandas教程，你可以在这里查看。 任务：尝试解决哈佛CS109课程的这个任务。 步骤5：有用的数据可视化 参加CS109的这个课程。你可以跳过前边的2分钟，但之后的内容都是干货。你可以根据这个任务来完成课程的学习。 步骤6：学习Scikit-learn库和机器学习的内容 现在，我们要开始学习整个过程的实质部分了。Scikit-learn是机器学习领域最有用的Python库。这里是该库的简要概述。完成哈佛CS109课程的课程10到课程18，这些课程包含了机器学习的概述，同时介绍了像回归、决策树、整体模型等监督算法以及聚类等非监督算法。你可以根据各个课程的任务来完成相应的课程。 额外资源： •如果说有那么一本书是你必读的，推荐Programming Collective Intelligence。这本书虽然有点老，但依然是该领域最好的书之一。 •此外，你还可以参加来自Yaser Abu-Mostafa的机器学习课程，这是最好的机器学习课程之一。如果你需要更易懂的机器学习技术的解释，你可以选择来自Andrew Ng的机器学习课程，并且利用Python做相关的课程练习。 •Scikit-learn的教程 任务：尝试Kaggle上的这个挑战 步骤7：练习，练习，再练习 恭喜你，你已经完成了整个学习旅程。 你现在已经学会了你需要的所有技能。现在就是如何练习的问题了，还有比通过在Kaggle上和数据科学家们进行竞赛来练习更好的方式吗？深入一个当前Kaggle上正在进行的比赛，尝试使用你已经学过的所有知识来完成这个比赛。 步骤8：深度学习 现在你已经学习了大部分的机器学习技术，是时候关注一下深度学习了。很可能你已经知道什么是深度学习，但是如果你仍然需要一个简短的介绍，可以看这里。 我自己也是深度学习的新手，所以请有选择性的采纳下边的一些建议。deeplearning.net上有深度学习方面最全面的资源，在这里你会发现所有你想要的东西—讲座、数据集、挑战、教程等。你也可以尝试参加Geoff Hinton的课程，来了解神经网络的基本知识。 附言：如果你需要大数据方面的库，可以试试Pydoop和PyMongo。大数据学习路线不是本文的范畴，是因为它自身就是一个完整的主题。 英文出处：http://www.analyticsvidhya.com 本文转自：http://dataunion.org/9805.html?utm_source=tuicool]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度UNIT]]></title>
    <url>%2F2017%2F07%2F10%2FBaiduUNIT%2F</url>
    <content type="text"><![CDATA[前几天看到百度在宿舍楼下贴的海报，“百度之星”，2017年比赛分为开发者大赛和程序设计大赛。开发者大赛是要求基于百度UNIT平台设计开发一个对话系统的智能硬件或APP等都可以。主要目的应该是推广UNIT平台。 UNIT（Understanding and Interaction Technology），即理解与交互技术，它是建立在百度多年积累的自然语言处理与对话技术以及大数据的基础上，面向第三方开发者提供的对话系统开发平台. 从介绍上看，百度的出发点是挺好的，之前做过智能镜子，想加入语音交互功能，但是没有相应中文API提供，很蛋疼！ 随着AI技术和理念的兴起，很多产品都希望采用对话式的人机交互方式。然而对话系统的研发对于大多数开发者而言却是一个很困难的工作，对技术和数据的要求都很高。 为此，百度将积累多年的自然语言理解与交互技术对外开放，将积累多年的自然语言理解与交互技术对外开放，推出了可定制的对话系统开发UNIT（Understanding and Interaction Technology），将业界领先的技术能力输出给广大的开发者，以便降低对话系统的研发门槛。 1、UNIT能做什么 帮开发者打造“面向任务的理解与交互能力”，可以解答用户的某些问题（天气、新闻、快递等生活服务）、执行用户指令（开空调等）、通过一系列交互引导用户达到某项需求（通过注册-选座-下单完成订票). 2、UNIT基本概念 （1）场景 一个场景对应一个独立完整的对话系统，用来满足您某个具体业务场景的需求。通常按垂类划分（例如，银行信用卡办理场景、电视遥控器场景等）。 (2)对话单元 在UNIT里，对话单元用来定义系统在一个具体的对话任务下对用户对话的理解、以及机器人的回应方式，是系统中的最小对话单位。对话任务例如查天气、查询信用卡年费、控制空调温度等。 (3)意图 意图表示用户的目的（例如，”北京天气”，意图是查天气）。 (4)词槽 是满足用户意图时的关键信息或限定条件，可以理解为用户需要提供的筛选条件。例如在查询天气时，词槽是地点和时间。 (5)对话样本 对话样本就是您给对话系统做示范，教它在用户说的具体句子里，该如何理解意图，哪个词是重要信息，对应的词槽是什么。 (6)训练模型 即把场景下所有的配置、标注的对话样本、对话模板等打包提交给系统来训练模型。 模型生效一般需要几分钟时间。 (7)沙盒 每个场景都配有一个沙盒环境，将训练好的模型生效到沙盒环境后，就可以进行效果验证了，同时可接入到您自己的业务系统中使用。您可以生成多个模型版本，但只能选择一个放到沙盒环境中。 3、UNIT使用步骤 （a)梳理业务逻辑 （b)配置对话系统 （c)标注对话数据 （d)对话训练与验证 （e)应用调用 相关连接 1、http://astar2017.baidu.com/ 2、http://ai.baidu.com/docs#/UNIT-guide/top]]></content>
      <categories>
        <category>百度语音</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 20 Valid Parentheses]]></title>
    <url>%2F2017%2F07%2F03%2FValidParentheses%2F</url>
    <content type="text"><![CDATA[LeetCode 20.Valid Parentheses问题如下： Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. The brackets must close in the correct order, “()” and “()[]{}” are all valid but “(]” and “([)]” are not. 仔细思考 ，发现用栈来处理是很好做的，但C语言没有栈这种数据类型，只能自己构建了。 typedef struct{ int *base;//栈底指针 int *top;//栈顶指针，非空栈中的栈顶指针始终在栈顶元素的下一个位置 int stackSize;//栈长度 }SqStack;//定义结构体 int Push(SqStack *S,int e) { *(*S).top++ = e;//入栈操作 return 1; } int Pop(SqStack *S, int *e) { *e = *--(*S).top;//出栈 return 1; } int StackEmpty(SqStack S) { return S.top == S.base;//栈空 } bool isValid(char* s) { if(s==NULL) return true; SqStack S; int N = strlen(s); if(N==1) return false; if(N%2) return false; S.base = (int *)malloc(N*sizeof(int));//分配空间 S.top = S.base;//空栈，栈顶指针等于栈底指针 S.stackSize = N;//栈长度，本例中，假设不会超过初始长度 int table[128] = { 10 }; int e=0; table[&apos;(&apos;] = &apos;)&apos;; table[&apos;{&apos;] = &apos;}&apos;; table[&apos;[&apos;] = &apos;]&apos;; table[&apos;]&apos;] = &apos;-&apos;; table[&apos;}&apos;] = &apos;-&apos;; table[&apos;)&apos;] = &apos;-&apos;; for (int i = 0; i &lt; N; i++) { if (!StackEmpty(S)) { Pop(&amp;S, &amp;e); if (table[e] == s[i]) { continue; } else Push(&amp;S, e); } if (table[s[i]] == s[i + 1]) { i++; continue; } else { Push(&amp;S, s[i]); } } int result = StackEmpty(S); free(S.base); return result; }]]></content>
      <categories>
        <category>编程</category>
        <category>C</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>算法</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[世纪三部曲]]></title>
    <url>%2F2017%2F06%2F30%2Ffallofgiants%2F</url>
    <content type="text"><![CDATA[第一部 巨人的陨落 在第一次世界大战的硝烟中，每一个迈向死亡的生命都在热烈地生长——威尔士的矿工少年、刚失恋的美国法律系大学生、穷困潦倒的俄国兄弟、富有英俊的英格兰伯爵，以及痴情的德国特工……从充满灰尘和危险的煤矿到闪闪发光的皇室宫殿，从代表着权力的走廊到爱恨纠缠的卧室，五个家族迥然不同又纠葛不断的命运逐渐揭晓，波澜壮阔地展现了一个我们自认为了解，但从未如此真切感受过的20世纪。 第二部 世界的凛冬 我亲眼目睹，每一个迈向死亡的生命都在热烈地生长。《世界的凛冬》是火遍全球的20世纪人类史诗“世纪三部曲”的第二部，是《巨人的陨落》的续篇。整个20世纪的吉光片羽，都被肯·福莱特写进了这部伟大的小说里。一切都始于那个裂变中的大时代——希特勒上台，爱德华八世退位，原子弹在广岛和长崎爆炸……世界剧烈改变，我该怎么办？这正是他们的困惑——一群处于人生黄金时代的少男少女，来自德国、美国、英国、苏俄和威尔士的五大家族，他们父辈的命运因一战而彻底改变。如今，世界再次破碎，甚至更加暴烈和残酷。然而，这就是他们的时代！在时间的永恒流动中，每个人都在创造历史。所以，为什么不一起来，会一会命运？ 第三部 永恒的边缘 《巨人的陨落》的大结局！我亲眼目睹，每一个迈向死亡的生命都在热烈地生长。“世纪三部曲”终于迎来了一个完美结局。如果说《巨人的陨落》是祖辈的传奇，《世界的凛冬》是父辈的人生，那么，《永恒的边缘》就是新一代的奋斗。真正残酷和激烈的世界大战，是思想的大战。来自美国、德国、苏联、英国和威尔士的五大家族，又一次迎来了新的考验。东西德分裂、柏林墙、苏联秘密警察、刺杀肯尼迪、民权运动、古巴导弹危机、入侵黎巴嫩、弹劾尼克松……此外，第三代生活中还有摇滚、嬉皮士、跨种族婚恋、性解放，以及对过去的误会与和解。说到底，世上只有一种英雄主义，就是在认清生活真相之后，依然热爱生活。 很棒的小说，作者是肯•福莱特（Ken Follett，1949－）现象级畅销小说大师，爱伦坡终身大师奖得主，完全可以当做历史书来读。当年的当年明月写的《明朝那些事儿》,以一种诙谐幽默的手法展现了明朝几百年历史，让人们发现原来历史还可以这么写。当时也是连着一段时间将《明朝那些事儿》一系列书刷完，好像是7本吧，后来过了一段时间又刷了一遍。而肯.福莱特的《世纪三部曲》在我看来和《明朝那些事儿》同样成功，通过几个家族的百年兴衰，将一战、二战及冷战历史描述的非常细致。相比历史教科书，这3部书，在历史中加入了更多的人性，与其说《世纪三部曲》是一本历史长篇小说，更不如说其是一本爱情长篇小说。]]></content>
      <categories>
        <category>book</category>
      </categories>
      <tags>
        <tag>reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chrome浏览器总是崩溃的一种解决办法]]></title>
    <url>%2F2017%2F06%2F28%2Fchromefiex%2F</url>
    <content type="text"><![CDATA[更新过Chrome浏览器后，任何网页都无法打开，包括设置页面！重新安装过好多次，各种版本也都尝试过了，都是崩溃页面。后来在知乎上找到了一种解决办法。 原因在于C:\Windows\System32\drivers\bd0001.sys这个文件，不管用什么方法，只要把该文件移除当前目录或者直接删除，重启电脑，Chrome浏览器就恢复正常咯！ bd0001.sys不知道是不是百度干的，好无耻~]]></content>
      <categories>
        <category>电脑</category>
      </categories>
      <tags>
        <tag>computer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速整数平方根算法]]></title>
    <url>%2F2017%2F06%2F28%2FFastSqrt%2F</url>
    <content type="text"><![CDATA[LeetCode 69.Sqrt(x)问题如下： Implement int sqrt(int x).Compute and return the square root of x. 其实意思就是对于一个32位无符号整数，找到一个16位无符号整数，使满足n^2≤v的n最大。相应的可以写作下式： 关键在于a_i 的确定，使用迭代求导算法，不需要乘法，只简单的使用加法和移位操作。方便起见，定义 则(1)可以表示为 根据参考文章3得到一个非常简便的算法，3ms. 代码如下： int Sqrt(int v) { unsigned long temp, nHat = 0, b = 0x8000, bshft = 15; do{ if (v &gt;= (temp = (((nHat &lt;&lt; 1) + b) &lt;&lt; bshft--))) { nHat += b; v -= temp; } } while (b &gt;&gt;= 1); return nHat; } 参考文章 https://wapwenku.baidu.com/view/53e2d77b168884868762d6fb.html?pn=3&amp;pu=usm@1,sz@320_1002,ta@iphone_2_7.0_2_6.6 http://www.cnblogs.com/nsnow/archive/2010/08/09/1796111.html http://www.azillionmonkeys.com/qed/ulerysqroot.pdf]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[要怎样努力，才能成为很厉害的人]]></title>
    <url>%2F2017%2F06%2F27%2FMy-First-Blog%2F</url>
    <content type="text"><![CDATA[#要怎样努力，才能成为很厉害的人 首先，少年，答应别人的承诺，就一定要兑现。 我以前啊，和你一样，很想成为一个很厉害很厉害很厉害的人。 喜欢看热血的东西，幻想自己是屠龙的勇士，登塔的先锋，我左手有剑，右手有光，没头没脑的燃烧自己，敌人的骑军来了，我说你们何人堪与之战，我的女人在等我。 后来我现实了一点，我觉得我要成为那种说走就走，说日就日的男人，我梳大背头，流浪在欧洲或者新几内亚的，我拍孩子，拍野兽，拍流浪的雏妓，与罗伯特德尼罗握手，说嘿，我给你写了愤怒的公牛2。 再后来，我觉得我人生的梦想，是在城市中心买上一间顶层公寓，把一整面墙都改造成钢化玻璃，在灯火通明的夜晚，我就要端着酒站在巨大的窗前，看整个城市在呼吸，然后我的朋友叩门，他带来了一打嫩模，我们就玩一些成年人的游戏 现在，我发现龙并不存在，我不会骑马，不会用单反，家住2楼，我能做的，就是把眼前的事儿做好，赚到足够的钱，这样我可以给我的姑娘一个地球仪，然后用飞镖扎它，扎到哪儿，就去哪儿玩。 这样看来，虽然我的想法随着生殖器的发育，始终在变，但那个很厉害很厉害的人，一直离我很远，甚至越来越远。 我心中曾经执剑的少年，此刻也混迹在市井之间。 血似乎都凉了。 我也不是没有惶恐过，是不是我这一生，都不能左手持剑，右手握着罗伯特德尼罗，说这里的嫩模随便你玩但是你他妈别从窗户上掉下去。 这样一看，我逊得不行，我的朋友都是一些凡人，比我还逊，业余生活就是推塔、中单、跪。 我心想，我是不是这辈子都要做一个逊逼，直到我的坟墓上写好墓志铭，我甚至都想好了： ###我来，我见，我挂了。最后我给了自己一个否定的答复，我不要。 我喜欢我的朋友们，喜欢我现在的生活，首先我希望你明白，没有厉害与逊逼得区分，只有血的凉与热，有的人觉得生活就这样吧，我算了，现在没什么不好。 有的人觉得生活这样挺好，但是我还要更好。 这种只要剧情稍微热血一点就会热泪盈眶的傻逼，已经不多了，一刻也不要停留。 所以现在，我和你不一样了，我仍然想成为一个很厉害很厉害很厉害的人，像我们这种剧情稍微热血一点就会热泪盈眶的傻逼，要好好珍惜自己。 很多人坐下来了，跟你说你不行，说你省点儿心吧，说你请静一静。 汹涌的人群就把你这样的少年淹没了，人群散去的时候，你也不见了，你那些承诺，谁也听不见，这个世界对于你，就再不可能有什么更有趣更漂亮的女朋友。 你就失约了，小逼崽子。 这么跟你说。 虽然随着年龄的增长，我趋于现实，不能像你那样分分钟冲动的燃烧，然而我每时每刻都有想做的事，有想达成的目标。 不排除以后的某一年，我会握着罗伯特德尼罗的手，他说这是你写得吗，愤怒的公牛2，只要他还没死。 故事里拳王拉莫塔忍着伤，他举着铁拳，挥汗如雨，要和命运斗争，他说我怎么能失约呢，我是那个要成为很厉害很厉害的拳王拉莫塔！ 小伙儿，成为很厉害很厉害的人，最重要的，就是要热血，永远也不要让你的血凉下去，你凉下去了，就再也不能找到一个更有趣更漂亮的女友，你就失约了，于是那天她踏梦而来，就成了一个彻头彻尾的笑话。 当有一天你成为你讨厌的那种人，浑浑噩噩，你走在街上，看见那些更有趣更漂亮的女孩，你会不会想起多年以前，你说我答应你，在一个承诺就是永远的年纪。 读书，交友，美容，都不如你这一腔狗血，滚烫，灼人，你要燃上大半辈子，才对得起你现在说的这些话。 我听闻最美的故事，是公主死去了，屠龙的少年还在燃烧。 火苗再小，你都要反复的点燃。 所谓热血的少年，青涩的爱恋，死亡与梦之约。 这么好的故事。 你可别演砸了。 最后我给你点个人建议：1.读书，读到倦，网上有很多方法，但你从来沉不下心看。2.学习，学到疼，网上有很多方法，但你从来沉不下心看。3.开口说话，冷场也要说话，脸皮薄也要说话，挨打也要说话。4.如果你现在不知道做什么，至少你还可以先从做一个牛逼的学生开始。5.更漂亮更有趣的女孩，五年以后再找。6.承诺是鞭子，不是兴奋剂。7.年纪大了，也不要说什么心如死灰。改变自己是非常，非常，非常痛苦的，我能看出来你一腔热血的优点，自然知道你孤僻懒散自以为是的缺点，方法很多，不过我不确定你吃不吃得了苦，我和你共勉吧。在成为最厉害最厉害最厉害的道路上。 完]]></content>
      <categories>
        <category>鸡汤</category>
      </categories>
      <tags>
        <tag>day day up</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy库学习总结]]></title>
    <url>%2F2017%2F01%2F13%2FNumpy%E5%BA%93%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[NumPy是用于Python进行科学计算，尤其是数据分析时，所用到的一个基础库。 NumPy 是一个运行速度非常快的数学库，主要用于数组计算。它可以让你在 Python 中使用向量和数学矩阵，以及许多用 C 语言实现的底层函数，你还可以体验到从未在原生 Python 上体验过的运行速度。 ndarray: NumPy库的心脏整个NumPy库的基础是ndarray(N-dimensional array,N维数组)对象。它是一种由同质元素组成的多维数组，元素数量是事先指定好的。同质指的是几乎所有元素的类型和大小都相同。事实上，数据类型由另外一个叫作dtype(data-type,数据类型)的NumPy对象来指定；每个ndarray只有一种dtype类型。 创建数组的几种方式 使用array()函数 a=np.array([[1,2,3],[4,5,6]]) NumPy自带函数 np.zeros((3,3)) np.ones((4,4)) np.arange(0,10) np.arange(0,12).reshape(3,4) np.linspace(0,10,5) np.random.random((3,3)) 可以指定dtype类型 np.array([[1,2,3],[4,5,6]],dtype=complex) 基本操作 算术运算符 a=np.arange(4) #out: [0,1,2,3] a+4 #out: [4,5,6,7] a*2 #out: [0,2,4,6] b=np.arange(4,8) #out: [4,5,6,7] a+b #out: [4,6,8,10] a-b #out: [-4,-4,-4,-4] 矩阵积 np.dot(A,B) #A,B为矩阵 #也可以用 A.dot(B) 自增和自减运算符 a=np.arange(4) # [0,1,2,3] a+=1 # [1,2,3,4] a-=1 # [0,1,2,3] 通用函数 a=np.arange(1,5) np.sqrt(a) np.log(a) np.sin(a) 聚合函数 聚合函数是指对一组值进行操作，返回一个单一值作为结果的函数。 a=np.array([3.3,4.5,1.2,5.7,0.3]) a.sum() # 15.0 a.min() #0.2999999999999999999 不明白为啥不是0.3 a.max() #5.7000000000000000000 a.mean() #3.0 a.std() #2.0079840636817816 索引机制、切片和迭代方法 索引机制 a=np.arange(10,16) #[10,11,12,13,14,15] a[2] #12, 从0开始 a[-2] #14，从-1开始 #可以索引多个值 a[[1,3,4]] #[11,13,14] 切片操作 切片操作是抽取数组的一部分元素生成新数组。对Python列表进行切片操作得到的数据是原数组的副本，而对NumPy数组进行切片操作得到的数组则是指向同一缓冲区的视图。 视图和副本的区别： 视图改变，原数组也进行改变，而副本则不影响原数组。 a=np.arange(10,16) #[10,11,12,13,14,15] a[1:5] #[11,12,13,14] a[1:5:2] #[11,13] a[::2] #省略start和end,[10,12,14] a[:5:2] #省略start,[10,12,14] a[:5:] #省略start,step,[10,11,12,13,14] a=np.array([1,2,3,4]) b=a #视图 c=a.copy() #副本 条件和布尔数组A=np.random.random((4,4)) A[A&lt;0.5] #筛选出所有小于0.5的元素 形状变换.reshape() .ravel()#重新变为一维数组 .transpose()#转置 数组操作 连接数组 A=np.ones((3,3)) B=np.zeros((3,3)) np.vstack((A,B)) #[[1,1,1], #[1,1,1], #[1,1,1], #[0,0,0], #[0,0,0], #[0,0,0]] np.hstack((A,B)) #[[1,1,1,0,0,0], #[1,1,1,0,0,0], #[1,1,1,0,0,0]] 数组切分 A=np.arange(16).reshape((4,4)) [B,C]=np.hsplit(A,2) [B,C]=np.vsplit(A,2) [A1,A2,A3]=np.split(A,[1,3],axis=1) [A1,A2,A3]=np.split(A,[1,3],axis=0) 数组数据文件的读写 二进制文件的读写 np.save(&apos;saved_data&apos;,data)#保存数组数据 np.load(&apos;saved_data.npy&apos;)#读取数组数据 读取文件中的列表形式数据 data=np.genfromtxt(&apos;data.csv&apos;,delimiter=&apos;,&apos;,names=True)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>好记性不如烂笔头</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迷宫问题]]></title>
    <url>%2F2016%2F07%2F03%2F%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[求迷宫中从入口到出口的所有路径是一个经典的程序设计问题。最简单方法是穷举法，即从某一入口出发，顺着某一方向向前探索，若能走通，则继续往前走；否则沿原路返回，换个方向继续尝试。因此，使用栈结构来处理较为合适。栈结构存储从入口到出口的路径。 比较简单的迷宫，‘+’表示墙壁，不能通过；‘o’表示通道，可以通过；但是从入口到出口路径上不能两次通过同一块通道。 求迷宫中一条路径的算法的基本思想是： 若当前位置“可通”，则入栈“当前路径”，并继续朝下一位置探索，如此重复；若当前位置“不可通”，则应顺着“来向”退回到“前一通道块”，然后朝着除“来向”之外的其他方向继续探索；若该通道块的四周4个方块均“不可通”，则应从“当前路径”上删除该通道块。 C语言代码如下： typedef struct{ int x; int y; }PosType;//坐标 typedef struct{ int ord; PosType seat; int di; }SElemType;//栈基本元素 //---------栈相关操作------------------// #define STACK_INIT_SIZE 100 typedef struct{ SElemType *base; SElemType *top; int stacksize; }SqStack; int InitStack(SqStack &amp;S) { S.base = (SElemType *)malloc(STACK_INIT_SIZE*sizeof(SElemType)); S.top = S.base; S.stacksize = STACK_INIT_SIZE; return 1; } int Push(SqStack &amp;S, SElemType e){ *S.top++ = e; return 1; } int Pop(SqStack &amp;S, SElemType &amp;e) { e = *--S.top; return 1; } int StackEmpty(SqStack S) { return S.top == S.base; } //---------栈相关操作------------------// int maze[10][10] = { {-1,-1,-1,-1,-1,-1,-1,-1,-1,-1}, {-1, 0, 0, 0, 0, 0, 0, 0,-1,-1}, {-1, 0, 0, 0,-1, 0,-1,-1, 0,-1}, {-1,-1,-1, 0,-1, 0, 0,-1, 0,-1}, {-1, 0, 0, 0,-1,-1, 0,-1, 0,-1}, {-1, 0, 0,-1, 0, 0, 0, 0, 0,-1}, {-1, 0, 0,-1, 0, 0,-1,-1, 0,-1}, {-1,-1,-1, 0, 0, 0, 0,-1, 0,-1}, {-1, 0, 0, 0, 0, 0, 0, 0, 0,-1}, {-1,-1,-1,-1,-1,-1,-1,-1,-1,-1} };//迷宫 PosType start = { 1, 1 };//迷宫起点 PosType end = { 8, 8 };//迷宫终点 int Pass(PosType pos)//当前位置是否可通过 { return (maze[pos.x][pos.y] != -1) &amp;&amp; (maze[pos.x][pos.y] != 1); } void FootPrint(PosType pos)//走过的通道块 { maze[pos.x][pos.y] = 1;//走过 } PosType NextPos(PosType pos, int di)//顺时钟方向尝试 { PosType resultPos; switch (di) { case 1://东 resultPos.x = pos.x + 1; resultPos.y = pos.y; break; case 2://南 resultPos.x = pos.x; resultPos.y = pos.y + 1; break; case 3://西 resultPos.x = pos.x - 1; resultPos.y = pos.y; break; case 4://北 resultPos.x = pos.x; resultPos.y = pos.y - 1; break; default: printf(&quot;Error!\n&quot;); } return resultPos; } void MarkPrint(PosType pos)//标记不可通过的位置 { maze[pos.x][pos.y] = -1; } int MazePath(){ SqStack S; InitStack(S);//初始化栈 PosType curpos = start; int curstep = 1; SElemType e; do{ if (Pass(curpos)){ FootPrint(curpos); e.ord = curstep; e.seat.x = curpos.x; e.seat.y = curpos.y; e.di = 1; Push(S, e); if ((curpos.x == end.x) &amp;&amp; (curpos.y == end.y)) {//到达终点，逐一打印栈中保持的路径； while (!StackEmpty(S)) { Pop(S, e); printf(&quot;(%d, %d)\n&quot;, e.seat.x, e.seat.y); } free(S.base); return true; } curpos = NextPos(curpos, 1); curstep++; } else { if (!StackEmpty(S)) { Pop(S, e); while (e.di == 4 &amp;&amp; !StackEmpty(S)){ MarkPrint(e.seat); Pop(S, e); } if (e.di &lt; 4){ e.di++; Push(S, e); curpos = NextPos(e.seat, e.di); } } } } while (!StackEmpty(S)); free(S.base); return false; } 运行结果： 红色是正确路径，蓝色是尝试路径。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>算法</tag>
      </tags>
  </entry>
</search>