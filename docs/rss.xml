<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>John Chen's blog</title><link>https://Andy1314Chen.github.io</link><description>&lt;script&gt;document.getElementById('content').innerHTML = `&lt;br&gt;&lt;br&gt;&lt;br&gt;雁过留痕, 人生一世, 还是应该记录点什么&lt;br&gt;&lt;br&gt;&lt;br&gt;又菜又爱玩, 一个爱折腾的程序员👨🏻‍💻&lt;br&gt;&lt;br&gt;&lt;br&gt;`;&lt;/script&gt;</description><copyright>John Chen's blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://raw.githubusercontent.com/Andy1314Chen/obsidian-pic/main/image/20241015155657.png</url><title>avatar</title><link>https://Andy1314Chen.github.io</link></image><lastBuildDate>Thu, 17 Oct 2024 09:59:21 +0000</lastBuildDate><managingEditor>John Chen's blog</managingEditor><ttl>60</ttl><webMaster>John Chen's blog</webMaster><item><title>How to Access Global Memory Efficiently in CUDA Kernels</title><link>https://Andy1314Chen.github.io/post/How%20to%20Access%20Global%20Memory%20Efficiently%20in%20CUDA%20Kernels.html</link><description>Avoid the large strides through global memory, 尽可能 Global Memory Coalescing 全局内存合并&#13;
&#13;
将 threads 分组为 warps 不仅与计算相关，还与全局内存访问相关。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/How%20to%20Access%20Global%20Memory%20Efficiently%20in%20CUDA%20Kernels.html</guid><pubDate>Thu, 17 Oct 2024 08:39:36 +0000</pubDate></item><item><title>CEFR 英语等级</title><link>https://Andy1314Chen.github.io/post/CEFR%20-ying-yu-deng-ji.html</link><description>&gt; CEFR 全名 Common European Framework of Reference for Language，即欧洲共同语言参考标准，是官方对于不同阶段外语水平的描述。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/CEFR%20-ying-yu-deng-ji.html</guid><pubDate>Wed, 16 Oct 2024 09:20:53 +0000</pubDate></item><item><title>CUDA Matrix Efficient Copy</title><link>https://Andy1314Chen.github.io/post/CUDA%20Matrix%20Efficient%20Copy.html</link><description>&#13;
&#13;
看 [CUDA Efficient Matrix Transpose 博客](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)的时候，我理解的 simple copy kernel 应该是下面这样的：&#13;
&#13;
```c&#13;
__global__ void copy_simple(float *output, const float *input, const int rows, const int cols)&#13;
{&#13;
    int x = blockIdx.x * blockDim.x + threadIdx.x;&#13;
    int y = blockIdx.y * blockDim.y + threadIdx.y;&#13;
&#13;
    if (x &lt; cols &amp;&amp; y &lt; rows)&#13;
    {&#13;
        output[y * cols + x] = input[y * cols + x];&#13;
    }&#13;
}&#13;
&#13;
// launch kernel&#13;
dim3 blockSize(TILE_DIM, TILE_DIM);&#13;
dim3 gridSize((nx + TILE_DIM - 1)/TILE_DIM, (ny + TILE_DIM - 1)/TILE_DIM);&#13;
copy_simple&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_cdata, d_idata, nx, ny);&#13;
```&#13;
&#13;
但博客中的 simple copy kernel 是这样的：&#13;
&#13;
```c&#13;
&#13;
const int TILE_DIM = 32;&#13;
const int BLOCK_ROWS = 8;&#13;
&#13;
__global__ void copy(float *odata, const float *idata)&#13;
{&#13;
    int x = blockIdx.x * TILE_DIM + threadIdx.x;&#13;
    int y = blockIdx.y * TILE_DIM + threadIdx.y;&#13;
    int width = gridDim.x * TILE_DIM;&#13;
&#13;
    for (int j = 0; j &lt; TILE_DIM; j += BLOCK_ROWS)&#13;
        odata[(y + j) * width + x] = idata[(y + j) * width + x];&#13;
}&#13;
&#13;
// launch kernel&#13;
dim3 dimGrid(nx / TILE_DIM, ny / TILE_DIM, 1);&#13;
dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);&#13;
copy&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_cdata, d_idata);&#13;
```&#13;
&#13;
在 Jetson AGX Orin 64G 上，测试 8192 x 8192 的矩阵，运行结果如下：&#13;
&#13;
```shell&#13;
copy_simple kernel execution time: 6.834688 ms&#13;
copy kernel execution time: 3.384800 ms&#13;
copy shared mem kernel execution time: 3.118784 ms&#13;
cudaMemcpyDeviceToDevice execution time: 3.246016 ms&#13;
```&#13;
&#13;
速度比我理解的 simple copy kernel 快一倍...，加上 shared memory 优化还能再快一些...&#13;
&#13;
但这里只测了 8192x8192 一个 case，带有 shared memory 优化的 copy kernel 可能性能最好，但大多数情况下应该是官方 `cudaMemcpyDeviceToDevice` 性能要好些，所以一般情况不用自己写 copy kernel。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/CUDA%20Matrix%20Efficient%20Copy.html</guid><pubDate>Wed, 16 Oct 2024 01:47:00 +0000</pubDate></item><item><title>超轻量级个人博客：Gmeek,  All in GitHub</title><link>https://Andy1314Chen.github.io/post/chao-qing-liang-ji-ge-ren-bo-ke-%EF%BC%9AGmeek%2C%20%20All%20in%20GitHub.html</link><description>之前也断断续续想写或写了一些东西，记记笔记、写写博客啥的，但是一直没有检查下来，上次搭建个人博客应该是三、四年之前了。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/chao-qing-liang-ji-ge-ren-bo-ke-%EF%BC%9AGmeek%2C%20%20All%20in%20GitHub.html</guid><pubDate>Mon, 14 Oct 2024 12:21:23 +0000</pubDate></item><item><title>为博客配置 GitHub 图床</title><link>https://Andy1314Chen.github.io/post/wei-bo-ke-pei-zhi-%20GitHub%20-tu-chuang.html</link><description>博客采用 Gmeek 框架部署在 GitHub 上，首先本地在 Obsidian 中利用 Markdown 写，再复制粘贴到 GitHub 中 Issue 中，最后自动发布。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/wei-bo-ke-pei-zhi-%20GitHub%20-tu-chuang.html</guid><pubDate>Mon, 14 Oct 2024 10:27:13 +0000</pubDate></item><item><title>Follow 认证订阅源</title><link>https://Andy1314Chen.github.io/post/Follow%20-ren-zheng-ding-yue-yuan.html</link><description>This message is used to verify that this feed (feedId:67574290980319232) belongs to me (userId:67405844465074176). Join me in enjoying the next generation information browser https://follow.is.。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/Follow%20-ren-zheng-ding-yue-yuan.html</guid><pubDate>Fri, 11 Oct 2024 15:01:05 +0000</pubDate></item><item><title>IPV6 电视 my-tv-0 直播配置</title><link>https://Andy1314Chen.github.io/post/IPV6%20-dian-shi-%20my-tv-0%20-zhi-bo-pei-zhi.html</link><description>目标：不开通电视盒子或者 IPTV 服务，收看直播电视，并且稳定可持续。</description><guid isPermaLink="true">https://Andy1314Chen.github.io/post/IPV6%20-dian-shi-%20my-tv-0%20-zhi-bo-pei-zhi.html</guid><pubDate>Fri, 11 Oct 2024 13:53:13 +0000</pubDate></item></channel></rss>