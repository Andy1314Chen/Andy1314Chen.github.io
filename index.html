<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
	<meta name="google-site-verification" content="y1NE4TYCS_y-0LIIks73rI2qINI9JYRwk6Su8ZG-ZIw" />
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Life, Books, Code" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Andy&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Andy&#39;s blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Andy&#39;s blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Andy's blog</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  
<a href="https://github.com/Andy1314Chen.github.io"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://camo.githubusercontent.com/82b228a3648bf44fc1163ef44c62fcc60081495e/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f6c6566745f7265645f6161303030302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png"></a>
  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andy's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">每天进步一点儿</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/07/机器学习实战第五章学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/07/机器学习实战第五章学习笔记/" itemprop="url">机器学习实战第五章学习笔记(Logistic回归)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-07T00:00:00+08:00">
                2017-09-07
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>回归的概念：假设现在有一些数据点，用一条直线对这些点进行拟合，这个拟合的过程就被称为回归。</p>
<p>利用Logistic回归进行分类的主要思想：根据现有数据对分类边界线建立回归公式，以此进行分类。而拟合过程中，需要<br>寻找最佳拟合参数，得到最佳拟合直线，这就需要一些最优化算法。</p>
<p>Logistic的一般过程：</p>
<ol>
<li>收集数据：采用任意方法收集数据。</li>
<li>准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。</li>
<li>分析数据：采用任意方法对数据进行分析。</li>
<li>训练数据：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。</li>
<li>测试数据：一旦训练步骤完成，分类将会很快。</li>
<li>使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。</li>
</ol>
<p><img src="http://images2015.cnblogs.com/blog/1015872/201609/1015872-20160904123448491-2104213729.png" alt=""></p>
<h2 id="基于Logistic回归和Sigmoid函数的分类"><a href="#基于Logistic回归和Sigmoid函数的分类" class="headerlink" title="基于Logistic回归和Sigmoid函数的分类"></a>基于Logistic回归和Sigmoid函数的分类</h2><ul>
<li>优点：计算代价不高，易于理解和实现。</li>
<li>缺点：容易欠拟合，分类精度可能不高。</li>
<li>适用数据类型：数值型和标称型数据。</li>
</ul>
<p>我们想通过已知数据拟合出一个函数，函数接收所有新的数据输入，然后预测出类别。在二分类情况下，上述函数输出0或1. 最简单的类似函数应该是单位阶跃函数，也被称为海维赛德阶跃函数(Heaviside step function).</p>
<p><img src="http://img.blog.csdn.net/20161116083548612" alt=""></p>
<p>而单位阶跃函数存在一个问题是，在跳跃点上，由0直接到1，这种瞬间跳跃过程是比较麻烦的。幸运的是，另外一种函数，Sigmoid函数具有和阶跃函数类似的性质，而且数学上更加易于处理。</p>
<p>说白了，Logistic回归分类器就是，在每个特征上都乘以相应的一个回归系数，然后把所有的结果值相加，并将总和代入Sigmoid函数中，进而可以得到一个范围在0~1之间的数值。结果大于0.5的数据被判定为类别1，小于0.5的数据被判定为类别0. 从这个角度来说，Logistic回归也可以被看作是一种概率估计。</p>
<p>由回归系数可以构成一个函数或者一条曲线(z=w0*x0+w1*x1+w2*x2+…+wn*xn=0），这条曲线位于两个类别的交界处。</p>
<p><img src="http://img.blog.csdn.net/20170425225033129" alt=""></p>
<p>那么，现在的关键在于，如何确定上述的最佳回归系数？</p>
<h2 id="基于最优化方法的最佳回归系数确定"><a href="#基于最优化方法的最佳回归系数确定" class="headerlink" title="基于最优化方法的最佳回归系数确定"></a>基于最优化方法的最佳回归系数确定</h2><p>Sigmoid函数：<br><img src="http://img.blog.csdn.net/20170426142819918" alt=""></p>
<p><img src="http://img.blog.csdn.net/20170426144754539" alt=""></p>
<p>这里以二分类为例，假设输入数据的特征向量是[x0,x1,x2,…,xn],乘以回归系统[w0,w1,w2,…,wn]，累加后代入Sigmoid函数中，输出就是一个0~1的值，用于分类。</p>
<p><img src="http://img.blog.csdn.net/20170426145716228" alt=""></p>
<h4 id="梯度上升法"><a href="#梯度上升法" class="headerlink" title="梯度上升法"></a>梯度上升法</h4><p>梯度上升法的主要思想是：从当前函数的最大梯度方向寻找，以求得到函数的最大值。</p>
<p><img src="http://img.blog.csdn.net/20170426152948300" alt=""></p>
<p>梯度上升算法沿着梯度方向移动，总是指向函数值增长最快的方向。</p>
<h4 id="梯度下降算法和梯度上升算法的区别"><a href="#梯度下降算法和梯度上升算法的区别" class="headerlink" title="梯度下降算法和梯度上升算法的区别"></a>梯度下降算法和梯度上升算法的区别</h4><p><img src="http://img.blog.csdn.net/20170426154316798" alt=""></p>
<p>梯度下降算法中的系数更新公式中，是减号。可以这样理解，梯度下降算法是下山的过程，寻找CostFunction最小的点。而梯度上升算法则是上山的过程，但是无论“上山”还是“下山”过程，走的方向始终是沿着当前函数值变化最快的方向(增长最快就是对应上山，下降最快就是对应下山).</p>
<h4 id="训练算法：使用梯度上升找到最佳参数"><a href="#训练算法：使用梯度上升找到最佳参数" class="headerlink" title="训练算法：使用梯度上升找到最佳参数"></a>训练算法：使用梯度上升找到最佳参数</h4><pre><code>#梯度上升法的伪代码：
每个回归系数初始化为1
重复R次：
    计算整个数据集的梯度
    使用α*gradient更新回归系数的向量
    返回回归系数

import numpy as np

#Logistic回归梯度上升法优化算法
def loadDataSet():#读取样本数据
    dataMat=[];labelMat=[]
    fr=open(&apos;testSet.txt&apos;)
    for line in fr.readlines():
        lineArr=line.strip().split()#除去空格后划分
        dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])])#因为只有两个特征
        labelMat.append(int(lineArr[2]))#标签
    return dataMat,labelMat

def sigmoid(inX):
    return 1.0/(1+np.exp(-inX))

def gradAscent(dataMatIn,classLabels):
    dataMatrix=np.mat(dataMatIn)#100*3
    labelMat=np.mat(classLabels).transpose()
    m,n=np.shape(dataMatrix)#m是样本个数，n是特征个数
    alpha=0.001#步长
    maxCycles=500#最大迭代次数
    weights=np.ones((n,1))#回归系数初始化为1
    for k in range(maxCycles):
        h=sigmoid(dataMatrix*weights)#m*n * n*1 -&gt; m*1
        error=(labelMat-h)#
        weights=weights+alpha*dataMatrix.transpose()*error
    return weights


#画出数据集和Logistic回归最佳拟合直线的函数
def plotBestFit(wei):
    import matplotlib.pyplot as plt
    weights=wei#.getA()
    dataMat,labelMat=loadDataSet()
    dataArr=np.array(dataMat)

    n=np.shape(dataArr)[0]
    xcord1=[];ycord1=[]
    xcord2=[];ycord2=[] 
    for i in range(n):
        if int(labelMat[i])==1:
            xcord1.append(dataArr[i,1])
            ycord1.append(dataArr[i,2])
        else:
            xcord2.append(dataArr[i,1])
            ycord2.append(dataArr[i,2])
    fig=plt.figure()
    ax=fig.add_subplot(111)
    print(len(xcord1),len(ycord1))
    ax.scatter(xcord1,ycord1,s=30,c=&apos;red&apos;,marker=&apos;s&apos;)
    ax.scatter(xcord2,ycord2,s=30,c=&apos;green&apos;)
    x=np.arange(-3.0,3.0,0.1)
    y=(-weights[0]-weights[1]*x)/weights[2]
    ax.plot(x,y)
    plt.xlabel(&apos;X1&apos;);plt.ylabel(&apos;X2&apos;);
    plt.show()
</code></pre><p><img src="http://img.blog.csdn.net/20170502140250228" alt=""></p>
<h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><p>回归问题中，需要一个代价函数用来评论拟合过程的“好坏”。对于线性回归问题，它的代价函数是<img src="http://img.blog.csdn.net/20150414211501282?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTHU1OTcyMDM5MzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br>估计值与实际值差的平方和来评断拟合的“好坏”，J(θ)越小，估计值和实际值差距越小，拟合效果越好。</p>
<p>那对于Logistic回归也沿用这个代价函数行不行？ 答案是否定的！因为Sigmoid函数带入J(θ)后，J(θ)不是一个凸函数，会存在很多个局部极值点，无法进行评价拟合的效果。有人定义另外一个代价函数：<img src="http://img.blog.csdn.net/20170427114316036" alt=""></p>
<p>观察这个函数的特性，当预测准确时(实际值y=1且预测值h=1或者实际值y=0且预测值h=0),cost是接近于0的，而当预测很离谱时(实际值y=1且预测值h=0或者实际值y=0且预测值h=1)，cost趋于∞。当然，中间的过程也是符合要求的。</p>
<p>这个分段函数还可以简写为：<img src="http://img.blog.csdn.net/20170427120139821" alt=""></p>
<p>那么对于m条训练样本而言，代价函数就是m个预测代价的累加和：<br><img src="http://img.blog.csdn.net/20170427160158802" alt=""></p>
<p>为了使J(θ)取最小值(这里是取最大值，因为对J(θ)加了一个负号)，对其求偏导：<br><img src="http://img.blog.csdn.net/20170427125124391" alt=""></p>
<p>所以结果就是:<img src="http://img.blog.csdn.net/20170427160242364" alt=""></p>
<pre><code>weights=weights+alpha*dataMatrix.transpose()*error
</code></pre><p><img src="http://img.blog.csdn.net/20170427163508100" alt=""></p>
<p>是一致的！！</p>
<p><img src="http://img.blog.csdn.net/20170427162423610" alt=""></p>
<h4 id="训练算法：随机梯度上升"><a href="#训练算法：随机梯度上升" class="headerlink" title="训练算法：随机梯度上升"></a>训练算法：随机梯度上升</h4><p>梯度上升算法在每次更新回归系数时都需要遍历整个数据集(因为代价函数是m个训练样本数据代价的累加和),该方法在处理100个左右的数据集时尚可，但是如果样本数据过多，特征数过多，那么该方法的计算复杂度就更高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。该方法的另外一个优点是，可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，一次处理所有数据被称作是“批处理”。</p>
<pre><code>#随机梯度上升算法伪代码：
所有回归系数初始化为1
对数据集中每个样本
    计算该样本的梯度
    使用alpha*gradient更新回归系数值
返回回归系数值

#随机梯度上升算法
def stocGradAscent0(dataMatrix,classLabels):
    m,n=np.shape(dataMatrix)
    alpha=0.01#步长
    weights=np.ones(n)#1*n
    X=[]#保持weights的历史数据
    for j in range(300):#迭代300次
        for i in range(m):
            h=sigmoid(sum(dataMatrix[i]*weights))#1*n * 1*n = 
            error=classLabels[i]-h
            weights=weights+alpha*error*dataMatrix[i]
            X.append(weights)
    return weights
</code></pre><p>这段代码与梯度上升算法很相似，但是也有些区别：</p>
<ol>
<li>这里的变量h和error都是数值，而梯度上升算法都是向量(因为这里的是一个样本)</li>
<li>没有矩阵的转换过程，所有变量的数据类型都是NumPy数组</li>
</ol>
<p><strong>但是有个问题，为什么随机梯度上升算法也可以达到最优呢？</strong>书中并没有给出相关解释。</p>
<p><img src="http://img.blog.csdn.net/20170502170148006" alt=""></p>
<p>在迭代200次过程中，weights=[X0,X1,X2]变化曲线；<br>而我自己的300次跌打过程中，变化曲线是：</p>
<p>X0:<br><img src="https://i.imgur.com/QgNPopg.png" alt=""></p>
<p>X1:<img src="https://i.imgur.com/iAz1YYD.png" alt=""></p>
<p>X2:<img src="https://i.imgur.com/Kug7oOn.png" alt=""></p>
<p>从图中看，在大的波动停止之后，还会有一些小的周期波动。产生这种现象的原因是存在一些不能正确分类的样本点，在每次迭代时会引发系数的剧烈变化。</p>
<p>因为这个算法是将m个训练样本一次代入进行weights的更新的，那么如果从m个样本中随机一个进行训练会不会就可以消除这个周期波动呢? 答案是肯定的，作者称之为改进的随机梯度上升算法。</p>
<p>代码如下：</p>
<pre><code>#改进的随机梯度上升算法
def stocGradAscent1(dataMatrix,classLabels,numIter=150):
    m,n=np.shape(dataMatrix)
    weights=np.ones(n)
    X=[]
    for j in range(numIter):
        dataIndex=list(range(m))
        for i in range(m):
            alpha=4/(1.0+j+i)+0.01
            randIndex=int(np.random.uniform(0,len(dataIndex)))
            h=sigmoid(sum(dataMatrix[randIndex]*weights))
            error=classLabels[randIndex]-h
            weights=weights+alpha*error*dataMatrix[randIndex]
            X.append(weights)
            del dataIndex[randIndex]
    X=np.array(X)
    return weights
</code></pre><p>改进之处有两个方面，<strong>每次迭代时调整alpha</strong>和<strong>随机选取更新</strong>。</p>
<p>步长alpha在每次迭代的时候都会调整，这会缓解数据波动和高频波动。另外alpha会随着迭代次数不断减小，但永远不会减小到0，这是因为alpha=4/(1.0+j+i)+0.01,存在一个常数项。必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。如果要处理的问题是动态变化的，那么可以适当加大上述常数项，来确保新的值获得更大的回归系数。另一点值得注意的是，在降低alpha的函数中，alpha每次减少1/(j+i)，其中j是迭代次数，i是样本点的下标。这样当j&lt;&lt;max(i)时，alpha就不是严格下降的。</p>
<p>通过随机选取样本来更新回归系数，可以减少周期性的波动。</p>
<p><img src="http://img.blog.csdn.net/20170502193141309" alt=""></p>
<h2 id="示例：从疝气病症预测病马的死亡率"><a href="#示例：从疝气病症预测病马的死亡率" class="headerlink" title="示例：从疝气病症预测病马的死亡率"></a>示例：从疝气病症预测病马的死亡率</h2><p>使用Logistic回归来预测患有疝气病的马的存活问题。数据集包括368个样本和21个特征。</p>
<p>使用Logistic回归估计马疝气病的死亡率</p>
<ol>
<li>收集数据：UCI机器学习数据库</li>
<li>准备数据：用Python解析文本文件并填充缺失值</li>
<li>分析数据：可视化并观察数据</li>
<li>训练算法：使用优化算法，找到最佳的系数</li>
<li>测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。</li>
<li>使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果。</li>
</ol>
<p>(数据集中有30%的值是缺失的)</p>
<h4 id="准备数据：处理数据中的缺失值"><a href="#准备数据：处理数据中的缺失值" class="headerlink" title="准备数据：处理数据中的缺失值"></a>准备数据：处理数据中的缺失值</h4><p>由于一些不可避免的原因，数据集经常会是缺失不全的，所以必须采用一些方法来解决这个问题：</p>
<ul>
<li>使用可用特征的均值来填充缺失值；</li>
<li>使用特殊值来填补缺失值，如-1；</li>
<li>忽略有缺失值的样本；</li>
<li>使用相似样本的均值添补缺失值；</li>
<li>使用另外的机器学习算法预测缺失值；</li>
</ul>
<p>在数据预处理阶段，所有缺失值用实数0代替。原因有二：</p>
<ol>
<li>根据回归系数的更新公式，若特征值为0，不会对特征系数进行更新；</li>
<li>sigmoid(0)=0.5,对结果不具有任何倾向性；</li>
</ol>
<p>对样本数据中类别标签已经缺失的，应该丢弃。因为类别标签与特征不同，很难确定采用某个合适的值来替代，而且对训练模型影响较大。</p>
<h4 id="测试算法：用Logistic回归进行分类"><a href="#测试算法：用Logistic回归进行分类" class="headerlink" title="测试算法：用Logistic回归进行分类"></a>测试算法：用Logistic回归进行分类</h4><pre><code>#Logistic回归分类函数
def classifyVector(inX,weights):
    prob=sigmoid(sum(inX*weights))
    if prob&gt;0.5:
        return 1.0
    else:
        return 0.0

def colicTest():
    frTrain=open(&apos;horseColicTraining.txt&apos;)
    frTest=open(&apos;horseColicTest.txt&apos;)
    trainingSet=[];trainingLabels=[]
    for line in frTrain.readlines():
        currLine=line.strip().split(&apos;\t&apos;)
        lineArr=[]
        for i in range(21):#21个特征
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[21])) #标签               
    trainWeights=stocGradAscent1(np.array(trainingSet),trainingLabels,500)
    errorCount=0;numTestVec=0.0
    for line in frTest.readlines():
        numTestVec+=1.0
        currLine=line.strip().split(&apos;\t&apos;)
        lineArr=[]
        for i in range(21):
            lineArr.append(float(currLine[i]))
        if int(classifyVector(np.array(lineArr),trainWeights))!=int(currLine[21]):
                errorCount+=1
    errorRate=(float(errorCount)/numTestVec)
    print(&quot;the error rate of this test is: %f&quot; % errorRate)
    return errorRate

def multiTest():
    numTests=10;errorSum=0.0
    for k in range(numTests):
        errorSum+=colicTest()
    print(&quot;after %d iterations the average error rate is: %f&quot; 
          % (numTests,errorSum/float(numTests)))
</code></pre><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://blog.csdn.net/lu597203933/article/details/38468303#comments" target="_blank" rel="external">http://blog.csdn.net/lu597203933/article/details/38468303#comments</a></li>
<li><a href="http://blog.csdn.net/CharlieLincy/article/details/70767791#comments" target="_blank" rel="external">http://blog.csdn.net/CharlieLincy/article/details/70767791#comments</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/05/机器学习实战第四章学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/05/机器学习实战第四章学习笔记/" itemprop="url">机器学习实战第四章学习笔记(朴素贝叶斯分类器)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-05T00:00:00+08:00">
                2017-09-05
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>之前两章，k-NN近邻算法和决策树算法构建的分类器都是对测试数据给出一种具体的类别，即‘非黑即白’。而朴素贝叶斯分类器会给出每个类别的概率，将概率最大的类别作为最优类别猜测结果。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201703/58b978fa8bdb6.png?imageMogr2/format/jpg/quality/90" alt="贝叶斯概率公式"></p>
<h2 id="基于贝叶斯决策理论的分类方法"><a href="#基于贝叶斯决策理论的分类方法" class="headerlink" title="基于贝叶斯决策理论的分类方法"></a>基于贝叶斯决策理论的分类方法</h2><ul>
<li>优点：在数据较少的情况下仍然有效，可以处理多类别问题。</li>
<li>缺点：对于输入数据的准备方式较为敏感</li>
<li>适用数据类型：离散型</li>
</ul>
<h4 id="贝叶斯决策理论"><a href="#贝叶斯决策理论" class="headerlink" title="贝叶斯决策理论"></a>贝叶斯决策理论</h4><p>举个简单的小例子：</p>
<p>假设有一个数据集，每条数据样本都有两个特征(x,y)和一个类别标签(0,1).目标是把一条新的数据贴上准确的标签。</p>
<p>用p1(x,y)表示数据(x,y)属于类别1的概率，用p2(x,y)表示数据(x,y)属于类别2的概率。那么，用条件概率公式来表示这两个概率，就分别是P(c1|x,y)和P(c2|x,y),具体意义就是：由x和y这两个属性值确定的数据，其属于类别1、类别2的概率。</p>
<p>由<a href="https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A6%82%E7%8E%87" target="_blank" rel="external">贝叶斯定理</a>，可得到：</p>
<p><img src="http://img.blog.csdn.net/20170418192042562" alt=""></p>
<p>p(x,y|c<em>i</em>):基于一个样本数据集，在c<em>i</em>这个类中，数据(x,y)出现的概率。</p>
<p>p(c<em>i</em>):基于这个样本数据集，属于c<em>i</em>这个类别的数据，占所有数据的数量。</p>
<p>p(x,y):在所有样本数据中，数据(x,y)出现的概率。</p>
<p><em>用具体的数字说明一下：</em></p>
<p>该样本数据集中共有20个样本，标签为c<em>1</em>的为15个，标签c<em>2</em>的为5个，(X0,Y0)是该样本集中独特的一个，标签为c<em>1</em>。</p>
<p>则p(X0,Y0|c<em>1</em>)=1/15,</p>
<p>p(c<em>1</em>)=2/3,</p>
<p>p(X0,Y0)=1/20</p>
<p>对于数据集中任意一个样本数据(x,y)来说，用p1(x,y)表示数据点(x,y)属于类别1的概率，用p2(x,y)表示数据点(x,y)属于类别2的概率，那么对于一个新数据点(x,y)，可以用下面的规则判断它的类别：</p>
<ul>
<li>如果p1(x,y)=p(c<em>1</em>|x,y) &gt; p2(x,y)=p(c<em>2</em>|x,y),那么类别为1.</li>
<li>如果p1(x,y)=p(c<em>1</em>|x,y) &lt; p2(x,y)=p(c<em>2</em>|x,y),那么类别为2.</li>
</ul>
<p>简单的讲，我们会选择高概率对应的类别，这也是贝叶斯决策理论的核心思想。</p>
<h2 id="使用朴素贝叶斯进行文档分类"><a href="#使用朴素贝叶斯进行文档分类" class="headerlink" title="使用朴素贝叶斯进行文档分类"></a>使用朴素贝叶斯进行文档分类</h2><p>朴素贝叶斯的一般过程：</p>
<ol>
<li>收集数据：可以使用任何方法，这里使用RSS源</li>
<li>准备数据：需要数值型或者布尔型数据。</li>
<li>分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。</li>
<li>训练算法：计算不同的独立特征的条件概率。</li>
<li>测试算法：计算错误率。</li>
<li>使用算法：一个常见的朴素贝叶斯应用是文档分类。</li>
</ol>
<p>由统计学知识：</p>
<p><strong>如果每个特征需要N个样本，那么对于10个特征将需要N^10个样本，对于包含1000个特征的词汇表将需要N^1000个样本。</strong></p>
<p>如果特征之间相互独立，那么样本数就可以从N^1000减少到1000<em>N. </em>那么什么是独立呢？？*</p>
<p>所谓独立指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。</p>
<p>而这正是朴素贝叶斯分类器的一个假设，即<strong>特征之间相互统计独立</strong>。朴素贝叶斯分类器的另一个假设是，<strong>每个特征同等重要</strong>。</p>
<p>而这两个<strong>天真</strong>的假设正是朴素贝叶斯分类器中朴素(naive)一词的来源。这个假设虽然看起来很不靠谱，但在实际运用的时候，往往能达到比较好的效果。</p>
<p>朴素贝叶斯分类器通常有两种实现方式：一种基于贝努力模型实现，一种基于多项式模型实现。区别在于，是否考虑词在文档中出现的次数。</p>
<h2 id="使用Python进行文本分类"><a href="#使用Python进行文本分类" class="headerlink" title="使用Python进行文本分类"></a>使用Python进行文本分类</h2><p>首先给出将文本转换为数字向量的过程，然后介绍如何基于这些向量来计算条件概率，并在此基础上构建分类器，最后还要介绍一些利用Python实现朴素贝叶斯过程中需要考虑的问题。</p>
<h4 id="从文本中构建词向量"><a href="#从文本中构建词向量" class="headerlink" title="从文本中构建词向量"></a>从文本中构建词向量</h4><pre><code>#词表到向量的转换函数
def loadDataSet():        
    postingList=[[&apos;my&apos;,&apos;dog&apos;,&apos;has&apos;,&apos;flea&apos;,&apos;problems&apos;,&apos;help&apos;,&apos;please&apos;],
                 [&apos;maybe&apos;,&apos;not&apos;,&apos;take&apos;,&apos;him&apos;,&apos;to&apos;,&apos;dog&apos;,&apos;park&apos;,&apos;stupid&apos;],
                 [&apos;my&apos;,&apos;dalmation&apos;,&apos;is&apos;,&apos;so&apos;,&apos;cute&apos;,&apos;I&apos;,&apos;love&apos;,&apos;him&apos;],
                 [&apos;stop&apos;,&apos;posting&apos;,&apos;stupid&apos;,&apos;worthless&apos;,&apos;garbage&apos;],
                 [&apos;mr&apos;,&apos;licks&apos;,&apos;ate&apos;,&apos;my&apos;,&apos;steak&apos;,&apos;how&apos;,&apos;to&apos;,&apos;stop&apos;,&apos;him&apos;],
                 [&apos;quit&apos;,&apos;buying&apos;,&apos;worthless&apos;,&apos;dog&apos;,&apos;food&apos;,&apos;stupid&apos;]]

    classVec=[0,1,0,1,0,1]
    #1，代表侮辱性文字，0，代表正常言论
    return postingList,classVec
#创建词汇表
def createVocabList(dataSet):
    vocabSet=set([])
    for document in dataSet:
        vocabSet=vocabSet|set(document)
    return list(vocabSet)
#将文本转换为向量
#依据词汇表中单词的有无，输出向量相应位置置1或置0
def setOfWords2Vec(vocabList,inputSet):
    returnVec=[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]=1
        else:
            print(&quot;the word: %s is not in my Vocabulary!&quot; % word)
    return returnVec
</code></pre><p>第一个函数创建的是数据样本，第二个函数合并所有样本文档，生成一个词汇表，这个不重复词表可看做是一个一维向量。第三个函数，根据样本文档，参考词汇表向量，若样本文档中含有某些词汇，则输出向量相应位置会置1.</p>
<h4 id="训练算法：从词向量计算概率"><a href="#训练算法：从词向量计算概率" class="headerlink" title="训练算法：从词向量计算概率"></a>训练算法：从词向量计算概率</h4><p><img src="http://img.blog.csdn.net/20170418214530308" alt=""></p>
<p>将之前的x,y替换为w,w在这里是一个词向量。</p>
<p>这个公式，等号右边的p(c<em>i</em>)是最好计算的，用类别<em>i</em>中文档树除以总的文档数，就可以得到类别c<em>i</em>的概率。接着计算p(w|c<em>i</em>),这里使用朴素贝叶斯假设，如果将w展开为一个个独立特征，则p(w|c<em>i</em>)等于p(w0,w1,w2,…,wN|c<em>i</em>),即等于p(w0|c<em>i</em>)p(w1|c<em>i</em>)p(w2|c<em>i</em>)…p(wN|c<em>i</em>).</p>
<pre><code>#基于条件独立性假设
#朴素贝叶斯分类器训练函数
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs=len(trainMatrix)#训练样本数量
    numWords=len(trainMatrix[0])#每个训练样本的特征数
    pAbusive=sum(trainCategory)/float(numTrainDocs)#侮辱性概率
    p0Num=np.ones(numWords);p1Num=np.ones(numWords)
    p0Denom=2.0;p1Denom=2.0
    for i in range(numTrainDocs):
        if trainCategory[i]==1:
            p1Num+=trainMatrix[i]#标签为abusive的文档中，词汇表的单词出现过多少次
            p1Denom+=sum(trainMatrix[i])#到目前为止，标签为abusive的文档中出现过单词的总量
        else:
            p0Num+=trainMatrix[i]#到目前为止，标签为normal的文档中，词汇表的单词出现过多少次
            p0Denom+=sum(trainMatrix[i])#到目前为止，标签为normal的文档中出现过单词的总量
    p1Vect=np.log(p1Num/p1Denom)#标签为abusive的文档中，词汇的各个单词的出现概率
    #计算的是p(w*j*|c*i*)
    p0Vect=np.log(p0Num/p0Denom)#取自然对数，防止下溢
    return p0Vect,p1Vect,pAbusive
</code></pre><p>函数的伪代码如下：</p>
<pre><code>计算每个类别中的文档数目
对每篇训练文档：
    对每个类别：
        如果词条出现文档中--&gt;增加该词条的计数值
        增加所有词条的计数值
    对每个类别：
        对每个词条：
            将该词条的数目除以总词条数目得到条件概率
    返回每个类别的条件概率
</code></pre><p>函数返回结果是：</p>
<pre><code>#类别0的各个词条条件概率
p0Vect=[p(w0|c*0*),p(w1|c*0*),p(w2|c*0*),...,p(wN|c*0*)]
#类别1的各个词条条件概率
p1Vect=[p(w0|c*1*),p(w1|c*1*),p(w2|c*1*),...,p(wN|c*1*)]
#类别为Abusive的概率
pAbusive
</code></pre><h4 id="测试算法：根据现实情况修改分类器"><a href="#测试算法：根据现实情况修改分类器" class="headerlink" title="测试算法：根据现实情况修改分类器"></a>测试算法：根据现实情况修改分类器</h4><p>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别，即计算p(w0|c<em>i</em>)p(w1|c<em>i</em>)p(w2|c<em>i</em>)。若其中一个概率值为0，那么最后的乘积也为0.为降低这种影响，可以将所有词的出现次数初始化为1，并将分母初始化为2.（因为要计算的是概率，若样本词条数较大的话，这样处理对结果的影响并不是很大）。</p>
<p>另一个问题，下溢出。当计算p(w0|c<em>i</em>)p(w1|c<em>i</em>)p(w2|c<em>i</em>)…p(wN|c<em>i</em>)时，可能大部分因子都是非常小的，所以程序会产生下溢出或者得不到正确答案（结果非常接近0，计算机精度有限，而直接将其近似为0）。一种解决办法是对乘积取自然对数，<em>ln(a\</em>b)=ln(a)+ln(b)*. 因为最终判决时，是比较两个类别概率的大小，对他们取自然对数并不影响他们的相对大小关系。</p>
<pre><code>#朴素贝叶斯分类器
def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):
    p1=sum(vec2Classify*p1Vec)+np.log(pClass1)
    #因为取自然对数，所以原来的乘积运算转换为加法运算
    p0=sum(vec2Classify*p0Vec)+np.log(1.0-pClass1)
    if p1&gt;p0:
        return 1
    else:
        return 0

def testingNB():
    listOPosts,listClasses=loadDataSet()
    myVocabList=createVocabList(listOPosts)#词汇表

    trainMat=[]
    for postinDoc in listOPosts:#获取训练样本集
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
    p0V,p1V,pAb=trainNB0(np.array(trainMat),np.array(listClasses))#训练

    testEntry=[&apos;love&apos;,&apos;my&apos;,&apos;dalmation&apos;]
    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))
    print(testEntry,&apos;classified as: &apos;,classifyNB(thisDoc,p0V,p1V,pAb))

    testEntry=[&apos;stupid&apos;,&apos;garbage&apos;]
    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))
    print(testEntry,&apos;classified as: &apos;,classifyNB(thisDoc,p0V,p1V,pAb))
</code></pre><p>做个简单的小结：</p>
<p>首先将文档合并提取出一个不重复的词汇表，对每个文档依据这个不重复词汇表转换为一维向量，向量的长度就是词汇表的个数。接下来，根据文档的类别标签，计算p(w<em>j</em>|c<em>i</em>),计算过程是：在c<em>i</em>类别中，词条w<em>j</em>出现的次数除以c<em>i</em>中总词条数目。得到p(w<em>j</em>|c<em>i</em>)后，基于朴素贝叶斯分类器的假设，可以计算p(<strong>w</strong>|c<em>i</em>).而p(c<em>i</em>)可以很容易求出来。然后比较p(<strong>w</strong>|c<em>0</em>)p(c<em>0</em>)和p(<strong>w</strong>|c<em>1</em>)p(c<em>1</em>)的大小，就可以判定属于哪个类别。</p>
<h4 id="文档词袋模型"><a href="#文档词袋模型" class="headerlink" title="文档词袋模型"></a>文档词袋模型</h4><p>目前为止，我们是将每个词的出现与否作为一个特征的，这可以被描述为词集模型（set-of-words model).如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为词袋模型(bag-of-words model).显然，词条出现的次数应该也要被考虑在内，因此，将函数setOfWords2Vec()修改为bagOfWords2Vec().</p>
<pre><code>#词集模型--》词袋模型
#朴素贝叶斯词袋模型
def bagOfWords2VecMN(vocabList,inputSet):
    returnVec=[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]+=1
    return returnVec
</code></pre><h2 id="示例：使用朴素贝叶斯过滤垃圾邮件"><a href="#示例：使用朴素贝叶斯过滤垃圾邮件" class="headerlink" title="示例：使用朴素贝叶斯过滤垃圾邮件"></a>示例：使用朴素贝叶斯过滤垃圾邮件</h2><ol>
<li>收集数据：提供文本文件</li>
<li>准备数据：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainNB0()函数</li>
<li>测试算法：使用classifyNB(),并且构建一个新的测试函数来计算文档集的错误率</li>
<li>使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。</li>
</ol>
<h4 id="测试算法：使用朴素贝叶斯进行交叉验证"><a href="#测试算法：使用朴素贝叶斯进行交叉验证" class="headerlink" title="测试算法：使用朴素贝叶斯进行交叉验证"></a>测试算法：使用朴素贝叶斯进行交叉验证</h4><pre><code>#文件解析及完整的垃圾邮件测试函数
def textParse(bigString):
    import re
    listOfTokens=re.split(r&apos;\W*&apos;,bigString)
    return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]
#垃圾邮件分类测试
def spamTest():
    docList=[];classList=[];fullText=[]
    for i in range(1,26):
        wordList=textParse(open(&apos;email/spam/%d.txt&apos; % i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)
        wordList=textParse(open(&apos;email/ham/%d.txt&apos; % i).read())
        #第23个文件有些问题，需要修改
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    vocabList=createVocabList(docList)#构建词汇表
    trainingSet=list(range(50));testSet=[]
    for i in range(10):#随机挑选10个邮件作为测试集
        randIndex=int(np.random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses))#进行训练
    errorCount=0
    for docIndex in testSet:
        wordVector=setOfWords2Vec(vocabList,docList[docIndex])
        if classifyNB(np.array(wordVector),p0V,p1V,pSpam)!= classList[docIndex]:
            errorCount+=1
    print(&apos;the error rate is: &apos;,float(errorCount)/len(testSet))
</code></pre><h2 id="示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向"><a href="#示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="headerlink" title="示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向"></a>示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向</h2><ol>
<li>收集数据：从RSS源收集内容，这里需要对RSS源构建一个借口</li>
<li>准备数据：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainNB0()函数</li>
<li>测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。</li>
<li>使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。</li>
</ol>
<h4 id="收集数据：导入RSS源"><a href="#收集数据：导入RSS源" class="headerlink" title="收集数据：导入RSS源"></a>收集数据：导入RSS源</h4><p>安装feedparser包</p>
<pre><code>pip install feedparser

#RSS源分类器及高频词去除函数
def calcMostFreq(vocabList,fullText):
    import operator
    freqDict={}
    for token in vocabList:
        freqDict[token]=fullText.count(token)
    sortedFreq=sorted(freqDict.items(),key=operator.itemgetter(1),\
                      reverse=True)
    return sortedFreq[:30]
#返回前30个高频词条


def localWords(feed1,feed0):
    import feedparser
    docList=[];classList=[];fullText=[]
    minLen=min(len(feed1[&apos;entries&apos;]),len(feed0[&apos;entries&apos;]))#帖子数量
    for i in range(minLen):
        wordList=textParse(feed1[&apos;entries&apos;][i][&apos;summary&apos;])#帖子内容解析
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)
        wordList=textParse(feed0[&apos;entries&apos;][i][&apos;summary&apos;])
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    vocabList=createVocabList(docList)#创建词汇表
    top30words=calcMostFreq(vocabList,fullText)
    for pairW in top30words:
        if pairW[0] in vocabList:
            vocabList.remove(pairW[0])#去除出现次数最高的那些词
    trainingSet=list(range(2*minLen));testSet=[]
    for i in range(int(minLen/10)):#选取10%的数据，建立测试集
        randIndex=int(np.random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat=[];trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses))
    errorCount=0
    for docIndex in testSet:
        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])
        if classifyNB(np.array(wordVector),p0V,p1V,pSpam) !=\
            classList[docIndex]:
                errorCount+=1
    print(&apos;the error rate is: &apos;,float(errorCount)/len(testSet))
    return vocabList,p0V,p1V
</code></pre><p>这个分类器的作用是给出一个帖子，判断它是来自哪个地区的（东部还是西部，因为两地的语言用词可能不太一样）。<br>前30个高频词去掉的原因是，一般很多词都是虚词，如“maybe”、”yes”、“will”、”can”等等。当然这样处理是比较粗糙的，一般会根据停用词表来进行处理。</p>
<pre><code>#最具表征性的词汇显示函数
def getTopWords(ny,sf):
    import operator
    vocabList,p0V,p1V=localWords(ny,sf)
    topNY=[];topSF=[]
    for i in range(len(p0V)):
        if p0V[i]&gt;-6.0:topSF.append((vocabList[i],p0V[i]))
        if p1V[i]&gt;-6.0:topNY.append((vocabList[i],p1V[i]))
    sortedSF=sorted(topSF,key=lambda pair:pair[1],reverse=True)
    print(&quot;SF**&quot;*10)
    for item in sortedSF:
        print(item[0])
    sortedNY=sorted(topNY,key=lambda pair:pair[1],reverse=True)
    print(&quot;NY**&quot;*10)
    for item in sortedNY:
        print(item[0])
</code></pre><p>Craigslist这个网站是分地区的，比如纽约（New York，美国东部）和旧金山（San Francisco，美国西部）。我们从这两个地区的Ctaigslist里面选取一些帖子，通过分析这些帖子里的征婚广告信息，来比较这两个城市的人们在广告用词上是否存在差异。如果确实存在差异，那么两个地区的人各自常用的词是哪些？</p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://blog.csdn.net/CharlieLincy/article/details/70244028" target="_blank" rel="external">《机器学习实战》第四章：朴素贝叶斯（2）两个实例</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/03/机器学习实战第三章学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/03/机器学习实战第三章学习笔记/" itemprop="url">机器学习实战第三章学习笔记(决策树-Decision Tree)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-03T00:00:00+08:00">
                2017-09-03
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="决策树-Decision-Tree-的概念"><a href="#决策树-Decision-Tree-的概念" class="headerlink" title="决策树(Decision Tree)的概念"></a>决策树(Decision Tree)的概念</h2><p><strong>基本思想：</strong>决策树是一种树结构，其中的每个内部节点代表对某一特征的一次测试，每条边代表一个测试结果，叶节点代表某个类或类的分布。决策树的决策过程需要从决策树的根节点开始，待测试数据与决策树中的特征节点进行比较，并按照比较结果选择下一个比较分支，直到叶子节点作为最终的决策结果。</p>
<p>决策树算法是一个分类算法，较k-近邻算法(无法给出数据的内在含义)的主要优势在于数据形式非常容易理解。</p>
<p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。</p>
<p>缺点：可能会产生过度匹配问题。</p>
<p>适用数据类型：数值型和标称型</p>
<p><img src="https://i.imgur.com/34kfLZ3.jpg" alt=""></p>
<h2 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h2><p>构造决策树时，首先面临的问题是，当前数据集上哪个特征在划分数据分类时起决定性作用。根据决定性的特征，进行划分，这样原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。直到所有具有相同类型的数据均在一个数据子集内。</p>
<h4 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h4><ol>
<li>收集数据：可以使用任何方法。</li>
<li>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。</li>
<li>分析数据：可以使用任何方法，构造树完成之后，应该检查图形是否符合预期。</li>
<li>训练算法：构造树的数据结构。</li>
<li>测试算法：使用经验树计算错误率</li>
<li>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。</li>
</ol>
<p>这里使用<a href="https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95" target="_blank" rel="external">ID3算法</a>划分数据集，相关算法还有C4.5算法等。</p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p><strong>划分数据集的大原则是：将无序的数据变得更加有序</strong></p>
<p>？(数据的无序和有序如何度量呢，用熵！)</p>
<p>在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择，就是起决定性作用的特征！</p>
<p>集合信息的度量方式被称为香农熵或简称熵，来源于信息论之父克劳德.香农。</p>
<p>克劳德.香农是被公认是20世纪最聪明的人之一，威廉.庞德斯通在2005年出版的《财富公式》一书中是这样描述克劳德.香农的:</p>
<blockquote>
<p>“贝尔实验室和MIT有很多人将香农和爱因斯坦相提并论，而其他人则认为这种对比是不公平的—对香农是不公平的。”</p>
</blockquote>
<p>信息增益(information gain)和熵(entropy)</p>
<p>熵是指信息的期望值，信息增益是数据集划分之前的熵减去划分之后的熵。</p>
<p>如果待分类事务可能划分在多个分类之中，假设X<em>i</em>是其中的一个类，则符号X<em>i</em>的信息定义为：<br><img src="http://img.blog.csdn.net/20170406145216705" alt=""></p>
<p>p(X<em>i</em>)是选择该分类的概率。</p>
<p>为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：<br><img src="http://img.blog.csdn.net/20170406145221063" alt=""></p>
<p>其中，n是分类的数目。</p>
<pre><code>from math import log
import operator

#计算给定数据集的香农熵
def calcShannonEnt(dataSet):
    numEntries=len(dataSet)
    labelCounts={}
    for featVec in dataSet:
        currentLabel=featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel]=0
        labelCounts[currentLabel]+=1
    shannonEnt=0.0
    for key in labelCounts:
        prob=float(labelCounts[key])/numEntries
        shannonEnt-=prob*log(prob,2.0)
    return shannonEnt
#创建简单数据集
def createDataSet():
    dataSet=[[1,1,&apos;yes&apos;],
             [1,1,&apos;yes&apos;],
             [1,0,&apos;no&apos;],
             [0,1,&apos;no&apos;],
             [0,1,&apos;no&apos;]]
    labels=[&apos;no surfacing&apos;,&apos;flippers&apos;]
    return dataSet, labels
</code></pre><h4 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h4><p>对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。</p>
<pre><code>#按照给定特征划分数据集
def splitDataSet(dataSet,axis,value):
#&quot;&quot;&quot;
#   待划分的数据集、划分数据集的特征，特征的返回值
#&quot;&quot;&quot;
#对数据集进行划分，数据子集的个数由该特征所有可能值个数
    retDataSet=[]
    for featVec in dataSet:
        if featVec[axis]== value:
            reducedFeatVec=featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet

#选择最好的数据集划分方式
def chooseBestFeatureToSplit(dataSet):
    numFeatures=len(dataSet[0])-1
    baseEntropy=calcShannonEnt(dataSet)
    bestInfoGain=0.0;bestFeature=-1
    for i in range(numFeatures):
        featList=[example[i] for example in dataSet]
        uniqueVals=set(featList)
        newEntropy=0.0
        for value in uniqueVals:
            subDatSet=splitDataSet(dataSet,i,value)
            prob=len(subDatSet)/float(len(dataSet))
            newEntropy+=prob*calcShannonEnt(subDatSet)
        infoGain=baseEntropy-newEntropy
        if(infoGain&gt;bestInfoGain):
            bestInfoGain=infoGain
            bestFeature=i
    return bestFeature
</code></pre><h4 id="递归构建决策树"><a href="#递归构建决策树" class="headerlink" title="递归构建决策树"></a>递归构建决策树</h4><p>决策树构建过程大致如下：</p>
<p>得到原始数据集，然后基于最好的属性值（特征）划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据集。因此我们可以采用递归的原则处理数据集。</p>
<p>递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。</p>
<pre><code>#返回出现次数最多的分类名称
def majorityCnt(classList):
    classCount={}
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote]=0
        classCount[vote]+=1
    sortedClassCount=sorted(classCount.items(),
                            key=operator.itemgetter(1),
                            reverse=True)
    return sortedClassCount[0][0]

#创建树的函数代码
def createTree(dataSet,labels):
    #dataSet:数据集
    #labels:标签列表
    classList=[example[-1] for example in dataSet]
    if classList.count(classList[0])==len(classList):
        return classList[0]#递归结束条件2，类别完全相同
    if len(dataSet[0])==1:
        return majorityCnt(classList)#递归结束条件3，遍历完所有特征后，类别还未完全相同，返回出现次数最多的

    bestFeat=chooseBestFeatureToSplit(dataSet)
    bestFeatLabel=labels[bestFeat]
    myTree={bestFeatLabel:{}}
    del(labels[bestFeat])
    featValues=[example[bestFeat] for example in dataSet]
    uniqueVals=set(featValues)
    for value in uniqueVals:
        subLabels=labels[:]
        myTree[bestFeatLabel][value]=createTree(splitDataSet\
              (dataSet,bestFeat,value),subLabels)
    #构建最佳属性的值为Value的子树
    return myTree
</code></pre><p>该函数有3个return，前两个返回类型是‘标签’对应叶子节点，而第3个返回类型是一棵树（dict).</p>
<h2 id="利用Matplotlib注解绘制树形图"><a href="#利用Matplotlib注解绘制树形图" class="headerlink" title="利用Matplotlib注解绘制树形图"></a>利用Matplotlib注解绘制树形图</h2><h4 id="Matplotlib注解"><a href="#Matplotlib注解" class="headerlink" title="Matplotlib注解"></a>Matplotlib注解</h4><p>Matplotlib提供了一个注解工具annotations,可以在数据图形上添加文本注释，注解通常用于解释数据的内容。</p>
<pre><code>import matplotlib.pyplot as plt

#文本框
decisionNode = dict(boxstyle=&apos;sawtooth&apos;,fc=&apos;0.8&apos;)
leafNode=dict(boxstyle=&apos;round4&apos;,fc=&apos;0.8&apos;)
#箭头格式
arrow_args=dict(arrowstyle=&apos;&lt;-&apos;)

绘制带箭头的注解
def plotNode(nodeTxt,centerPt,parentPt,nodeType):
    createPlot.ax1.annotate(nodeTxt,xy=parentPt,
                            xycoords=&apos;axes fraction&apos;,
                            xytext=centerPt,
                            textcoords=&apos;axes fraction&apos;,
    va=&apos;center&apos;,ha=&apos;center&apos;,bbox=nodeType,arrowprops=
    arrow_args)

def createPlot(inTree):
    fig=plt.figure(1,facecolor=&apos;white&apos;)
    fig.clf()
    axprops=dict(xticks=[],yticks=[])
    createPlot.ax1=plt.subplot(111,frameon=False,**axprops)
    plotTree.totalW=float(getNumLeafs(inTree))
    plotTree.totalD=float(getTreeDepth(inTree))
    plotTree.xOff=-0.5/plotTree.totalW;plotTree.yOff=1.0;
    plotTree(inTree,(0.5,1.0),&apos;&apos;)

#    plotNode(&apos;a decision node&apos;,(0.5,0.1),(0.1,0.5),decisionNode)
#    plotNode(&apos;a leaf node&apos;,(0.8,0.1),(0.3,0.8),leafNode)
    plt.show()
</code></pre><h4 id="构造注解树"><a href="#构造注解树" class="headerlink" title="构造注解树"></a>构造注解树</h4><p>定义两个新函数getNumLeafs()和getTreeDepth()，来获取叶节点的数目和树的层数，以确定图x轴和y轴。</p>
<pre><code>#获取叶节点的数目
def getNumLeafs(myTree):
    numLeafs=0
    firstStr=list(myTree.keys())[0]
    secondDict=myTree[firstStr]
    for key in secondDict.keys():
        #测试节点的类型是否为字典，不是则为叶子节点
        if type(secondDict[key]).__name__==&apos;dict&apos;:
            numLeafs+=getNumLeafs(secondDict[key])
        else:
            numLeafs+=1
    return numLeafs

#获取树的层数
def getTreeDepth(myTree):
    maxDepth=0
    firstStr=list(myTree.keys())[0]
    secondDict=myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__==&apos;dict&apos;:
            thisDepth=1+getTreeDepth(secondDict[key])
        else:
            thisDepth=1
        if thisDepth&gt;maxDepth:
            maxDepth=thisDepth
    return maxDepth

def retrieveTree(i):
    listOfTrees=[{&apos;no surfacing&apos;:{0:&apos;no&apos;,1:{&apos;flippers&apos;:\
                                            {0:&apos;no&apos;,1:&apos;yes&apos;}}}},
    {&apos;no surfacing&apos;:{0:&apos;no&apos;,1:{&apos;flippers&apos;:\
                               {0:{&apos;head&apos;:{0:&apos;no&apos;,1:&apos;yes&apos;}},1: &apos;no&apos;}}}}]
    return listOfTrees[i]
</code></pre><p>函数retrieveTree输出预先存储的树信息，避免了每次测试代码时都要从数据中创建树的麻烦。</p>
<pre><code>def plotMidText(cntrPt,parentPt,txtString):
    xMid=(parentPt[0]-cntrPt[0])/2.0+cntrPt[0]
    yMid=(parentPt[1]-cntrPt[1])/2.0+cntrPt[1]
    createPlot.ax1.text(xMid,yMid,txtString)

def plotTree(myTree,parentPt,nodeTxt):
    numLeafs=getNumLeafs(myTree)
    depth=getTreeDepth(myTree)
    firstStr=list(myTree.keys())[0]
    cntrPt=(plotTree.xOff+(1.0+float(numLeafs))/2.0/plotTree.totalW,
            plotTree.yOff)
    plotMidText(cntrPt,parentPt,nodeTxt)
    plotNode(firstStr,cntrPt,parentPt,decisionNode)
    secondDict=myTree[firstStr]
    plotTree.yOff=plotTree.yOff-1.0/plotTree.totalD
    for key in secondDict.keys():
        if type(secondDict[key]).__name__==&apos;dict&apos;:
            plotTree(secondDict[key],cntrPt,str(key))
        else:
            plotTree.xOff=plotTree.xOff+1.0/plotTree.totalW
            plotNode(secondDict[key],(plotTree.xOff,plotTree.yOff),
                     cntrPt,leafNode)
            plotMidText((plotTree.xOff,plotTree.yOff),cntrPt,str(key))
    plotTree.yOff=plotTree.yOff+1.0/plotTree.totalD
</code></pre><p>绘制的树形图<br><img src="https://i.imgur.com/FTBCVsg.jpg" alt=""></p>
<h2 id="测试和存储分类器"><a href="#测试和存储分类器" class="headerlink" title="测试和存储分类器"></a>测试和存储分类器</h2><p>依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类，在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。</p>
<h4 id="测试算法："><a href="#测试算法：" class="headerlink" title="测试算法："></a>测试算法：</h4><pre><code>#使用决策树的分类函数
def classify(inputTree,featLabels,testVec):
    firstStr=list(inputTree.keys())[0]
    secondDict=inputTree[firstStr]
    featIndex=featLabels.index(firstStr)
    for key in secondDict.keys():
        if testVec[featIndex]==key:
            if type(secondDict[key]).__name__==&apos;dict&apos;:
                classLabel=classify(secondDict[key],featLabels,testVec)
            else:
                classLabel=secondDict[key]
    return classLabel
</code></pre><h4 id="使用算法：决策树的存储"><a href="#使用算法：决策树的存储" class="headerlink" title="使用算法：决策树的存储"></a>使用算法：决策树的存储</h4><p>构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。为了解决这个问题，需要使用Python模块pickle序列化对象，可以在磁盘上保存对象，并在需要的时候读取出来。</p>
<p>#使用pickle模块存储决策树<br>    def storeTree(inputTree,filename):<br>        import pickle<br>        fw=open(filename,’wb’)<br>        pickle.dump(inputTree,fw)<br>        fw.close()</p>
<pre><code>#读取决策树
def grabTree(filename):
    import pickle
    fr=open(filename)
    return pickle.load(fr)
</code></pre><h2 id="示例：使用决策树预测隐形眼镜类型"><a href="#示例：使用决策树预测隐形眼镜类型" class="headerlink" title="示例：使用决策树预测隐形眼镜类型"></a>示例：使用决策树预测隐形眼镜类型</h2><p>由ID3算法产生的决策树<br><img src="https://i.imgur.com/Ygs4Kwd.jpg" alt=""></p>
<p>上图所示的决策树可以非常好地匹配实验数据，然而这些匹配选项可能太多了，造成<strong>过度匹配</strong>问题。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。ID3算法可以用于划分标称型数据集。构建决策树时，通常采用递归的方法将数据集转化为决策树。</p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://lib.csdn.net/article/datastructure/9208?knId=813" target="_blank" rel="external">机器学习：决策树ID3\C4.5\CART\随机森林总结及python上的实现 (2)</a></li>
<li><a href="http://blog.csdn.net/CharlieLincy/article/details/69301490" target="_blank" rel="external">《机器学习实战》第三章：决策树（1）基本概念</a></li>
<li><a href="http://blog.csdn.net/CharlieLincy/article/details/69525051" target="_blank" rel="external">《机器学习实战》第三章：决策树（2）树的构造</a></li>
<li><a href="http://blog.csdn.net/CharlieLincy/article/details/69831446" target="_blank" rel="external">《机器学习实战》第三章：决策树（3）测试、存储、实例</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/01/机器学习实战第二章笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/01/机器学习实战第二章笔记/" itemprop="url">机器学习实战第二章学习笔记(k-NN)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-01T00:00:00+08:00">
                2017-09-01
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>k-近邻算法(kNN)采用测量不同特征值之间的距离方法进行分类。</p>
<ul>
<li>优点： 精度高、对异常值不敏感、无数据输入假定</li>
<li>缺点： 计算复杂度高、空间复杂度高</li>
</ul>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>存在训练样本集，每个样本都有标签，输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据(也称为最近邻)的分类标签。一般来说，只选择样本数据集中前k个最相似的数据，这也就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHdNJrDq4rkAaAucglnuDjOujx_RZeG0qzAVUQwXb7fWNy0EH_" alt=""></p>
<h3 id="k-近邻算法的一般流程"><a href="#k-近邻算法的一般流程" class="headerlink" title="k-近邻算法的一般流程"></a>k-近邻算法的一般流程</h3><ol>
<li>收集数据：可以使用任何方法</li>
<li>准备数据：距离计算所需要的数值，最好是结构化的数据格式</li>
<li>分析数据：可以使用任何方法</li>
<li>训练算法：此步骤不适用于k-近邻算法</li>
<li>测试算法：计算错误率</li>
<li>使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。</li>
</ol>
<h3 id="kNN代码实现"><a href="#kNN代码实现" class="headerlink" title="kNN代码实现"></a>kNN代码实现</h3><p><strong>伪代码：</strong></p>
<ol>
<li>计算已知类别数据集中的点和当前点之间的距离；</li>
<li>按照距离递增次序排序；</li>
<li>选取与当前点距离最小的k个点；</li>
<li>确定前k个点所在类别的出行频率；</li>
<li>返回前k个点出行频率最高的类别作为当前点的预测分类。</li>
</ol>
<p><strong>Python代码：</strong></p>
<pre><code>#python3.6

import numpy as np
import operator
from os import listdir

def createDataSet():
    group=np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
    labels=[&apos;A&apos;,&apos;A&apos;,&apos;B&apos;,&apos;B&apos;]
    return group,labels

    # inX: input
    # dataSet: train data set
    # labels: labels of dataSet
    # k:  k of kNN

def classify0(inX,dataSet,labels,k):
    dataSetSize=dataSet.shape[0]#样本个数
    diffMat=np.tile(inX,(dataSetSize,1))-dataSet
    #np.tile()函数用法
    sqDiffMat=diffMat**2
    sqDistances=sqDiffMat.sum(axis=1)#欧式距离
    distances=sqDistances**0.5
    sortedDistIndicies=distances.argsort()#按距离排序，返回从小到大的索引值

    classCount={}
    for i in range(k):
        voteIlabel=labels[sortedDistIndicies[i]]
        classCount[voteIlabel]=classCount.get(voteIlabel,0)+1#计算前k个，各个类别的个数
    sortedClassCount=sorted(classCount.items(),
                            key=operator.itemgetter(1),
                            reverse=True)
    #Python3中没有.iteritems(),更换为.items()
    #print(sortedClassCount)
    return sortedClassCount[0][0]

    #测试
import kNN
group,labels=kNN.createDataSet()
kNN.classify0([0,0],group,labels,3)
#结果应该是B,可以改变输入[0,0]
</code></pre><h3 id="示例1、改进约会网站的配对效果"><a href="#示例1、改进约会网站的配对效果" class="headerlink" title="示例1、改进约会网站的配对效果"></a>示例1、改进约会网站的配对效果</h3><p>约会数据存放在datingTestSet2.txt中，每个样本占一行，共有1000行，即1000个样本。主要特征：</p>
<ul>
<li>每年获得的飞行常客里程数</li>
<li>玩视频游戏所耗时间百分比</li>
<li>每周消费的冰淇淋公升数</li>
</ul>
<p><a href="https://github.com/pbharrin/machinelearninginaction" target="_blank" rel="external">数据下载</a></p>
<h4 id="将文本记录到转换NumPy的解析程序"><a href="#将文本记录到转换NumPy的解析程序" class="headerlink" title="将文本记录到转换NumPy的解析程序"></a>将文本记录到转换NumPy的解析程序</h4><pre><code>#输入文件名，输出训练样本矩阵和类标签向量
def file2matrix(filename):
    #打开文件
    fr=open(filename)
    #按行全部读取数据
    arrayOLines=fr.readlines()
    #行数
    numberOfLines=len(arrayOLines)
    #用于存放样本，主要有3个特征
    returnMat=np.zeros((numberOfLines,3))
    #分类标签
    classLabelVector=[]
    index=0
    for line in arrayOLines:
        line=line.strip()
        listFromLine=line.split(&apos;\t&apos;)
        returnMat[index,:]=listFromLine[0:3]
        classLabelVector.append(int(listFromLine[-1]))
        index+=1
    return returnMat,classLabelVector

#测试
reload(kNN)#重新加载kNN
#python3中没有内置该函数，需要from imp import reload
datingDataMat,datingLabels=kNN.file2matrix(&apos;datingTestSet2.txt&apos;)
</code></pre><h4 id="归一化数值"><a href="#归一化数值" class="headerlink" title="归一化数值"></a>归一化数值</h4><p>因为kNN是依赖样本间距离进行分类的，欧式距离的计算中，数字差值最大的特征对计算结果的影响要大一些。但是，一般来说，每个特征视为同等重要的。这样就需要对特征值进行处理，通常采用的方法是将数值归一化，如将取值范围处理为0-1或-1-1之间。</p>
<blockquote>
<p>newValue=(oldValue-min)/(max-min)</p>
</blockquote>
<p>公式可以将任意取值范围的特征值转化为0到1区间内的值。</p>
<h4 id="归一化特征值Python3程序"><a href="#归一化特征值Python3程序" class="headerlink" title="归一化特征值Python3程序"></a>归一化特征值Python3程序</h4><pre><code>#输入样本集，输出归一化后的样本集、ranges，minVals
def autoNorm(dataSet):
#newValue=(oldValue-min)/(max-min)
    minVals=dataSet.min(0)
    maxVals=dataSet.max(0)
    ranges=maxVals-minVals
    normDataSet=np.zeros(np.shape(dataSet))
    m=dataSet.shape[0]
    normDataSet=dataSet-np.tile(minVals,(m,1))
    normDataSet=normDataSet/np.tile(ranges,(m,1))
    return normDataSet,ranges,minVals
#测试
reload(kNN)
normMat,ranges,minVals=kNN.autoNorm(datingDataMat)
</code></pre><h4 id="kNN约会网站测试代码"><a href="#kNN约会网站测试代码" class="headerlink" title="kNN约会网站测试代码"></a>kNN约会网站测试代码</h4><pre><code>def datingClassTest():
    hoRatio=0.10
    datingDataMat,datingLabels=file2matrix(&apos;datingTestSet2.txt&apos;)
    normMat,ranges,minVals=autoNorm(datingDataMat)
    m=normMat.shape[0]
    numTestVecs=int(m*hoRatio)
    errorCount=0.0
    for i in range(numTestVecs):
        classifierResult=classify0(normMat[i,:],normMat[numTestVecs:m,:],
        datingLabels[numTestVecs:m],5)
        print(&apos;the classifier came back with: %d, the real answer is: %d&apos; % (classifierResult,datingLabels[i]))
        if (classifierResult != datingLabels[i]):
            errorCount+=1.0
    print(&apos;the total error rate is : %f&apos; % (errorCount/float(numTestVecs)))

#测试
kNN.datingClassTest()
</code></pre><h4 id="约会网站预测函数"><a href="#约会网站预测函数" class="headerlink" title="约会网站预测函数"></a>约会网站预测函数</h4><pre><code>def classifyPerson():
    resultList=[&apos;not at all&apos;,&apos;in small doses&apos;,&apos;in large doses&apos;]
    percentTats=float(input(&apos;percentage of time spent playing video games?&apos;))
    ffMiles=float(input(&apos;frequent fliter miles earned per year?&apos;))
    iceCream=float(input(&apos;liters of ice cream consumed per year?&apos;))
    datingDataMat,datingLabels=file2matrix(&apos;datingTestSet2.txt&apos;)
    normMat,ranges,minVals=autoNorm(datingDataMat)
    inArr=np.array([ffMiles,percentTats,iceCream])
    classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3)
    print(&apos;You will probably like this person: &apos;,\
        resultList[classifierResult-1])
    #python2中的raw_input()，python3中为input()
</code></pre><h3 id="示例2、手写识别系统"><a href="#示例2、手写识别系统" class="headerlink" title="示例2、手写识别系统"></a>示例2、手写识别系统</h3><h4 id="将32-32的二进制图像矩阵转化为1-1024向量"><a href="#将32-32的二进制图像矩阵转化为1-1024向量" class="headerlink" title="将32*32的二进制图像矩阵转化为1*1024向量"></a>将32*32的二进制图像矩阵转化为1*1024向量</h4><pre><code>def img2vector(filename):
    returnVect=np.zeros((1,1024))
    fr=open(filename)
    for i in range(32):
        lineStr=fr.readline()
        for j in range(32):
            returnVect[0,32*i+j]=int(lineStr[j])
    return returnVect
#测试
testVector=kNN.img2vector(&apos;testDigits/0_13.txt&apos;)
</code></pre><h4 id="手写数字识别系统的测试代码"><a href="#手写数字识别系统的测试代码" class="headerlink" title="手写数字识别系统的测试代码"></a>手写数字识别系统的测试代码</h4><pre><code>from os import listdir
def handwritingClassTest():
    hwLabels=[]
    trainingFileList=listdir(&apos;trainingDigits&apos;)
    m=len(trainingFileList)
    trainingMat=np.zeros((m,1024))
    for i in range(m):
        fileNameStr=trainingFileList[i]
        fileStr=fileNameStr.split(&apos;.&apos;)[0]
        classNumStr=int(fileStr.split(&apos;_&apos;)[0])#文件名是标签
        hwLabels.append(classNumStr)
        trainingMat[i,:]=img2vector(&apos;trainingDigits/%s&apos; % fileNameStr)
    testFileList=listdir(&apos;testDigits&apos;)
    errorCount=0.0
    mTest=len(testFileList)
    for i in range(mTest):
        fileNameStr=testFileList[i]
        fileStr=fileNameStr.split(&apos;.&apos;)[0]
        classNumStr=int(fileStr.split(&apos;_&apos;)[0])
        vectorUnderTest=img2vector(&apos;testDigits/%s&apos; % fileNameStr)
        classifierResult=classify0(vectorUnderTest,\
            trainingMat,hwLabels,3)
        print(&apos;the classifier came back with: %d, the real anser is: %d&apos; \
            % (classifierResult,classNumStr))
        if    classifierResult != classNumStr:
            errorCount+=1.0
    print(&apos;the total number of errors is: %d&apos; % errorCount)
#测试
kNN.handwritingClassTest()
</code></pre><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>k-近邻算法是分类数据最简单最有效的算法。k-近邻算法是基于实例的学习，使用算法时必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。</p>
<p>k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此无法知晓平均实例样本和典型实例样本具有什么特征。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/30/Windows下安装Xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/30/Windows下安装Xgboost/" itemprop="url">Windows系统下Xgboost安装【转】</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-30T12:37:43+08:00">
                2017-08-30
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>XGBoost是Gradient Boosting算法的一种高级实现，在Kaggle competitions上崭露头角。下面就对XGBoost在Windows上的安装作一个介绍，因为XGBoost在Windows平台上的安装不是那么简单直接。我在实验室的电脑上（Windows 7，64 bits）通过这些步骤安装成功，希望能对后来人有所帮助。</p>
<p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR7eIqfJRaX1uWf9oLGlL-3BT2qR2lswhzWj_psFGKy8xgKvJbs" alt=""></p>
<h2 id="安装必要的软件"><a href="#安装必要的软件" class="headerlink" title="安装必要的软件"></a>安装必要的软件</h2><p>为了能在Windows上通过Python使用XGBoost，需要先安装以下三个软件：</p>
<ul>
<li>Python</li>
<li>Git</li>
<li>MINGW</li>
</ul>
<h2 id="Python和Git的安装"><a href="#Python和Git的安装" class="headerlink" title="Python和Git的安装"></a>Python和Git的安装</h2><p>对于Python，你可以到Python官网上下载你想安装的版本，安装很简单，这里就跳过。对于Git的安装有很多种选择，一种选择就是使用Git for Windows，Git for Windows的安装也比较简单，遵从指示就行，这里也跳过。</p>
<h2 id="XGBoost的下载"><a href="#XGBoost的下载" class="headerlink" title="XGBoost的下载"></a>XGBoost的下载</h2><p>Git安装完成后，开始菜单中会出现一个叫Git Bash的程序，点开后就会出现一个类似Windows命令行的窗口，首先在这个Bash窗口，使用cd命令进入你想保存XGBoost代码的文件夹，比如下面的示例：</p>
<pre><code>$ cd /e/algorithm
</code></pre><p>然后输入下面的代码下载XGBoost文件包：</p>
<pre><code>$ git clone --recursive https://github.com/dmlc/xgboost
$ cd xgboost
$ git submodule init
$ git submodule update
</code></pre><h2 id="编译XGBoost代码"><a href="#编译XGBoost代码" class="headerlink" title="编译XGBoost代码"></a>编译XGBoost代码</h2><h3 id="MinGW-W64的安装"><a href="#MinGW-W64的安装" class="headerlink" title="MinGW-W64的安装"></a>MinGW-W64的安装</h3><p>接下来就是编译我们刚刚下载的XGBoost的代码。这就需要用到MinGW-W64。它的安装包我是从这里下载的，下载完成后双击安装，出现下面的安装界面，点击Next：</p>
<p><img src="http://o9zemtn5i.bkt.clouddn.com/mingw_1.JPG" alt=""></p>
<p>然后<strong>在Architecture选项处选择x86_64</strong>(!!!不要忘记了)即可，其他选项保持默认，如下图：</p>
<p><img src="http://o9zemtn5i.bkt.clouddn.com/mingw_2.JPG" alt=""></p>
<p>然后点击下一步，就能安装完成。我使用的是默认安装路径C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1。那么make命令和运行库就在下面的文件夹中（也就是包含mingw32-make的文件夹）：C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1\mingw64\bin，接下来就是把上面的路径添加到系统的Path中，关于如何添加环境变量到系统的Path中，可以参考<a href="https://www.computerhope.com/issues/ch000549.htm" target="_blank" rel="external">这篇文章</a>。</p>
<p>上面的步骤完成后，关闭Git Bash窗口后重新打开，为了确认添加环境变量已经添加成功，可以在Bash中键入下面的命令：</p>
<pre><code>$ which mingw32-make
</code></pre><p>如果添加成功的话，应该返回类似下面这样的信息：</p>
<pre><code>C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1\mingw64\bin\mingw32-make
</code></pre><p>为了输入的方便，可以简化mingw32-make命令为make</p>
<pre><code>$ alias make=&apos;mingw32-make&apos;
</code></pre><h3 id="XGBoost的编译"><a href="#XGBoost的编译" class="headerlink" title="XGBoost的编译"></a>XGBoost的编译</h3><p>现在就可以开始编译XGBoost了，首先进入xgboost文件夹</p>
<pre><code>$ cd /e/algorithm/xgboost
</code></pre><p>通过<a href="https://xgboost.readthedocs.io/en/latest/build.html#building-on-windows" target="_blank" rel="external">这篇官方文档</a>给出的统一编译的方法在写这篇文章时还不能正常编译成功，所以我们采用下面的命令来分开编译，每次编译一个子模块。注意，我们要等每个命令编译完成后才能键入下一个命令。</p>
<pre><code>$ cd dmlc-core
$ make -j4
$ cd ../rabit
$ make lib/librabit_empty.a -j4
$ cd ..
$ cp make/mingw64.mk config.mk
$ make -j4
</code></pre><p>一旦最后一个命令完成后，整个编译过程就完成了。下面就开始安装Python模块。进入XGBoost文件夹下面的python-package子文件夹，然后键入：</p>
<pre><code>$ cd /e/algorithm/xgboost/python-package&gt;python setup.py install
</code></pre><p>进行到这儿，基本上就完成了，这时打开一个Jupyter notebook，直接导入xgboost包会出现错误，我们需要先运行下面的代码：</p>
<pre><code>import os
mingw_path = &apos;C:\Program Files\mingw-w64\x86_64-6.3.0-posix-seh-rt_v5-rev1\mingw64\bin&apos;
os.environ[&apos;PATH&apos;] = mingw_path + &apos;;&apos; + os.environ[&apos;PATH&apos;]
</code></pre><h3 id="成功示例"><a href="#成功示例" class="headerlink" title="成功示例"></a>成功示例</h3><p>然后我们就可以开始导入xgboost包去运行下面的示例：</p>
<pre><code>import xgboost as xgb
import numpy as np
data = np.random.rand(5,10) # 5 entities, each contains 10 features
label = np.random.randint(2, size=5) # binary target
dtrain = xgb.DMatrix( data, label=label)
dtest = dtrain
param = {&apos;bst:max_depth&apos;:2, &apos;bst:eta&apos;:1, &apos;silent&apos;:1, &apos;objective&apos;:&apos;binary:logistic&apos; }
param[&apos;nthread&apos;] = 4
param[&apos;eval_metric&apos;] = &apos;auc&apos;
evallist  = [(dtest,&apos;eval&apos;), (dtrain,&apos;train&apos;)]
num_round = 10
bst = xgb.train( param, dtrain, num_round, evallist )
bst.dump_model(&apos;dump.raw.txt&apos;)
</code></pre><p>至此，如果没有出现错误，就表示安装成功。</p>
<h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><ol>
<li>可能对git bash不熟悉的可以先看看git操作命令</li>
<li>上面添加PATH步骤很重要，有些网上的博客并未提及，我第一次就是这里没有安装成功</li>
<li>xgboost这个文件夹最好不要放在python的工作路径内，也不要随意删除</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://wang-shuo.github.io/2017/02/21/%E5%9C%A8Windows%E4%B8%8B%E5%AE%89%E8%A3%85XGBoost/" target="_blank" rel="external">https://wang-shuo.github.io/2017/02/21/%E5%9C%A8Windows%E4%B8%8B%E5%AE%89%E8%A3%85XGBoost/</a></li>
<li><a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en" target="_blank" rel="external">https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/25/windows系统与树莓派间文件传输/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/25/windows系统与树莓派间文件传输/" itemprop="url">Windows系统与树莓派间文件传输</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-25T00:00:00+08:00">
                2017-08-25
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/树莓派3/" itemprop="url" rel="index">
                    <span itemprop="name">树莓派3</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/树莓派3/好记性不如烂笔头/" itemprop="url" rel="index">
                    <span itemprop="name">好记性不如烂笔头</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>windows系统和树莓派之间文件传输方式有很多种，利用FileZilla来传输，是我用的最简便快捷的一种。</p>
<h3 id="FileZilla特点"><a href="#FileZilla特点" class="headerlink" title="FileZilla特点"></a>FileZilla特点</h3><ol>
<li>易于使用</li>
</ol>
<blockquote>
<p>FileZilla比其他任何一款FTP软件都要简单</p>
</blockquote>
<ol>
<li>多协议支持</li>
</ol>
<blockquote>
<p>FileZilla支持FTP、FTPS、SFTP等文件传输协议</p>
</blockquote>
<ol>
<li>多种语言</li>
</ol>
<blockquote>
<p>FileZilla支持多国语言，完美支持简体中文</p>
</blockquote>
<ol>
<li>多标签界面</li>
</ol>
<blockquote>
<p>多标签界面</p>
</blockquote>
<ol>
<li>远程查找文件</li>
</ol>
<blockquote>
<p>FileZilla支持远程查找文件功能</p>
</blockquote>
<ol>
<li>站点管理器</li>
</ol>
<blockquote>
<p>FileZilla自带功能强大的站点管理和传输队列管理</p>
</blockquote>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>虽然介绍的功能比较多，但我关心的是与树莓派的文件传输功能。</p>
<p>直接上图：</p>
<p><img src="http://i.imgur.com/ofIVqB2.jpg" alt=""></p>
<ol>
<li>主机：<strong>sftp://YOUR DEVICE IP ADDRESS(192.168.1.102)</strong></li>
<li>填写用户名和密码，端口可不填</li>
<li><strong>快速连接</strong></li>
</ol>
<p>左边<strong>本底站点</strong>为你的windows系统资源管理器，右边<strong>远程站点</strong>是树莓派文件系统，直接拖拽就可以实现文件传输。<br><em>需要注意的是有些树莓派中的文件是有访问等级的，可能无法操作</em></p>
<p>下载地址：<a href="https://filezilla-project.org/" target="_blank" rel="external">FileZilla - The free FTP solution</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/22/ReliefF特征选择(python)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/22/ReliefF特征选择(python)/" itemprop="url">Relief特征选择算法Python实现</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-22T00:00:00+08:00">
                2017-08-22
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Relief算法"><a href="#Relief算法" class="headerlink" title="Relief算法"></a>Relief算法</h2><p>Relief算法最早由Kira在1992年提出，最初局限于两类数据的分类问题。Relief算法是一种特征权重算法(Feature weighting algorithms)，根据各个特征和类别的相关性赋予特征不同的权重，权重小于某个阈值的特征将被移除。Relief算法中特征和类别的相关性是基于特征对近距离样本的区分能力。</p>
<p><img src="https://www.gia.edu/images/Gold-Diggers-195648-636x358.jpg" alt="特征提取"></p>
<p><strong>基本思想：</strong>算法从训练集D中随机选择一个样本R，然后从和R同类的样本中寻找最近邻样本H，称为NearHit，从和R不同类的样本中寻找最近邻样本M，称为NearMiss，然后根据以下规则更新每个特征的权重：如果R和NearHit在某个特征上的距离小于R和NearMiss上的距离，则说明该特征对区分同类和不同类的最近邻是有益的，则增加该特征的权重；反之，如果R和NearHit在某个特征的距离大于R和NearMiss上的距离，说明该特征对区分同类和不同类的最近邻起负面作用，则降低该特征的权重。以上过程重复m次，最后得到各特征的平均权重。特征的权重越大，表示该特征的分类能力越强，反之，表示该特征分类能力越弱。Relief算法的运行时间随着样本的抽样次数m和原始特征个数N的增加线性增加，因而运行效率非常高。</p>
<p><strong>具体算法：</strong></p>
<p><img src="http://images.cnitblog.com/blog/79603/201308/29173431-34ed1713754f42e9a875eaa2e4049f8e.jpg" alt="Relief具体算法"></p>
<h2 id="ReliefF算法"><a href="#ReliefF算法" class="headerlink" title="ReliefF算法"></a>ReliefF算法</h2><p>虽然Relief算法比较简单，但运行效率高，并且结果也比较令人满意，因此得到广泛应用，但是其局限性在于只能处理两类别数据，因此1994年Kononeill对其进行了扩展，得到了ReliefF算法，可以处理多类别问题。ReliefF算法在处理多类问题时，每次从训练样本集中随机取出一个样本R，然后从和R同类的样本集中找出R的k个近邻样本(near Hits)，从每个R的不同类的样本集中均找出k个近邻样本(near Misses)，然后更新每个特征的权重，如下式所示：</p>
<p><img src="http://images.cnitblog.com/blog/79603/201308/29173617-b9db490cd1b84472a5583ac52d3df72b.jpg" alt=""></p>
<p><img src="http://images.cnitblog.com/blog/79603/201308/29173649-320dfe76b46e4c1097edb518dacea888.jpg" alt=""></p>
<h2 id="ReliefF算法python程序"><a href="#ReliefF算法python程序" class="headerlink" title="ReliefF算法python程序"></a>ReliefF算法python程序</h2><pre><code>import numpy as np
from sklearn.metrics.pairwise import pairwise_distances
def reliefF(X,y,**kwargs):
   if &quot;k&quot; not in kwargs.keys():
        k=5
   else:
        k=kwargs[&quot;k&quot;]
n_samples,n_features=X.shape

#计算两两距离，曼哈顿距离
distance=pairwise_distances(X,metric=&apos;manhattan&apos;)

score=np.zeros(n_features)

for idx in range(n_samples):
    #同类最近邻
    near_hit=[]
    #异类最近邻
    near_miss=dict()

    self_fea=X[idx,:]
    #类别数
    c=np.unique(y).tolist()

    stop_dict=dict()
    for label in c:
        stop_dict[label]=0
    del c[c.index(y[idx])]

    P_dict=dict()
    p_label_idx=float(len(y[y==y[idx]]))/float(n_samples)

    for label in c:
        p_label_c=float(len(y[y==label]))/float(n_samples)
        p_dict[label]=p_label_c/(1-p_label_idx)
        near_miss[label]=[]

    distance_sort=[]
    distance[idx,idx]=np.max(distance[idx,:])

    for i in range(n_samples):
            distance_sort.append([distance[idx,i],int(i),y[i]])
    distance_sort.sort(key=lambda x: x[0])

    for i in range(n_samples):
        #找到同类最近邻
        if distance_sort[i][2]==y[idx]:
            if len(near_hit) &lt;k:
                near_hit.append(distance_sort[i][1])
            elif len(near_hit)==k:
                stop_dict[y[idx]]=1
        else:
        #异类最近邻
            if len(near_miss[distance_sort[i][2])&lt;k:
                near_miss[distance_sort[i][2].append(distance_sort[i][1])
            else:
                if len(near_miss[distance_sort[i][2]]) == k:
                    stop_dict[distance_sort[i][2]] = 1
        stop = True
        for (key, value) in stop_dict.items():
                if value != 1:
                    stop = False
        if stop:
            break

    #更新reliefF分数
    for ele in near_hit:
        near_hit_term = np.array(abs(self_fea-X[ele, :]))+np.array(near_hit_term)

    near_miss_term = dict()
    for (label, miss_list) in near_miss.items():
        near_miss_term[label] = np.zeros(n_features)
        for ele in miss_list:
            near_miss_term[label] = np.array(abs(self_fea-X[ele, :]))+np.array(near_miss_term[label])
        score += near_miss_term[label]/(k*p_dict[label])
    score -= near_hit_term/k
return score

def feature_ranking(score):
    idx = np.argsort(score, 0)
    return idx[::-1]
</code></pre><p>参考博文：</p>
<ol>
<li><a href="http://www.cnblogs.com/asxinyu/archive/2013/08/29/3289682.html" target="_blank" rel="external">http://www.cnblogs.com/asxinyu/archive/2013/08/29/3289682.html</a></li>
<li><a href="https://github.com/jundongl/scikit-feature" target="_blank" rel="external">https://github.com/jundongl/scikit-feature</a></li>
</ol>
<p>特征提取Python库：<br><a href="http://featureselection.asu.edu/" target="_blank" rel="external">http://featureselection.asu.edu/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/17/机器学习书单/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/17/机器学习书单/" itemprop="url">机器学习路线【转】</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-17T00:00:00+08:00">
                2017-08-17
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/book/" itemprop="url" rel="index">
                    <span itemprop="name">book</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/book/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><ul>
<li><a href="#preparation">前言</a> </li>
<li><a href="#curriculum">课程列表</a></li>
<li><p><a href="#learning_route">推荐学习路线</a></p>
<ul>
<li><a href="#math_basic">数学基础初级</a></li>
<li><a href="#programming_basic">程序语言能力</a> </li>
<li><a href="#machine_learning_basic">机器学习课程初级</a></li>
<li><a href="#math_median">数学基础中级</a></li>
<li><a href="#machine_learning_median">机器学习课程中级</a></li>
</ul>
</li>
<li><p><a href="#booklists">推荐书籍列表</a></p>
</li>
<li><a href="#special_learning">机器学习专项领域学习</a></li>
<li><a href="#many_thanks">致谢</a></li>
</ul>
<p><img src="http://i.imgur.com/WDsQmWN.jpg" alt="Machine Learning"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><h2 id="preparation">前言</h2></h2><p>   我们要求把这些课程的所有Notes,Slides以及作者强烈推荐的论文看懂看明白，并完成所有的老师布置的习题，而推荐的书籍是不做要求的，如果有些书籍是需要看完的，我们会进行额外的说明。</p>
<h2 id="课程列表"><a href="#课程列表" class="headerlink" title="课程列表"></a><h2 id="curriculum">课程列表</h2></h2><table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">机构</th>
<th style="text-align:center">参考书</th>
<th style="text-align:center">Notes等其他资料</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="http://open.163.com/movie/2006/8/M/L/M6GLI5A07_M6GLJH1ML.html" target="_blank" rel="external">单变量微积分</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="https://www.amazon.com/exec/obidos/ASIN/0070576424/ref=nosim/mitopencourse-20" target="_blank" rel="external">Calculus with Analytic Geometry</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/" target="_blank" rel="external">链接</a> </td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/special/opencourse/multivariable.html" target="_blank" rel="external">多变量微积分</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="https://www.amazon.com/exec/obidos/ASIN/0130339679/ref=nosim/mitopencourse-20" target="_blank" rel="external">Multivariable Calculus</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/special/opencourse/daishu.html" target="_blank" rel="external">线性代数</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="http://math.mit.edu/~gs/linearalgebra/" target="_blank" rel="external">Introduction to Linear Algebra</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/study-materials/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/movie/2011/6/6/0/M82IC6GQU_M83J9IK60.html" target="_blank" rel="external">统计入门</a></td>
<td style="text-align:center">可汗学院</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
</tr>
<tr>
<td style="text-align:left">概率论入门: <a href="http://mooc.guokr.com/course/461/%E6%A9%9F%E7%8E%87/" target="_blank" rel="external">链接1</a>,<a href="https://www.youtube.com/watch?v=GwSEguqJj6U&amp;index=1&amp;list=PLtvno3VRDR_jMAJcNY1n4pnP5kXtPOmVk" target="_blank" rel="external">链接2</a></td>
<td style="text-align:center">NTU</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.youtube.com/watch?v=j9WZyLZCBzs&amp;list=PLQ3khvAsNhargDx0dG1cQXOrA2u3JsFKc" target="_blank" rel="external">概率与统计</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="https://www.amazon.com/exec/obidos/ASIN/188652923X/ref=nosim/mitopencourse-20" target="_blank" rel="external">Introduction to Probability</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/tutorials/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left">矩阵论</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="https://www.amazon.cn/%E7%9F%A9%E9%98%B5%E8%AE%BA-%E6%88%B4%E5%8D%8E/dp/B00116BRO0/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1478614198&amp;sr=1-1&amp;keywords=%E6%88%B4%E5%8D%8E%EF%BC%8C+%E7%9F%A9%E9%98%B5%E8%AE%BA" target="_blank" rel="external">矩阵论</a></td>
<td style="text-align:center">暂无 </td>
</tr>
<tr>
<td style="text-align:left"><a href="https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about" target="_blank" rel="external">凸优化1</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center"><a href="http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="external">Convex Optimization</a></td>
<td style="text-align:center"><a href="http://stanford.edu/class/ee364a/index.html" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.youtube.com/watch?v=U3lJAObbMFI&amp;list=PL3940DD956CDF0622&amp;index=20" target="_blank" rel="external">凸优化2</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="http://stanford.edu/class/ee364b/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" target="_blank" rel="external">统计学习入门</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center"><a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank" rel="external">An Introduction to Statistical Learning</a></td>
<td style="text-align:center"><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.coursera.org/instructor/htlin" target="_blank" rel="external">机器学习基石</a></td>
<td style="text-align:center">NTU</td>
<td style="text-align:center"><a href="https://www.amazon.com/gp/product/1600490069" target="_blank" rel="external">Learning from Data</a></td>
<td style="text-align:center"><a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound16fall/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.coursera.org/instructor/htlin" target="_blank" rel="external">机器学习技法</a></td>
<td style="text-align:center">NTU</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="https://www.csie.ntu.edu.tw/~htlin/course/ml15fall/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.youtube.com/watch?v=mbyG85GZ0PI&amp;index=1&amp;list=PLD63A284B7615313A" target="_blank" rel="external">机器学习</a></td>
<td style="text-align:center">Caltech</td>
<td style="text-align:center"><a href="https://www.amazon.com/gp/product/1600490069" target="_blank" rel="external">Learning from Data</a></td>
<td style="text-align:center"><a href="http://work.caltech.edu/lectures.html" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html" target="_blank" rel="external">机器学习(matlab)</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="http://cs229.stanford.edu/materials.html" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left">Python程序语言设计</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
</tr>
<tr>
<td style="text-align:left">Matlab程序语言设计</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
</tr>
</tbody>
</table>
<h2 id="推荐学习路线"><a href="#推荐学习路线" class="headerlink" title="推荐学习路线"></a><h2 id="learning_route">推荐学习路线</h2></h2><h3 id="数学基础初级"><a href="#数学基础初级" class="headerlink" title="数学基础初级"></a><h3 id="math_basic">数学基础初级</h3></h3><table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">机构</th>
<th style="text-align:center">参考书</th>
<th style="text-align:center">Notes等其他资料</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="http://open.163.com/movie/2006/8/M/L/M6GLI5A07_M6GLJH1ML.html" target="_blank" rel="external">单变量微积分</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="https://www.amazon.com/exec/obidos/ASIN/0070576424/ref=nosim/mitopencourse-20" target="_blank" rel="external">Calculus with Analytic Geometry</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/" target="_blank" rel="external">链接</a> </td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/special/opencourse/multivariable.html" target="_blank" rel="external">多变量微积分</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="https://www.amazon.com/exec/obidos/ASIN/0130339679/ref=nosim/mitopencourse-20" target="_blank" rel="external">Multivariable Calculus</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/special/opencourse/daishu.html" target="_blank" rel="external">线性代数</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="http://math.mit.edu/~gs/linearalgebra/" target="_blank" rel="external">Introduction to Linear Algebra</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/study-materials/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/movie/2011/6/6/0/M82IC6GQU_M83J9IK60.html" target="_blank" rel="external">统计入门</a></td>
<td style="text-align:center">可汗学院</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
</tr>
<tr>
<td style="text-align:left">概率论入门: <a href="http://mooc.guokr.com/course/461/%E6%A9%9F%E7%8E%87/" target="_blank" rel="external">链接1</a>,<a href="https://www.youtube.com/watch?v=GwSEguqJj6U&amp;index=1&amp;list=PLtvno3VRDR_jMAJcNY1n4pnP5kXtPOmVk" target="_blank" rel="external">链接2</a></td>
<td style="text-align:center">NTU</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.youtube.com/watch?v=j9WZyLZCBzs&amp;list=PLQ3khvAsNhargDx0dG1cQXOrA2u3JsFKc" target="_blank" rel="external">概率与统计</a></td>
<td style="text-align:center">MIT</td>
<td style="text-align:center"><a href="https://www.amazon.com/exec/obidos/ASIN/188652923X/ref=nosim/mitopencourse-20" target="_blank" rel="external">Introduction to Probability</a></td>
<td style="text-align:center"><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/tutorials/" target="_blank" rel="external">链接</a></td>
</tr>
</tbody>
</table>
<h3 id="程序语言能力"><a href="#程序语言能力" class="headerlink" title="程序语言能力"></a><h3 id="programming_basic">程序语言能力</h3></h3><p>考虑到机器学习的核心是里面的数学原理和算法思想，程序语言目前主要是帮助大家较好的完成课后作业以及实现自己的一些idea，此处我们仅仅给出推荐的参考学习链接，大家掌握一些常用的模块即可，即完成参考学习链接部分的内容即可，推荐书籍比较经典，但不做要求。</p>
<table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">参考学习链接</th>
<th style="text-align:center">推荐书籍</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Python程序语言设计</td>
<td style="text-align:center"><a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="external">链接</a></td>
<td style="text-align:center">暂无  </td>
</tr>
<tr>
<td style="text-align:left">Matlab程序语言设计</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无 </td>
</tr>
<tr>
<td style="text-align:left">R程序语言设计</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center">暂无 </td>
</tr>
</tbody>
</table>
<h3 id="机器学习课程初级"><a href="#机器学习课程初级" class="headerlink" title="机器学习课程初级"></a><h3 id="machine_learning_basic">机器学习课程初级</h3></h3><table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">机构</th>
<th style="text-align:center">参考书</th>
<th style="text-align:center">Notes等其他资料</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" target="_blank" rel="external">统计学习入门</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center"><a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank" rel="external">An Introduction to Statistical Learning</a></td>
<td style="text-align:center"><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">机器学习入门</a></td>
<td style="text-align:center">Coursera</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">链接</a></td>
</tr>
</tbody>
</table>
<h3 id="数学基础中级"><a href="#数学基础中级" class="headerlink" title="数学基础中级"></a><h3 id="math_median">数学基础中级</h3></h3><table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">机构</th>
<th style="text-align:center">参考书</th>
<th style="text-align:center">Notes等其他资料</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">矩阵论</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="https://www.amazon.cn/%E7%9F%A9%E9%98%B5%E8%AE%BA-%E6%88%B4%E5%8D%8E/dp/B00116BRO0/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1478614198&amp;sr=1-1&amp;keywords=%E6%88%B4%E5%8D%8E%EF%BC%8C+%E7%9F%A9%E9%98%B5%E8%AE%BA" target="_blank" rel="external">矩阵论</a></td>
<td style="text-align:center">暂无 </td>
</tr>
<tr>
<td style="text-align:left"><a href="https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about" target="_blank" rel="external">凸优化1</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center"><a href="http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="external">Convex Optimization</a></td>
<td style="text-align:center"><a href="http://stanford.edu/class/ee364a/index.html" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.youtube.com/watch?v=U3lJAObbMFI&amp;list=PL3940DD956CDF0622&amp;index=20" target="_blank" rel="external">凸优化2</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="http://stanford.edu/class/ee364b/" target="_blank" rel="external">链接</a></td>
</tr>
</tbody>
</table>
<p>下面这个概述必须看完。</p>
<ul>
<li><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1405.4980" target="_blank" rel="external">Convex Optimization: Algorithms and Complexity</a></li>
</ul>
<h3 id="机器学习课程中级"><a href="#机器学习课程中级" class="headerlink" title="机器学习课程中级"></a><h3 id="machine_learning_median">机器学习课程中级</h3></h3><p>   此处NTU和Caltech两个大学的课程是由《Learning from Data》一书的两个不同的作者讲的，所以仅仅只需选择一个完成即可，注意：如果选择完成NTU的机器学习课程，则<strong>NTU的“机器学习基石”和“机器学习技法”需同时完成。</strong>。</p>
<table>
<thead>
<tr>
<th style="text-align:left">课程</th>
<th style="text-align:center">机构</th>
<th style="text-align:center">参考书</th>
<th style="text-align:center">Notes等其他资料</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://www.coursera.org/instructor/htlin" target="_blank" rel="external">机器学习基石</a></td>
<td style="text-align:center">NTU</td>
<td style="text-align:center"><a href="https://www.amazon.com/gp/product/1600490069" target="_blank" rel="external">Learning from Data</a></td>
<td style="text-align:center"><a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound16fall/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.coursera.org/instructor/htlin" target="_blank" rel="external">机器学习技法</a></td>
<td style="text-align:center">NTU</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="https://www.csie.ntu.edu.tw/~htlin/course/ml15fall/" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html" target="_blank" rel="external">机器学习</a></td>
<td style="text-align:center">Stanford</td>
<td style="text-align:center">暂无</td>
<td style="text-align:center"><a href="http://cs229.stanford.edu/materials.html" target="_blank" rel="external">链接</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.youtube.com/watch?v=mbyG85GZ0PI&amp;index=1&amp;list=PLD63A284B7615313A" target="_blank" rel="external">机器学习</a></td>
<td style="text-align:center">Caltech</td>
<td style="text-align:center"><a href="https://www.amazon.com/gp/product/1600490069" target="_blank" rel="external">Learning from Data</a></td>
<td style="text-align:center"><a href="http://work.caltech.edu/lectures.html" target="_blank" rel="external">链接</a></td>
</tr>
</tbody>
</table>
<h2 id="推荐书籍列表"><a href="#推荐书籍列表" class="headerlink" title="推荐书籍列表"></a><h2 id="booklists">推荐书籍列表</h2></h2><p>   以下推荐的书籍都是公认的机器学习领域界的好书，建议<strong>一般难度的书籍至少详细阅读一本，建议看两本</strong>，而较难的书籍不做任何要求，大家可以在学有余力时细细品味经典。</p>
<table>
<thead>
<tr>
<th style="text-align:left">书名</th>
<th style="text-align:center">难度</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://www.amazon.cn/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA/dp/B007TSFMTA" target="_blank" rel="external">统计学习方法</a></td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:left"> <a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank" rel="external">An Introduction to Statistical Learning</a></td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.amazon.com/gp/product/0071154671?ie=UTF8&amp;tag=jefork-20&amp;linkCode=as2&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0071154671" target="_blank" rel="external">Machine Learning</a></td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.amazon.com/gp/product/1600490069" target="_blank" rel="external">Learning from Data</a></td>
<td style="text-align:center">一般，<a href="https://work.caltech.edu/telecourse.html" target="_blank" rel="external">配套讲义</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=pd_sim_14_1?ie=UTF8&amp;dpID=61f0EXfMRvL&amp;dpSrc=sims&amp;preST=_AC_UL160_SR118%2C160_&amp;refRID=119X50P5F0DFA339S9DR" target="_blank" rel="external">Pattern Recognition and Machine Learning</a></td>
<td style="text-align:center">较难(偏贝叶斯),<a href="http://cs.brown.edu/courses/csci1420/lectures.html" target="_blank" rel="external">配套讲义</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.amazon.com/The-Elements-Statistical-Learning-Prediction/dp/0387848576/ref=pd_sim_14_2?ie=UTF8&amp;dpID=41LeU3HcBdL&amp;dpSrc=sims&amp;preST=_AC_UL160_SR103%2C160_&amp;refRID=119X50P5F0DFA339S9DR" target="_blank" rel="external">The Elements of Statistical Learning</a></td>
<td style="text-align:center">较难</td>
</tr>
<tr>
<td style="text-align:left"><a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf" target="_blank" rel="external">Understanding Machine Learning:From Theory to Algorithms</a></td>
<td style="text-align:center">较难</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020" target="_blank" rel="external">Machine Learning: A probabilistic approach</a></td>
<td style="text-align:center">较难</td>
</tr>
</tbody>
</table>
<h2 id="机器学习专项领域学习"><a href="#机器学习专项领域学习" class="headerlink" title="机器学习专项领域学习"></a><h3 id="special_learning">机器学习专项领域学习</h3></h2><p>如果您已经完成了上述的所有科目，恭喜您已经拥有十分扎实的机器学习基础了，已经是一名合格的机器学习成员了，可以较为顺利的进入下面某一专项领域进行较为深入研究,因为并不是所有的专项领域都有对应的课程或者书籍等学习资料，所以此处我们仅列举一些我们知道的专项领域的学习资料，当然这些领域不能涵盖所有，还有很多领域没有整理（希望大家一起完善），如果这些领域适合你，那就继续加油！如果不清楚，那么大家可以去下面列举的高级会议期刊上去寻找自己感兴趣的话题进行学习研究。</p>
<h3 id="一些专项领域资料"><a href="#一些专项领域资料" class="headerlink" title="一些专项领域资料"></a><h3 id="special_learning_data">一些专项领域资料</h3></h3><ul>
<li><a href="https://github.com/JustFollowUs/Deep-Learning" target="_blank" rel="external">深度学习</a></li>
<li><a href="https://github.com/JustFollowUs/Probabilistic-graphical-models" target="_blank" rel="external">图模型</a></li>
<li><a href="https://github.com/JustFollowUs/Reinforcement-Learning" target="_blank" rel="external">强化学习</a></li>
<li><a href="http://cs.nju.edu.cn/lwj/L2H.html" target="_blank" rel="external">Hash</a> </li>
<li><a href="https://github.com/JustFollowUs/Theoretical-Machine-Learning/" target="_blank" rel="external">理论机器学习</a></li>
<li>其他(尚未完善)</li>
</ul>
<h3 id="领域会议期刊"><a href="#领域会议期刊" class="headerlink" title="领域会议期刊"></a><h3 id="special_learning_data">领域会议期刊</h3></h3><ul>
<li><a href="https://nips.cc/" target="_blank" rel="external">NIPS</a></li>
<li><a href="http://icml.cc/" target="_blank" rel="external">ICML</a></li>
<li><a href="http://www.aaai.org/" target="_blank" rel="external">AAAI</a></li>
<li><a href="http://www.ijcai.org/" target="_blank" rel="external">IJCAI</a></li>
<li><a href="http://www.kdd.org/" target="_blank" rel="external">KDD</a></li>
<li><a href="http://www.cs.uvm.edu/~icdm/" target="_blank" rel="external">ICDM</a></li>
<li><a href="http://www.learningtheory.org/" target="_blank" rel="external">COLT</a></li>
<li>其他(尚未完善)</li>
</ul>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a><h2 id="many_thanks">致谢</h2></h2><p>  感谢南京大学LAMDA实验组杨杨博士的建议与资料的分享。</p>
<p>原文链接：<a href="https://github.com/JustFollowUs/Machine-Learning" target="_blank" rel="external">https://github.com/JustFollowUs/Machine-Learning</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/16/Snowboy使用说明_Hotword Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/16/Snowboy使用说明_Hotword Detection/" itemprop="url">Snowboy使用说明_Hotword Detection</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-16T00:00:00+08:00">
                2017-08-16
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/树莓派3/" itemprop="url" rel="index">
                    <span itemprop="name">树莓派3</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Snowboy是一个高度自定义的基于实时甚至离线情况的hotword detection engine(敏感词检测机制？)，兼容Raspberry Pi,Linux and Mac OS X.</p>
<p>hotword也被称为唤醒词(wake word)或触发词(trigger word)通常是一个关键词或短语，计算机会一直监听作为一个信号用于触发其他操作。</p>
<p>有一些例子，如Amazon Echo的“Alexa”、Google Assistant的”OK Google”和iPhone的“Hey Siri”. 这些关键词用于触发一个全面的语音交互。但是，hotwords同样可以用于其他方面，像是命令和控制等。</p>
<p>一种简单的方案是，运行全ASR(Automatic Speech Recognition)来检测hotword detection.在这种方案中，设备会一直观察特定触发词。但是ASR也会消耗设备和带宽资源。同样的，如果基于云的应用，将不能保护你的隐私。（因为会一直开着麦克风来检测关键词，那么周围环境包括你说的话也会实时被监听了).幸运的是，Snowboy会用来解决此类问题。</p>
<ul>
<li>highly customizable(高度自定义）</li>
<li>always listening but protects your privacy(总是检测但是不会泄露隐私）</li>
<li>light-weight and embedded</li>
<li>Apache licensed!</li>
</ul>
<p><img src="http://i.imgur.com/iJulCvp.jpg" alt="Snowboy"></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>准备工作：</p>
<ol>
<li>一台带有microphone的设备</li>
<li>相应的解压软件 </li>
<li>训练好的模型</li>
</ol>
<p><a href="https://s3-us-west-2.amazonaws.com/snowboy/snowboy-releases/rpi-arm-raspbian-8.0-1.1.1.tar.bz2" target="_blank" rel="external">树莓派pre-packaged</a></p>
<h3 id="Access-Microphone"><a href="#Access-Microphone" class="headerlink" title="Access Microphone"></a>Access Microphone</h3><p>使用PortAudio作为一个跨平台的音频输入/输出。同样也使用sox快速检查microphone是否正确安装。</p>
<ol>
<li>Install Sox</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install python-pyaudio python3-pyaudio sox</div></pre></td></tr></table></figure>
<ol>
<li>Install PortAudio’s Python bindings:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install pyaudio</div><div class="line"><span class="comment">#pip-3.2 install pyaudio</span></div></pre></td></tr></table></figure>
<ol>
<li>To check whether you can record via your microphone, open a terminal and run:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rec temp.wav</div><div class="line"><span class="comment">#记录个几秒中，ctrl+c,再play temp.wav,听声</span></div></pre></td></tr></table></figure>
<h3 id="Decoder-Structures"><a href="#Decoder-Structures" class="headerlink" title="Decoder Structures"></a>Decoder Structures</h3><p>上面预安装包下载解压后，如下：</p>
<blockquote>
<p>├── README.md</p>
<p>├── _snowboydetect.so</p>
<p>├── demo.py</p>
<p>├── demo2.py</p>
<p>├── light.py</p>
<p>├── requirements.txt</p>
<p>├── resources</p>
<p>│   ├── ding.wav</p>
<p>│   ├── dong.wav</p>
<p>│   ├── common.res</p>
<p>│   └── snowboy.umdl</p>
<p>├── snowboydecoder.py</p>
<p>├── snowboydetect.py</p>
<p>└── version</p>
</blockquote>
<p><strong>_snowboydetect.so</strong>是用SWIG编译的一个动态链接库，依赖于系统的Python2库。snowboy所有相关库都被静态连接在这个文件里。</p>
<p><strong>snowboydetect.py</strong>是一个SWIG生成的python wrapper文件。因为不易阅读，我们创建了高等级的wrapper: <strong>snowboydecoder.py</strong></p>
<p>应该在<a href="https://snowboy.kitt.ai" target="_blank" rel="external">https://snowboy.kitt.ai </a>上训练你的模型(snowboy.pmdl)，或者你也可以使用同一模型<strong>resources/snowboy.umdl</strong></p>
<h3 id="Runing-a-Demo"><a href="#Runing-a-Demo" class="headerlink" title="Runing a Demo"></a>Runing a Demo</h3><ol>
<li>To access the simple demo in <strong>main</strong> code of snowboydecoder.py, run the following command in your Terminal:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">python demo.py snowboy.pmdl</div><div class="line"><span class="comment">#snowboy.pmdl是你训练的hotword模型</span></div></pre></td></tr></table></figure>
<ol>
<li>When prompt,speak into your microphone to see whether snowboy detects your magic phrase.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">#demo.py</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> snowboydecoder</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> signal</div><div class="line"></div><div class="line">interrupted = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">signal_handler</span><span class="params">(signal, frame)</span>:</span></div><div class="line">   	<span class="keyword">global</span> interrupted</div><div class="line">   	interrupted = <span class="keyword">True</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">interrupt_callback</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">global</span> interrupted</div><div class="line">    <span class="keyword">return</span> interrupted</div><div class="line"></div><div class="line"><span class="keyword">if</span> len(sys.argv) == <span class="number">1</span>:</div><div class="line">    print(<span class="string">"Error: need to specify model name"</span>)</div><div class="line">    print(<span class="string">"Usage: python demo.py your.model"</span>)</div><div class="line">    sys.exit(<span class="number">-1</span>)</div><div class="line"></div><div class="line">model = sys.argv[<span class="number">1</span>]</div><div class="line"></div><div class="line">signal.signal(signal.SIGINT, signal_handler)</div><div class="line"></div><div class="line">detector = snowboydecoder.HotwordDetector(model, sensitivity=<span class="number">0.5</span>)</div><div class="line">print(<span class="string">'Listening... Press Ctrl+C to exit'</span>)</div><div class="line"></div><div class="line">detector.start(detected_callback=snowboydecoder.ding_callback,</div><div class="line">               interrupt_check=interrupt_callback,</div><div class="line">               sleep_time=<span class="number">0.03</span>)</div><div class="line"></div><div class="line">detector.terminate()</div></pre></td></tr></table></figure>
<p>主程序在<strong>detector.start()</strong>中循环，每<strong>sleep_time=0.03</strong>:</p>
<ol>
<li>检查ring buffer 是否有hotword, if <strong>YES</strong>,调用<strong>detected_callback</strong>函数</li>
<li>调用<strong>interrupt_check</strong>函数，if <strong>True</strong>，中断主程序，返回.</li>
</ol>
<p>目前，在demo中令detected_callback=snowboydecoder.ding_callback,所以每当检测到关键词时，设备会“叮”一下。</p>
<p><strong>原文出处</strong>：<a href="http://docs.kitt.ai/snowboy/#running-a-demo" target="_blank" rel="external">http://docs.kitt.ai/snowboy/#running-a-demo</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/11/ReSpeaker智能语音双麦克风阵列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andy's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/11/ReSpeaker智能语音双麦克风阵列/" itemprop="url">ReSpeaker智能语音双麦克风阵列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-11T00:00:00+08:00">
                2017-08-11
              </time>
            

            

            
          </span>

		  
		  
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/树莓派3/" itemprop="url" rel="index">
                    <span itemprop="name">树莓派3</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
			
			
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>去年圣诞节买了块树莓派，看了网上好多基于树莓派的DIY语音助手，一直想自己也山寨一个。主要有两个问题，一是语音输入如何解决？二是Alexa和google home都是国外，不仅仅是不支持中文，而且还有一堵“墙”在，比较麻烦。</p>
<p>第一个问题，网上主要有两种办法，要么直接搞免驱mini USB麦克风或者大的比较丑的的麦克风，要么就是科技范十足的麦克风阵列。我是非常倾向于用麦克风阵列的，也没想4麦克、7麦克、8麦克等，两麦克阵列就OK了，但是找了一圈没找着合适的。主要是价钱太贵，不忍心剁手。后来将就着买了个mini USB麦克风，但是除了敲桌子声音或者直接吼，根本没啥效果，检测不到声音(明明看着国外的小哥哥们用的挺好的。。。也可能是淘宝上买到假货了)。</p>
<p><img src="https://ksr-ugc.imgix.net/assets/013/450/587/88ed9594144b9cf686c055b282e3fa4b_original.jpg?w=680&amp;fit=max&amp;v=1471845738&amp;auto=format&amp;q=92&amp;s=3e81cd923a00655c28c20c5b143b57c2" alt="麦克风阵列"></p>
<p><img src="https://gd4.alicdn.com/imgextra/i2/329675712/TB2FS30spXXXXX6XpXXXXXXXXXX_!!329675712.jpg_400x400.jpg" alt="mini USB麦克风"></p>
<p>第二个，明显今年百度开始在AI上下了不少功夫。语音这一块，主要有UNIT、DuerOS平台。虽然做的没有Google Home 和Alexa那么知名，但是支持中文，而且在国内。</p>
<p>所以借着百度之星UNIT对话系统，又开始了我的瞎折腾(之前搞了Alexa和Google Home,加上不支持中文、墙、硬件问题等，搞得头大就放弃了)。这一次，借助着百度AI平台，语音识别转文字，然后利用UNIT解析，再语音合成输出。那么就剩下硬件了，还是上面的选择，USB麦克风或者麦克风阵列。经人推荐，找到了一款双麦克阵列模块，兼容树莓派3，淘宝上有旗舰店，80元RMB.(哈哈，当时直接就下单了！)</p>
<p>淘宝上给的简介是：</p>
<blockquote>
<h3 id="ReSpeaker智能语音方案-双麦克风扩展板-兼容树莓派Zero-3B-2B"><a href="#ReSpeaker智能语音方案-双麦克风扩展板-兼容树莓派Zero-3B-2B" class="headerlink" title="ReSpeaker智能语音方案 双麦克风扩展板 兼容树莓派Zero/3B/2B"></a>ReSpeaker智能语音方案 双麦克风扩展板 兼容树莓派Zero/3B/2B</h3><p>此产品集成了亚马逊语言和谷歌助手等，兼容树莓派Zero、树莓派3B/2B，可以构建一个更强大更灵活的语音产品。</p>
</blockquote>
<p><a href="https://item.taobao.com/item.htm?spm=a1z10.3-c.w4002-11172317909.38.5e478797SzWPpR&amp;id=553438198956" target="_blank" rel="external">淘宝链接</a></p>
<p>但实际上，这货的外国名是<strong>ReSpeaker 2-Mics Pi HAT</strong></p>
<p><a href="http://wiki.seeed.cc/Respeaker_2_Mics_Pi_HAT/" target="_blank" rel="external">相关链接</a></p>
<p><img src="https://github.com/SeeedDocument/MIC_HATv1.0_for_raspberrypi/blob/master/img/2mics-zero-high-res.jpg?raw=true" alt="ReSpeaker 2-Mics Pi HAT"></p>
<p>ReSpeaker 2-Mics Pi HAT是一款为树莓派而设计针对AI或语音应用的双麦克风扩展板。这意味着你可以基于树莓派（集成Amazon Alexa,Google Assistant)建立一个更强大、更灵活的语音产品.</p>
<p>这块板子基于WM8960,一片低功耗立体声编解码器。有两个麦克风分别位于板子的两侧用于采集声音。板子上还有3个APA102 RGB LED,1个用户按键和两个Grove接口用于扩展应用。更惊喜的是，还有3.5mm Audio Jack和JST 2.0 Speaker接口用于输出声音。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul>
<li>兼容树莓派(Raspberry Pi Zero and Zero W, Raspberry Pi B+,Raspberry Pi 2B and Raspberry Pi 3B)</li>
<li>2个麦克风</li>
<li>2个Grove接口</li>
<li>1个用户按键</li>
<li>3.5mm音频接口</li>
<li>JST2.0音频输出</li>
</ul>
<h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><ul>
<li>语音交互应用</li>
<li>AI助手</li>
</ul>
<h2 id="硬件介绍"><a href="#硬件介绍" class="headerlink" title="硬件介绍"></a>硬件介绍</h2><p><img src="https://github.com/SeeedDocument/MIC_HATv1.0_for_raspberrypi/blob/master/img/mic_hatv1.0.png?raw=true" alt="Hardware Overview"></p>
<ul>
<li>BUTTON:用户按键，连接到GPIO17</li>
<li>MIC_L &amp; MIC_R:位于板子两侧的麦克风</li>
<li>RGB LED:3颗APA102 RGB LED,连接到了SPI接口</li>
<li>WM8960: 低功耗立体声编解码器</li>
<li>Raspberry Pi 40-Pin Headers:</li>
<li>POWER:板子的USB供电口，当使用扬声器时，要保证足够的电流</li>
<li>I2C:Grove I2C接口，连接I2C-1</li>
<li>GPIO 12：Grove 数字端口，连接GPIO12 &amp; GPIO13</li>
<li>JST 2.0 SPEAKER OUT:连接扬声器</li>
<li>3.5mm AUDIO JACK:连接带有3.5mm插口的耳机或扬声器</li>
</ul>
<p>具体配置及使用参考：<a href="http://wiki.seeed.cc/Respeaker_2_Mics_Pi_HAT/" target="_blank" rel="external">http://wiki.seeed.cc/Respeaker_2_Mics_Pi_HAT/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Andy Chen" />
          <p class="site-author-name" itemprop="name">Andy Chen</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Andy1314Chen" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.jianshu.com" target="_blank" title="简书">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  简书
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andy Chen</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

  |  本页点击 <span id="busuanzi_value_page_pv"></span> 次
  |  本站总点击 <span id="busuanzi_value_site_pv"></span> 次
  |  您是第 <span id="busuanzi_value_site_uv"></span> 位访客
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  
  <script type="text/javascript" src="/js/src/particle.js" count="50" zindex="-2" opacity="1" color="0,104,183"></script>
  <!-- 小红心 -->
  <script type="text/javascript" src="/js/src/love.js"></script>
</body>
</html>
